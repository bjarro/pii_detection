{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4babe69d-ab05-4787-8930-3607005fbe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e34afcb-3a17-48d7-b47d-dc4fae3df907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span, Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52d869a-8c6c-4570-a4d9-be7e4e728d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835d155c-82df-4c32-895f-9e51160f7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9b365d-c55c-4442-9718-9cc706dee739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690be1e5-1ed1-43e4-a90f-755d041736c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f60fbbf4-6416-44bd-ad5f-bb215b158ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run PII_Util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee78546-ef00-4935-88ec-b78bcf8879da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "390c6941-1865-4bc0-95d9-f703f390f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = './in/train.json'\n",
    "path_test = './in/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "318a1421-4a76-4774-a646-dafa05ba4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = json.load(open(path_train))\n",
    "df_train = pd.json_normalize(train_json)\n",
    "\n",
    "test_json = json.load(open(path_test))\n",
    "df_test = pd.json_normalize(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac7afd30-843f-4688-939a-b3d2b359b3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37e2a937-11d8-4dc9-99d0-0513db2d515f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[True, True, True, False, False, True, False, ...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Assignment:  Visualization Reflection  Submitt...</td>\n",
       "      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
       "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
       "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "2  [True, False, False, True, True, False, False,...   \n",
       "3  [True, True, True, False, False, True, False, ...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b469fb3-ba16-4c54-bb46-c936e93decd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = \"\"\"Meet Jane Doe, a brilliant student at XYZ University. She can be reached at jane.doe@email.com or through her phone number +1234567890. Jane resides at 123 Main Street, Cityville. Her student ID is 987654 and her personal website is www.janedoe.com. Connect with her on social media using the username @janedoe.\n",
    "\n",
    "Meanwhile, John Smith, another outstanding student, can be contacted at john.smith@email.com or at +9876543210. John lives at 456 Oak Avenue, Townsville. His student ID is 123456, and you can visit his personal blog at www.johnsmithblog.com. Follow him on Twitter with the handle @johnsmith123.\n",
    "\n",
    "For any inquiries about the university's programs, you can contact the administration office at admin@xyzuniversity.edu or call +5551234567. The office is located at 789 University Boulevard.\n",
    "\n",
    "Visit our official website at www.xyzuniversity.edu for more information on courses and admission procedures.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2fac1f1-2a6d-4daa-8fe3-a8d82f3ca637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meet Jane Doe, a brilliant student at XYZ University. She can be reached at jane.doe@email.com or through her phone number +1234567890. Jane resides at 123 Main Street, Cityville. Her student ID is 987654 and her personal website is www.janedoe.com. Connect with her on social media using the username @janedoe.\\n\\nMeanwhile, John Smith, another outstanding student, can be contacted at john.smith@email.com or at +9876543210. John lives at 456 Oak Avenue, Townsville. His student ID is 123456, and you can visit his personal blog at www.johnsmithblog.com. Follow him on Twitter with the handle @johnsmith123.\\n\\nFor any inquiries about the university's programs, you can contact the administration office at admin@xyzuniversity.edu or call +5551234567. The office is located at 789 University Boulevard.\\n\\nVisit our official website at www.xyzuniversity.edu for more information on courses and admission procedures.\\n\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b39df-91fe-495a-a76d-459c5183412c",
   "metadata": {},
   "source": [
    "# Dataset Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1625036-8b64-4fb6-bc3e-8ba8df1263b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['O',\n",
    "'B-EMAIL',\n",
    "'B-ID_NUM',\n",
    "'B-NAME_STUDENT',\n",
    "'B-PHONE_NUM',\n",
    "'B-STREET_ADDRESS',\n",
    "'B-URL_PERSONAL',\n",
    "'B-USERNAME',\n",
    "'I-ID_NUM',\n",
    "'I-NAME_STUDENT',\n",
    "'I-PHONE_NUM',\n",
    "'I-STREET_ADDRESS',\n",
    "'I-URL_PERSONAL',\n",
    "]\n",
    "\n",
    "classes2id = {clas:i for i, clas in enumerate(classes)}\n",
    "id2classes = {i:clas for i, clas in enumerate(classes)}\n",
    "classes_pos = classes[1:]\n",
    "classes_pos_id = [classes2id[label] for label in classes_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a09f5-0fe7-40df-925c-7bb62f7c0488",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb6f93-0310-42de-a9b0-28ce367fe921",
   "metadata": {},
   "source": [
    "## Yanis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca1299c5-6c5f-4904-9759-613123534571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PII_Adapter():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "#Model not in BIO Format\n",
    "class Yanis_Adapter(PII_Adapter):\n",
    "    def __init__(self, threshold = 0.1):\n",
    "        super().__init__(\"Yanis/microsoft-deberta-v3-large_ner_conll2003-anonimization_TRY_1\")\n",
    "        \n",
    "        self.O_id = self.model.config.label2id['O']\n",
    "            \n",
    "        self.threshold = threshold\n",
    "        self.model_id2cur_label = yanis_to_cur = {\n",
    "            0:'O', \n",
    "            1:'O',\n",
    "            2:'NAME_STUDENT',\n",
    "            3:'O',\n",
    "            4:'PHONE_NUM',\n",
    "            5:'O',\n",
    "            6:'O',\n",
    "            7:'O',\n",
    "            8:'ID_NUM',\n",
    "            9:'O',\n",
    "            10:'ID_NUM',\n",
    "            11:'O',\n",
    "            12:'STREET_ADDRESS',\n",
    "            13:'O',\n",
    "            14:'EMAIL',\n",
    "            15:'O',\n",
    "            16:'O',\n",
    "            17:'O'}\n",
    "        \n",
    "        self.labels_irrelevant = [key for key,value in self.model_id2cur_label.items() if value == 'O' and key != self.O_id]\n",
    "        \n",
    "        rev_model_id2cur_label = {value:key for key,value in  self.model_id2cur_label.items()}\n",
    "        rev_model_id2cur_label['O'] = self.O_id\n",
    "        \n",
    "        cur_label2model_id = {'O': self.O_id}\n",
    "        for label in classes:\n",
    "            if label == 'O':  \n",
    "                continue\n",
    "            else:\n",
    "                entity = label.split('-')[1]\n",
    "\n",
    "                if entity in rev_model_id2cur_label:\n",
    "                    cur_label2model_id[label] = rev_model_id2cur_label[entity]        \n",
    "                else:\n",
    "                    cur_label2model_id[label] = rev_model_id2cur_label['O']\n",
    "                    \n",
    "        self.cur_label2model_id = cur_label2model_id\n",
    "                    \n",
    "                    \n",
    "#         self.cur_label2model_id = {label: rev_model_id2cur_label(label) if in self.model.config.labels2id.keys() else  'O' for label in self.model.config.labels2id.keys()}       \n",
    "#         self.label2id = { id2classes: for label in self.model.labels}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37ca7f-723f-4d2a-b9b5-12f139bd4798",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5a816-4451-4546-a2cc-1fcc33af60ab",
   "metadata": {},
   "source": [
    "## Align token to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642b1bd9-5503-4973-b395-e0b56c43b643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_tokens(tokens, word_ids, labels):\n",
    "    \n",
    "    #Tokens to word\n",
    "    \n",
    "    word_subword_mapping = {}\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is not None:\n",
    "            if word_id not in word_subword_mapping:\n",
    "                word_subword_mapping[word_id] = []\n",
    "            word_subword_mapping[word_id].append(i)\n",
    "\n",
    "    # Step 5: Iterate through pairs of words and subwords to count the majority label\n",
    "    word_labels = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i in word_subword_mapping:\n",
    "            subword_labels = pred_labels[word_subword_mapping[i]]\n",
    "            majority_label = get_majority(subword_labels)\n",
    "            word_labels.append((word, majority_label))\n",
    "        else:\n",
    "            word_labels.append((word,'O'))\n",
    "            \n",
    "            \n",
    "    return word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0163d596-f54e-4d93-b5a0-ef967ce77296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(tokens, words, word_ids, labels):\n",
    "    \n",
    "#     global word_ids_cur\n",
    "#     global words_cur\n",
    "#     global labels_cur\n",
    "    \n",
    "#     words_cur = words\n",
    "#     word_ids_cur = word_ids\n",
    "#     labels_cur = labels\n",
    "    \n",
    "    #word to tokens\n",
    "    token_labels = []\n",
    "    prev_entity = None\n",
    "    prev_ent_type = None\n",
    "    \n",
    "    # Step 5: Iterate through pairs of words and subwords to count the majority label\n",
    "    for i, (word_id, token) in enumerate(zip(word_ids, tokens)):\n",
    "        if word_id is None:\n",
    "            token_labels.append('O')\n",
    "#             continue\n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                token_label = labels[word_id]\n",
    "            except Exception as e:\n",
    "                print(word_id)\n",
    "                print(len(labels))\n",
    "                \n",
    "                raise(e)\n",
    "            #Outside\n",
    "            if token_label == 'O':\n",
    "                token_labels.append('O')\n",
    "                ent_type = 'O'\n",
    "                \n",
    "            else:\n",
    "                prefix, ent_type = token_label.split('-')\n",
    "            \n",
    "                #Same entity: B-ent:B-ent, I-ent,I-ent, B-ent,I-ent\n",
    "                if prev_entity == token_label or f'I-{prev_ent_type}' == token_label:\n",
    "                    token_labels.append(f'I-{prev_ent_type}')\n",
    "\n",
    "                #New Entity: I-ent: B-ent, x-ent1: x-ent2\\\n",
    "                else:\n",
    "                    token_labels.append(f'B-{ent_type}')\n",
    "\n",
    "            prev_entity = token_labels\n",
    "            prev_ent_type = ent_type\n",
    "                \n",
    "            \n",
    "    return token_labels\n",
    "\n",
    "# tokens = inputs.tokens()\n",
    "# token_labels = align_words(tokens, df_train.loc[0].tokens,word_ids, labels_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00cb22a1-a761-445c-ac1b-70ae3e4954c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pos_threshold(tensor_probs, neg_index,  threshold=0.5):\n",
    "    \n",
    "#     indices_max = np.argmax(tensor_probs, axis = 1,2)\n",
    "    \n",
    "    \n",
    "    \n",
    "# #     highest_non_O_labels = []\n",
    "\n",
    "# #     for token, probabilities in probabilities:\n",
    "# #         non_O_labels = []\n",
    "# #         non_O_probabilities = []\n",
    "\n",
    "# #         # Iterate through predicted label probabilities\n",
    "# #         for label_idx, probability in enumerate(probabilities):\n",
    "# #             label_name = id2label[label_idx]\n",
    "# #             # Exclude 'O' labels and labels with probabilities below the threshold\n",
    "# #             if label_name != 'O' and probability >= threshold:\n",
    "# #                 non_O_labels.append(label_name)\n",
    "# #                 non_O_probabilities.append(probability)\n",
    "\n",
    "# #         # If there are non-'O' labels above the threshold, select the one with the highest probability\n",
    "# #         if non_O_labels:\n",
    "# #             highest_non_O_label = non_O_labels[non_O_probabilities.index(max(non_O_probabilities))]\n",
    "# #             highest_non_O_labels.append((token, highest_non_O_label))\n",
    "# #         else:\n",
    "# #             # If all labels are 'O' or below the threshold, consider it as non-entity\n",
    "# #             highest_non_O_labels.append((token, 'O'))\n",
    "\n",
    "#     return highest_non_O_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11c13b7d-f57d-4203-ad29-ee1bfae6765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_threshold_label(tokens, label_probabilities, id2label, threshold=0.5):\n",
    "    highest_non_O_labels = []\n",
    "\n",
    "    for token, probabilities in zip(tokens, label_probabilities):\n",
    "        non_O_labels = []\n",
    "        non_O_probabilities = []\n",
    "\n",
    "        # Iterate through predicted label probabilities\n",
    "        for label_idx, probability in enumerate(probabilities):\n",
    "            label_name = id2label[label_idx]\n",
    "            # Exclude 'O' labels and labels with probabilities below the threshold\n",
    "            if label_name != 'O' and probability >= threshold:\n",
    "                non_O_labels.append(label_name)\n",
    "                non_O_probabilities.append(probability)\n",
    "\n",
    "        # If there are non-'O' labels above the threshold, select the one with the highest probability\n",
    "        if non_O_labels:\n",
    "            highest_non_O_label = non_O_labels[non_O_probabilities.index(max(non_O_probabilities))]\n",
    "            highest_non_O_labels.append((token, highest_non_O_label))\n",
    "        else:\n",
    "            # If all labels are 'O' or below the threshold, consider it as non-entity\n",
    "            highest_non_O_labels.append((token, 'O'))\n",
    "\n",
    "    return highest_non_O_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c965e6b1-9111-463d-8690-8ed69840bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority(arr):\n",
    "    unique_elements, counts = np.unique(arr, return_counts=True)\n",
    "    max_count_index = np.argmax(counts)\n",
    "    return unique_elements[max_count_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce00851d-95fc-40fd-bb9f-296c5bfbe153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_map_pii = {'B-EMAIL': '#2fc3da',\n",
    "             'B-ID_NUM': '#1c0cfa',\n",
    "             'B-NAME_STUDENT':'#e01d82',\n",
    "             'B-PHONE_NUM': '#ebe70e',\n",
    "             'B-STREET_ADDRESS':'#f2860a',\n",
    "             'B-URL_PERSONAL': '#c9f211',\n",
    "             'B-USERNAME': '#0eebb7',\n",
    "             'I-ID_NUM': '#8e87ed',\n",
    "             'I-NAME_STUDENT':'#eb83b9',\n",
    "             'I-PHONE_NUM': '#e6e4a1',\n",
    "             'I-STREET_ADDRESS': '#f2c694',\n",
    "             'I-URL_PERSONAL':'#e5f2ac'}\n",
    "options_pii = {'colors': color_map_pii}\n",
    "\n",
    "def visualize_label(nlp, doc, tokens, labels, options = None):\n",
    "    global span_infos, doc_spans\n",
    "    \n",
    "    start_pos = -1\n",
    "    span_infos = []\n",
    "    for label_index, label in enumerate(labels):\n",
    "        if label!= 'O':\n",
    "            start_pos = label_index\n",
    "            end_pos = start_pos + 1\n",
    "            span_dict = {'start_pos':start_pos, 'end_pos':end_pos, 'label':label}\n",
    "            span_infos.append(span_dict)\n",
    "\n",
    "    doc_spans = []\n",
    "    doc = Doc(nlp.vocab, words=tokens)\n",
    "    \n",
    "    for span_info in span_infos:\n",
    "        _span = Span(doc, span_info['start_pos'], span_info['end_pos'], span_info['label'])\n",
    "        doc_spans.append(_span)\n",
    "\n",
    "    doc.spans['sc'] = doc_spans\n",
    "    displacy.render(doc, style = 'span', options = options)\n",
    "#     displacy.render(doc, style = 'span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9faebc85-7c78-4a87-aab1-13815c75b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio(tokens, labels):\n",
    "    bio_labels = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == 'O':\n",
    "            bio_labels.append(label)\n",
    "            current_entity = None\n",
    "        else:\n",
    "            if current_entity == label:\n",
    "                bio_labels.append('I-' + label)\n",
    "            else:\n",
    "                bio_labels.append('B-' + label)\n",
    "            current_entity = label\n",
    "    \n",
    "    return bio_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983b7b0-428d-472e-af36-8020ca054ff9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f5ff241-1bc7-4d0a-a7f1-ab5d2e6c0a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.48 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_adapter = Yanis_Adapter()\n",
    "# text = text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b9409c-b700-4bed-84c7-08b030a11482",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train.loc[0].full_text\n",
    "labels_true = df_train.loc[0].labels\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adaf5b5-d3e7-42ea-a90c-844ca156465a",
   "metadata": {},
   "source": [
    "## Inference (single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370a3c8-d84d-4087-94c7-668ea0ed2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(text, labels, model_adapter, threshold=0.1):\n",
    "    \n",
    "    global pred_bio, word_ids, word_labels, token_v, pred_v, token_labels\n",
    "    \n",
    "    model_name = model_adapter.model_name\n",
    "    model_id2cur_label = model_adapter.model_id2cur_label\n",
    "\n",
    "    tokenizer = model_adapter.tokenizer\n",
    "    model = model_adapter.model\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Step 1: Tokenize the text using spaCy for words\n",
    "    words = [token.text for token in doc]\n",
    "\n",
    "    # Step 2: Tokenize the text using the model's tokenizer and get word to subword mapping\n",
    "    inputs = tokenizer(words, return_tensors='pt', return_offsets_mapping=False, is_split_into_words=True)\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    # Step 3: Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "\n",
    "    label_probabilities = torch.softmax(outputs, dim=2)[0].cpu().numpy()\n",
    "    tokens = inputs.tokens()\n",
    "\n",
    "    # Get highest non-'O' labels for each token with thresholding\n",
    "    token_labels = pos_threshold_label(tokens, label_probabilities, model.config.id2label, threshold)\n",
    "\n",
    "    pred_labels = np.array([label for token,label in token_labels])\n",
    "\n",
    "    # Step 4: Create a list of which tokens or subwords correspond to a word using the word_ids variable\n",
    "    word_subword_mapping = {}\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is not None:\n",
    "            if word_id not in word_subword_mapping:\n",
    "                word_subword_mapping[word_id] = []\n",
    "            word_subword_mapping[word_id].append(i)\n",
    "\n",
    "    # Step 5: Iterate through pairs of words and subwords to count the majority label\n",
    "    word_labels = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i in word_subword_mapping:\n",
    "            subword_labels = pred_labels[word_subword_mapping[i]]\n",
    "            majority_label = get_majority(subword_labels)\n",
    "            word_labels.append((word, majority_label))\n",
    "        else:\n",
    "            word_labels.append((word,'O'))\n",
    "\n",
    "\n",
    "    token_v, pred_v = zip(*word_labels)\n",
    "\n",
    "    pred_conv = [model_id2cur_label[model.config.label2id[pred]] for pred in pred_v]\n",
    "    pred_bio = convert_to_bio(token_v, pred_conv)\n",
    "\n",
    "    f_beta = fbeta_score(labels, pred_bio, labels = classes_pos,beta=5, average='micro')\n",
    "    print(\"F-beta score:\", f_beta)\n",
    "    \n",
    "    return f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1ba6fd21-c07d-4f1b-bfa9-ef8d6a6af915",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 'O'),\n",
       " ('▁Design', 'O'),\n",
       " ('▁Thinking', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁innovation', 'O'),\n",
       " ('▁reflex', 'O'),\n",
       " ('ion', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁Avril', 'O'),\n",
       " ('▁2021', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁Nathalie', 'Name'),\n",
       " ('▁S', 'Name'),\n",
       " ('ylla', 'Name'),\n",
       " ('▁Challenge', 'O'),\n",
       " ('▁&', 'O'),\n",
       " ('▁selection', 'O'),\n",
       " ('▁The', 'O'),\n",
       " ('▁tool', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁use', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁help', 'O'),\n",
       " ('▁all', 'O'),\n",
       " ('▁stakeholders', 'O'),\n",
       " ('▁finding', 'O'),\n",
       " ('▁their', 'O'),\n",
       " ('▁way', 'O'),\n",
       " ('▁through', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁complexity', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁project', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁What', 'O'),\n",
       " ('▁exactly', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁?', 'O'),\n",
       " ('▁According', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁definition', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁Buz', 'Name'),\n",
       " ('an', 'Name'),\n",
       " ('▁T', 'O'),\n",
       " ('.', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁Buz', 'Name'),\n",
       " ('an', 'Name'),\n",
       " ('▁B', 'Name'),\n",
       " ('.', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁1999', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁Des', 'O'),\n",
       " ('s', 'O'),\n",
       " ('ine', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁moi', 'O'),\n",
       " ('▁l', 'O'),\n",
       " (\"'\", 'O'),\n",
       " ('intelligence', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Paris', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁Les', 'O'),\n",
       " ('▁É', 'O'),\n",
       " ('dition', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁d', 'O'),\n",
       " (\"'\", 'O'),\n",
       " ('Organ', 'O'),\n",
       " ('isation', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁or', 'O'),\n",
       " ('▁heuristic', 'O'),\n",
       " ('▁diagram', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁graphic', 'O'),\n",
       " ('▁representation', 'O'),\n",
       " ('▁technique', 'O'),\n",
       " ('▁that', 'O'),\n",
       " ('▁follows', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁natural', 'O'),\n",
       " ('▁functioning', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁allows', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁brain', 'O'),\n",
       " (\"▁'\", 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁potential', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁be', 'O'),\n",
       " ('▁released', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Cf', 'O'),\n",
       " ('▁Annex', 'O'),\n",
       " ('1', 'O'),\n",
       " ('▁This', 'O'),\n",
       " ('▁tool', 'O'),\n",
       " ('▁has', 'O'),\n",
       " ('▁many', 'O'),\n",
       " ('▁advantages', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁accessible', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁all', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁does', 'O'),\n",
       " ('▁not', 'O'),\n",
       " ('▁require', 'O'),\n",
       " ('▁significant', 'O'),\n",
       " ('▁material', 'O'),\n",
       " ('▁investment', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁can', 'O'),\n",
       " ('▁be', 'O'),\n",
       " ('▁done', 'O'),\n",
       " ('▁quickly', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁scalable', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁allows', 'O'),\n",
       " ('▁categorization', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁linking', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁information', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁can', 'O'),\n",
       " ('▁be', 'O'),\n",
       " ('▁applied', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁any', 'O'),\n",
       " ('▁type', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁situation', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁note', 'O'),\n",
       " ('taking', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁problem', 'O'),\n",
       " ('▁solving', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁analysis', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁creation', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁new', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁suitable', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁all', 'O'),\n",
       " ('▁people', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁easy', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁learn', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁fun', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁encourages', 'O'),\n",
       " ('▁exchanges', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁makes', 'O'),\n",
       " ('▁visible', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁dimension', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁projects', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁opportunities', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁interconnections', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁synthesize', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁makes', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁project', 'O'),\n",
       " ('▁understandable', 'O'),\n",
       " ('▁•', 'O'),\n",
       " ('▁It', 'O'),\n",
       " ('▁allows', 'O'),\n",
       " ('▁you', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁explore', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁The', 'O'),\n",
       " ('▁creation', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁starts', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁an', 'O'),\n",
       " ('▁idea', 'O'),\n",
       " ('▁/', 'O'),\n",
       " ('▁problem', 'O'),\n",
       " ('▁located', 'O'),\n",
       " ('▁at', 'O'),\n",
       " ('▁its', 'O'),\n",
       " ('▁center', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁This', 'O'),\n",
       " ('▁starting', 'O'),\n",
       " ('▁point', 'O'),\n",
       " ('▁generates', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁/', 'O'),\n",
       " ('▁work', 'O'),\n",
       " ('▁areas', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁incremented', 'O'),\n",
       " ('▁around', 'O'),\n",
       " ('▁this', 'O'),\n",
       " ('▁center', 'O'),\n",
       " ('▁in', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁radial', 'O'),\n",
       " ('▁structure', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁which', 'O'),\n",
       " ('▁in', 'O'),\n",
       " ('▁turn', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁completed', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁as', 'O'),\n",
       " ('▁many', 'O'),\n",
       " ('▁branches', 'O'),\n",
       " ('▁as', 'O'),\n",
       " ('▁new', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁This', 'O'),\n",
       " ('▁tool', 'O'),\n",
       " ('▁enables', 'O'),\n",
       " ('▁creativity', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁logic', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁be', 'O'),\n",
       " ('▁mobilized', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁it', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁thoughts', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Creativity', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁enhanced', 'O'),\n",
       " ('▁because', 'O'),\n",
       " ('▁participants', 'O'),\n",
       " ('▁feel', 'O'),\n",
       " ('▁comfortable', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁method', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Application', 'O'),\n",
       " ('▁&', 'O'),\n",
       " ('▁Insight', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁start', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁process', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁creation', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁stakeholders', 'O'),\n",
       " ('▁standing', 'O'),\n",
       " ('▁around', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁large', 'O'),\n",
       " ('▁board', 'O'),\n",
       " ('▁(', 'O'),\n",
       " ('▁white', 'O'),\n",
       " ('▁or', 'O'),\n",
       " ('▁paper', 'O'),\n",
       " ('▁board', 'O'),\n",
       " ('▁)', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁In', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁center', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁board', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁write', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁highlight', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁topic', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁design', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Through', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁series', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁questions', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁guide', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁stakeholders', 'O'),\n",
       " ('▁in', 'O'),\n",
       " ('▁modelling', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁adapt', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁series', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁questions', 'O'),\n",
       " ('▁according', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁topic', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁be', 'O'),\n",
       " ('▁addressed', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁In', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁type', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁questions', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁we', 'O'),\n",
       " ('▁can', 'O'),\n",
       " ('▁use', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁who', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁what', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁when', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁where', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁why', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁how', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁how', 'O'),\n",
       " ('▁much', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁The', 'O'),\n",
       " ('▁use', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁“', 'O'),\n",
       " ('▁why', 'O'),\n",
       " ('▁', 'O'),\n",
       " ('”', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁very', 'O'),\n",
       " ('▁interesting', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁understand', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁origin', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁By', 'O'),\n",
       " ('▁this', 'O'),\n",
       " ('▁way', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁interviewed', 'O'),\n",
       " ('▁person', 'O'),\n",
       " ('▁free', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁itself', 'O'),\n",
       " ('▁from', 'O'),\n",
       " ('▁paradigms', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁thus', 'O'),\n",
       " ('▁dares', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁propose', 'O'),\n",
       " ('▁new', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁/', 'O'),\n",
       " ('▁ways', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁functioning', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁plan', 'O'),\n",
       " ('▁two', 'O'),\n",
       " ('▁hours', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁workshop', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Design', 'O'),\n",
       " ('▁Thinking', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁innovation', 'O'),\n",
       " ('▁reflex', 'O'),\n",
       " ('ion', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁Avril', 'O'),\n",
       " ('▁2021', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁Nathalie', 'Name'),\n",
       " ('▁S', 'Name'),\n",
       " ('ylla', 'Name'),\n",
       " ('▁After', 'O'),\n",
       " ('▁modelling', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁on', 'O'),\n",
       " ('▁paper', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁propose', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁participants', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁digital', 'O'),\n",
       " ('▁visualization', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁their', 'O'),\n",
       " ('▁work', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁addition', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁color', 'O'),\n",
       " ('▁codes', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁images', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁interconnections', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁This', 'O'),\n",
       " ('▁second', 'O'),\n",
       " ('▁workshop', 'O'),\n",
       " ('▁also', 'O'),\n",
       " ('▁last', 'O'),\n",
       " ('s', 'O'),\n",
       " ('▁two', 'O'),\n",
       " ('▁hours', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁allows', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁evolve', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Once', 'O'),\n",
       " ('▁familiarized', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁it', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁stakeholders', 'O'),\n",
       " ('▁discover', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁power', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁tool', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Then', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁second', 'O'),\n",
       " ('▁workshop', 'O'),\n",
       " ('▁brings', 'O'),\n",
       " ('▁out', 'O'),\n",
       " ('▁even', 'O'),\n",
       " ('▁more', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁constructive', 'O'),\n",
       " ('▁exchanges', 'O'),\n",
       " ('▁between', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁stakeholders', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Around', 'O'),\n",
       " ('▁this', 'O'),\n",
       " ('▁new', 'O'),\n",
       " ('▁mind', 'O'),\n",
       " ('▁map', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁they', 'O'),\n",
       " ('▁have', 'O'),\n",
       " ('▁learned', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁work', 'O'),\n",
       " ('▁together', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁want', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁make', 'O'),\n",
       " ('▁visible', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁untold', 'O'),\n",
       " ('▁ideas', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁now', 'O'),\n",
       " ('▁present', 'O'),\n",
       " ('▁all', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁projects', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁manage', 'O'),\n",
       " ('▁in', 'O'),\n",
       " ('▁this', 'O'),\n",
       " ('▁type', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁format', 'O'),\n",
       " ('▁in', 'O'),\n",
       " ('▁order', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁ease', 'O'),\n",
       " ('▁rapid', 'O'),\n",
       " ('▁understanding', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁decision', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁makers', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁These', 'O'),\n",
       " ('▁presentations', 'O'),\n",
       " ('▁are', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁core', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁my', 'O'),\n",
       " ('▁business', 'O'),\n",
       " ('▁models', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁The', 'O'),\n",
       " ('▁decision', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁makers', 'O'),\n",
       " ('▁are', 'O'),\n",
       " ('▁thus', 'O'),\n",
       " ('▁able', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁identify', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁opportunities', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁projects', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁can', 'O'),\n",
       " ('▁take', 'O'),\n",
       " ('▁quick', 'O'),\n",
       " ('▁decisions', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁validate', 'O'),\n",
       " ('▁them', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁They', 'O'),\n",
       " ('▁find', 'O'),\n",
       " ('▁answers', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁their', 'O'),\n",
       " ('▁questions', 'O'),\n",
       " ('▁thank', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁a', 'O'),\n",
       " ('▁schematic', 'O'),\n",
       " ('▁representation', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Approach', 'O'),\n",
       " ('▁What', 'O'),\n",
       " ('▁I', 'O'),\n",
       " ('▁find', 'O'),\n",
       " ('▁amazing', 'O'),\n",
       " ('▁with', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁facilitation', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁this', 'O'),\n",
       " ('▁type', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁workshop', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁participants', 'O'),\n",
       " ('▁commitment', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁project', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁This', 'O'),\n",
       " ('▁tool', 'O'),\n",
       " ('▁helps', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁give', 'O'),\n",
       " ('▁meaning', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁The', 'O'),\n",
       " ('▁participants', 'O'),\n",
       " ('▁appropriate', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁story', 'O'),\n",
       " ('▁and', 'O'),\n",
       " ('▁want', 'O'),\n",
       " ('▁to', 'O'),\n",
       " ('▁keep', 'O'),\n",
       " ('▁writing', 'O'),\n",
       " ('▁it', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Then', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁they', 'O'),\n",
       " ('▁easily', 'O'),\n",
       " ('▁become', 'O'),\n",
       " ('▁actors', 'O'),\n",
       " ('▁or', 'O'),\n",
       " ('▁sponsors', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁project', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁A', 'O'),\n",
       " ('▁trust', 'O'),\n",
       " ('▁relationship', 'O'),\n",
       " ('▁is', 'O'),\n",
       " ('▁built', 'O'),\n",
       " ('▁,', 'O'),\n",
       " ('▁thus', 'O'),\n",
       " ('▁facilitating', 'O'),\n",
       " ('▁the', 'O'),\n",
       " ('▁implementation', 'O'),\n",
       " ('▁of', 'O'),\n",
       " ('▁related', 'O'),\n",
       " ('▁actions', 'O'),\n",
       " ('▁.', 'O'),\n",
       " ('▁Design', 'O'),\n",
       " ('▁Thinking', 'O'),\n",
       " ('▁for', 'O'),\n",
       " ('▁innovation', 'O'),\n",
       " ('▁reflex', 'O'),\n",
       " ('ion', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁Avril', 'O'),\n",
       " ('▁2021', 'O'),\n",
       " ('▁-', 'O'),\n",
       " ('▁Nathalie', 'Name'),\n",
       " ('▁S', 'Name'),\n",
       " ('ylla', 'Name'),\n",
       " ('▁Annex', 'O'),\n",
       " ('▁1', 'O'),\n",
       " ('▁:', 'O'),\n",
       " ('▁Mind', 'O'),\n",
       " ('▁Map', 'O'),\n",
       " ('▁Shared', 'O'),\n",
       " ('▁facilities', 'O'),\n",
       " ('▁project', 'O'),\n",
       " ('[SEP]', 'O')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ca7e2-16b9-4e22-be91-1c0d5ff0a69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa6ce536-449c-4ba1-a4e0-c3b1b86a7925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-beta score: 0.968944099378882\n",
      "CPU times: total: 50.3 s\n",
      "Wall time: 9.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.968944099378882"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_inference(text, labels_true, model_adapter, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8be110d8-6d88-4a97-8eca-582716b3dde0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(text, labels, model_adapter, threshold)\u001b[0m\n\u001b[0;32m     26\u001b[0m tokens \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtokens()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get highest non-'O' labels for each token with thresholding\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m token_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_labels\u001b[49m(tokens, label_probabilities, model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label, threshold)\n\u001b[0;32m     31\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([label \u001b[38;5;28;01mfor\u001b[39;00m token,label \u001b[38;5;129;01min\u001b[39;00m token_labels])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Step 4: Create a list of which tokens or subwords correspond to a word using the word_ids variable\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_labels' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_inference(text, labels_true, model_adapter, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d296e80d-8b84-48ad-86cf-7d50a1fcc74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-beta score: 0.968944099378882\n",
      "CPU times: total: 6min 49s\n",
      "Wall time: 4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.968944099378882"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_inference(text, labels_true, model_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e218e-f52b-4613-a0dc-c67be7762428",
   "metadata": {},
   "source": [
    "### dEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abe2ee34-873d-44a2-b9e9-caf204126333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33891e94-d574-4a07-9aaa-cbcab7dc3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.09 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = model_adapter.model_name\n",
    "to_cur = model_adapter.to_cur\n",
    "\n",
    "tokenizer = model_adapter.tokenizer\n",
    "model = model_adapter.model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3407fafa-2c04-4725-b810-cd9eed297f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 79.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Step 1: Tokenize the text using spaCy for words\n",
    "words = [token.text for token in doc]\n",
    "\n",
    "# Step 2: Tokenize the text using the model's tokenizer and get word to subword mapping\n",
    "inputs = tokenizer(words, return_tensors='pt', return_offsets_mapping=False, is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b37d5357-9d53-4eb3-ac09-2a861e71bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 44.4 s\n",
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 3: Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44764e-de77-4f82-a611-6d5eb2c9c161",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ed3960b-dd02-4aa1-8c53-fef531b13f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-NAME_STUDENT'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bio[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0bc6736-94fb-4202-9c33-bdcf922bdfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1030a56-c346-4a03-9295-d592877cf757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 B-NAME_STUDENT O\n",
      "53 I-NAME_STUDENT O\n",
      "54 I-NAME_STUDENT O\n",
      "55 I-NAME_STUDENT O\n",
      "56 I-NAME_STUDENT O\n"
     ]
    }
   ],
   "source": [
    "for i, (pred,label) in enumerate(zip(pred_bio,labels_true)):\n",
    "    if pred != label:\n",
    "        print(i, pred,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403910d6-5a3d-4cb0-901b-95b13aadb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_comp, labels_true_comp = zip(*[(pred,label) for pred,label in zip(pred_bio,labels_true) if (label != 'O' or pred != 'O' or label != pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c43b9d6-2bfd-49a4-99fc-5aa92fcb2e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_comp[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85fd3789-dd11-433c-a1da-c2e8e4a34fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT',\n",
       " 'B-NAME_STUDENT',\n",
       " 'I-NAME_STUDENT')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true_comp[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e453bed-5d60-45d1-bec3-06a9705ad264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-EMAIL',\n",
       " 'B-ID_NUM',\n",
       " 'B-NAME_STUDENT',\n",
       " 'B-PHONE_NUM',\n",
       " 'B-STREET_ADDRESS',\n",
       " 'B-URL_PERSONAL',\n",
       " 'B-USERNAME',\n",
       " 'I-ID_NUM',\n",
       " 'I-NAME_STUDENT',\n",
       " 'I-PHONE_NUM',\n",
       " 'I-STREET_ADDRESS',\n",
       " 'I-URL_PERSONAL']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aab1ab45-c1e6-46a9-8973-4bc5280ca770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968944099378882"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(labels_true_comp, pred_comp , labels = classes_pos,beta=5, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6851c807-2709-40f3-9fda-bc2e7bfe12e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968944099378882"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(labels_true, pred_bio, labels = classes_pos,beta=5, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8ba5f0c-d890-4670-9fae-cd71f2c5ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbeta_score_multiclass(y_true, y_pred, beta=5):\n",
    "    '''\n",
    "    Parameters:\n",
    "    - y_true: ground truth labels (shape: (batch_size))\n",
    "    - y_pred: predicted labels (shape: (batch_size))\n",
    "    - beta: beta value for F-beta score\n",
    "\n",
    "    Returns:\n",
    "    - float: micro F-beta multiclass score disregarding the background class (class 0)\n",
    "    '''\n",
    "    # Calculate FNFP both\n",
    "    FNFP = torch.sum((y_pred != 0) & (y_true != 0) & (y_pred != y_true))\n",
    "    # Calculate FP\n",
    "    FP = torch.sum((y_pred != 0) & (y_true == 0)) + FNFP\n",
    "    # Calculate FN\n",
    "    FN = torch.sum((y_pred == 0) & (y_true != 0)) + FNFP\n",
    "    # Calculate TP\n",
    "    TP = torch.sum((y_pred != 0) & (y_true != 0) & (y_pred == y_true))\n",
    "    \n",
    "    # Calculate F-beta score\n",
    "    fbeta = (1 + beta**2) * TP / ((1 + beta**2) * TP + beta**2 * FN + FP)\n",
    "    return fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a223670-d76b-4fb5-9dc2-28a5831ebb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_torch = torch.Tensor([classes2id[label] for label in labels_true])\n",
    "pred_torch = torch.Tensor([classes2id[pred] for pred in pred_bio])\n",
    "\n",
    "labels_comp_t = torch.Tensor([classes2id[label] for label in labels_true_comp])\n",
    "pred_comp_t = torch.Tensor([classes2id[pred] for pred in pred_comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04a8a84d-f546-47f9-90ae-1cc39c7d5266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9689)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fbeta_score_multiclass(labels_torch, pred_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fb3aefe-8bbb-4c3b-8671-529d601ec7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9689)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fbeta_score_multiclass(labels_comp_t, pred_comp_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c33871-1b0c-4ad8-b7f2-2c3f6bf946fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b0bb39-a3b6-43b8-9781-f6a6dc9e85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Postponed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba61fc3-7611-469d-84a9-81a7c58c57d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096dd0aa-b64c-4490-ac58-70b981be8708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179546f1-0100-4765-8133-48f8a26e682e",
   "metadata": {},
   "source": [
    "# Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f17c0-fd21-4142-9d90-e698c3144a7e",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c4ed267-e823-4df0-aad2-99ae27d967b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.33 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_adapter = Yanis_Adapter(threshold = 0.1)\n",
    "# text = text_test\n",
    "# text = df_train.loc[0].full_text\n",
    "# labels_true = df_train.loc[0].labels\n",
    "# threshold = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "132c75d9-af66-49d9-934a-6c22df390095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_adapter.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4f6a4-4c27-4a0b-8421-31d065c1004a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d718cf4-84d8-486b-901c-c90fc98201df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the Pandas DataFrame to a Hugging Face dataset\n",
    "# hf_dataset = Dataset.from_pandas(df_train.iloc[:100])\n",
    "hf_dataset = Dataset.from_pandas(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b2c48d0-a3d6-4cf2-bb64-cefa9b7fca11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bac6a-5ea4-4c98-ac80-191da41db9f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b770da94-3cd7-4ec1-9b5d-5bc2ddeb7343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def convert_to_features(sample):\n",
    "#     return {\n",
    "#         \"input_ids\": tokenizer(sample[\"text\"], return_tensors=\"pt\")[\"input_ids\"].squeeze(),\n",
    "#         \"attention_mask\": tokenizer(sample[\"text\"], return_tensors=\"pt\")[\"attention_mask\"].squeeze(),\n",
    "#         \"labels\": torch.tensor(sample[\"labels\"])\n",
    "#     }\n",
    "\n",
    "# # Convert dataset to features\n",
    "# features = [convert_to_features(sample) for sample in hf_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f2d00-12ba-4a55-9bca-3b63445037e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9661b47-a90d-4688-82cd-63741ec7643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 6807/6807 [01:34<00:00, 71.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 36s\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def tokenize_function(example, model_adapter):\n",
    "    global labels_aligned\n",
    "    \n",
    "    tokenizer = model_adapter.tokenizer\n",
    "    cur_label2model_id = model_adapter.cur_label2model_id\n",
    "    \n",
    "    inputs = tokenizer(example[\"tokens\"], return_tensors='pt', return_offsets_mapping=False, is_split_into_words=True)\n",
    "    tokens = inputs.tokens()\n",
    "    word_ids = inputs.word_ids()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return {\n",
    "#         \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "#         \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "#         \"tokens\": tokens,\n",
    "#         \"words\": example[\"tokens\"],\n",
    "#         \"word_labels\": example[\"labels\"],\n",
    "#         \"word_ids\": inputs.word_ids()\n",
    "#     }\n",
    "\n",
    "    labels = align_words(tokens, example[\"tokens\"], word_ids, example[\"labels\"])\n",
    "    label_ids = [cur_label2model_id[label] for label in labels]\n",
    "    labels_aligned = labels\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"tokens\": tokens,\n",
    "        \"words\": example[\"tokens\"],\n",
    "        \"word_labels\": example[\"labels\"],\n",
    "        \"word_ids\": inputs.word_ids(),\n",
    "        \"labels\": torch.tensor(label_ids),\n",
    "    }\n",
    "\n",
    "tokenized_datasets = hf_dataset.map(tokenize_function, fn_kwargs={\"model_adapter\": model_adapter}, batched=False)\n",
    "\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "# tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a982b-3cd7-4d90-b83c-d51e3fa0f46f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0edda06-6d8c-4223-a275-2f165f1a5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(hf_dataset))\n",
    "eval_size = len(hf_dataset) - train_size\n",
    "# train_dataset, eval_dataset = hf_dataset.train_test_split(test_size=eval_size)\n",
    "# list_remove = ['labels']\n",
    "# dataset_filtered = tokenized_datasets.remove_columns(list_remove)\n",
    "dataset_filtered = tokenized_datasets\n",
    "\n",
    "split_datasets = dataset_filtered.train_test_split(test_size=eval_size, seed = 42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d6b8b5f-c5a8-46dd-ad74-6ee9a0086dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'attention_mask', 'words', 'word_labels', 'word_ids'],\n",
       "    num_rows: 5445\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c927056c-9027-47c8-962a-bd6fbfbf1b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'attention_mask', 'words', 'word_labels', 'word_ids'],\n",
       "    num_rows: 1362\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85894d-be4a-49fd-be14-a87b0f52c86f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c374d4b1-959c-4984-a26f-22f42a77e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(model_adapter.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f7f0b-9d74-4337-bc14-0c54d6d15025",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric and Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb7fbe16-ffd4-4950-8045-46cec0843d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits(logits, label):\n",
    "    np_probs = torch.softmax(logits, axis=-1) \n",
    "    \n",
    "    return np_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d81a6a25-08e9-46ea-b3d2-9440e6bbafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b0ba7e6-a549-4b48-9a7b-23454e269288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thorn\\AppData\\Local\\Temp\\ipykernel_19544\\3877367123.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  METRIC_F1 = load_metric(\"f1\",  beta=5)\n",
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "METRIC_F1 = load_metric(\"f1\",  beta=5)\n",
    "METRIC_PRECISION = load_metric(\"precision\")\n",
    "METRIC_RECALL = load_metric(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b85095a7-275a-4bfb-bc85-fb07008f40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## (Vectorized) Get highest non-'O' labels for each token with thresholding\n",
    "    # Get 'O' index of model\n",
    "    # Get argsort\n",
    "    # Get First max\n",
    "    # Get == 'O' Mask\n",
    "    # Get threshold mask\n",
    "    # Get final indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19908ca7-5376-4b31-a920-0ef7b15162fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_base(eval_preds, model_adapter, threshold = 0.1):\n",
    "    \n",
    "    global eval_preds_copy\n",
    "    \n",
    "    \n",
    "    eval_preds_copy = eval_preds\n",
    "    \n",
    "    np_probs, true_labels_id, inputs = eval_preds\n",
    "#     logits, true_labels_id = eval_preds\n",
    "#     np_probs = np.argmax(logits, axis=-1)\n",
    "#     tokens = inputs.tokens()\n",
    "\n",
    "\n",
    "    label2id =  model_adapter.model.config.label2id\n",
    "    o_index = label2id['O']\n",
    "\n",
    "    np_sorted_indices = np.argsort(np_probs)\n",
    "    np_max_indices = np_sorted_indices[:,:, -1]\n",
    "    # np_max = np_sorted_indices[np.arange(np_max_indices.shape[0]), np_max_indices]\n",
    "    np_max_prob = np.take_along_axis(np_probs, np_max_indices[:, :, np.newaxis], axis=2).squeeze()\n",
    "\n",
    "    np_2nd_max_indices = np_sorted_indices[:, :, -2]\n",
    "    # np_2nd_max = np_sorted_indices[np.arange(np_sorted_indices.shape[0]), np_max_indices]\n",
    "    np_2nd_max_prob = np.take_along_axis(np_probs, np_2nd_max_indices[:, :, np.newaxis], axis=2).squeeze()\n",
    "\n",
    "    np_O_mask = np_max_indices == o_index\n",
    "    np_threshold_mask = np_2nd_max_prob > threshold\n",
    "\n",
    "    np_replace_mask = np_threshold_mask & np_O_mask\n",
    "\n",
    "    np_label_ids = np.where(np_replace_mask, np_2nd_max_indices, np_max_indices)\n",
    "\n",
    "    flat_label_ids = np_label_ids.flatten()\n",
    "    flat_true_labels_id = true_labels_id.flatten()\n",
    "\n",
    "    # Postprocess labels, convert irrelevant labels to 'O'\n",
    "    np_labels_irrelevant = np.array(model_adapter.labels_irrelevant)\n",
    "    flat_label_ids_mask = np.isin(flat_label_ids, np_labels_irrelevant)\n",
    "    flat_label_ids[flat_label_ids_mask] = o_index\n",
    "\n",
    "    # Also remove paddings\n",
    "    mask_padding_inv = flat_true_labels_id != -100\n",
    "    flat_true_labels_id = flat_true_labels_id[mask_padding_inv]\n",
    "    flat_label_ids = flat_label_ids[mask_padding_inv]\n",
    "\n",
    "    dict_scores = {}\n",
    "\n",
    "    f1_score = METRIC_F1.compute(predictions=flat_label_ids, references=flat_true_labels_id, labels=classes_pos_id, average ='micro')\n",
    "    f_beta_score = fbeta_score(y_true = flat_true_labels_id, y_pred = flat_label_ids , labels = classes_pos_id,beta=5, average='micro')\n",
    "    precision = METRIC_PRECISION.compute(predictions=flat_label_ids, references=flat_true_labels_id,labels=classes_pos_id, average ='micro')\n",
    "    recall = METRIC_RECALL.compute(predictions=flat_label_ids, references=flat_true_labels_id,labels=classes_pos_id, average ='micro')\n",
    "\n",
    "    for score in [f1_score, precision, recall]:\n",
    "        dict_scores.update(score) \n",
    "\n",
    "    dict_scores['f_beta'] = f_beta_score\n",
    "\n",
    "    return dict_scores\n",
    "\n",
    "\n",
    "#     #Test above first\n",
    "    \n",
    "    \n",
    "#     ##Compute competition metrics\n",
    "    \n",
    "#     # Step 4: Create a list of which tokens or subwords correspond to a word using the word_ids variable\n",
    "#     word_subword_mapping = {}\n",
    "#     for i, word_id in enumerate(word_ids):\n",
    "#         if word_id is not None:\n",
    "#             if word_id not in word_subword_mapping:\n",
    "#                 word_subword_mapping[word_id] = []\n",
    "#             word_subword_mapping[word_id].append(i)\n",
    "\n",
    "#     # Step 5: Iterate through pairs of words and subwords to count the majority label\n",
    "#     word_labels = []\n",
    "#     for i, word in enumerate(words):\n",
    "#         if i in word_subword_mapping:\n",
    "#             subword_labels = pred_labels[word_subword_mapping[i]]\n",
    "#             majority_label = get_majority(subword_labels)\n",
    "#             word_labels.append((word, majority_label))\n",
    "#         else:\n",
    "#             word_labels.append((word,'O'))\n",
    "\n",
    "\n",
    "#     token_v, pred_v = zip(*word_labels)\n",
    "\n",
    "#     pred_conv = [model_id2cur_label[model.config.label2id[pred]] for pred in pred_v]\n",
    "#     pred_bio = convert_to_bio(token_v, pred_conv)\n",
    "\n",
    "#     f_beta = fbeta_score(labels, pred_bio, labels = classes_pos,beta=5, average='micro')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    #Threshold + postprocessing\n",
    "    #\n",
    "    #Compute precision, recall, f1_beta from preds and labels\n",
    "    \n",
    "    #\n",
    "    \n",
    "    #Get \n",
    "    #Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3079f68-3f81-40e4-93de-8d7c4ec3239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = functools.partial(compute_metrics_base, model_adapter=model_adapter, threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899758d-8715-4bfb-8493-36e71cbd898c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22fd9a44-f13f-45ec-a6ca-324395f8e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f930932-9def-44f0-a989-e6a7c9ca9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c47a23e-bc08-4063-9920-94a15e901c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    per_device_eval_batch_size=4,    # batch size per device during evaluation\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "#     do_eval=True                     # Perform evaluation\n",
    "    include_inputs_for_metrics = True\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the evaluation only mode\n",
    "trainer = Trainer(\n",
    "    model=model_adapter.model,       # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    eval_dataset=eval_dataset,       # evaluation dataset\n",
    "    data_collator=data_collator,     # data collator for evaluation\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99093a1f-b368-4054-8029-444ba0bc8f7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_results)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\transformers\\trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3094\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3095\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3099\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3105\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\transformers\\trainer.py:3386\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3382\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3383\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3384\u001b[0m         )\n\u001b[0;32m   3385\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3386\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3388\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m, in \u001b[0;36mcompute_metrics_base\u001b[1;34m(eval_preds, model_adapter, threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m logits, true_labels_id \u001b[38;5;241m=\u001b[39m eval_preds\n\u001b[0;32m      7\u001b[0m np_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241m.\u001b[39mtokens()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m## (Vectorized) Get highest non-'O' labels for each token with thresholding\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get 'O' index of model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get argsort\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get threshold mask\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Get final indices\u001b[39;00m\n\u001b[0;32m     21\u001b[0m label2id \u001b[38;5;241m=\u001b[39m  model_adapter\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlabel2id\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# # Evaluate the model\n",
    "# eval_results = trainer.evaluate()\n",
    "\n",
    "# print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a15a14-4863-4b2c-bd7b-1411f4f12b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'attention_mask', 'words', 'word_labels', 'word_ids'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "337275b7-5943-45e6-b4e6-64414b221941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 14:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ compute_metrics called ------------------\n",
      "<class 'transformers.trainer_utils.EvalPrediction'>\n",
      "<transformers...001E5DA7C87A0>\n",
      "{'eval_loss': 8.329302787780762, 'eval_runtime': 72.6745, 'eval_samples_per_second': 0.028, 'eval_steps_per_second': 0.014}\n",
      "CPU times: total: 7min 4s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# eval_results = trainer.evaluate(eval_dataset.select(range(2)))\n",
    "\n",
    "# print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c7aa215-7a91-456e-a008-220153297a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 26min 18s\n",
      "Wall time: 4min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_results = trainer.predict(eval_dataset.select(range(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40d12a09-3508-42ac-a31a-85a4b58b1f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'attention_mask', 'words', 'word_labels', 'word_ids'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b2397-f1b8-4ee1-ac17-d91a6e5aa6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eac39d57-cf6f-4d4e-8627-263e0d372c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'attention_mask', 'words', 'word_labels', 'word_ids'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf1afa04-e7b9-4e5c-a05f-1bc4ff7a0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_start, cur_stop = (0,20)\n",
    "cur_dataset = tokenized_datasets.select(range(cur_start,cur_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cfbca-26e7-41e3-919c-120e5cca0a95",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80931e9a-29c2-45c4-8072-e3bdef9365c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Batch size 8 = 13m 16s\n",
    "#### Batch size 4 = 4min 54s\n",
    "#### Batch size 2 = 7min 41s\n",
    "#### Batch size 1 = 5min 28s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b6b6b5f-4a64-4f08-8a73-e80820809ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 41min 1s\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_results = trainer.predict(cur_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb67b54c-dc6f-45bb-8b19-d9c1d8870fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1929, 18)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc3e073e-4e5b-47c3-9b73-a8f00c07357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a693efd-4ff5-433f-81f3-55e87b74aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./results/preds_240227_{cur_start}_{cur_stop}', 'wb') as file:\n",
    "    # Use pickle to dump the object into the file\n",
    "    pickle.dump(pred_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f98fc-8f10-4162-9578-11179958770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dsad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7b8cef9-4d09-4791-8c9c-cc764d331350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.05136679485440254,\n",
       " 'test_f1': 0.6239316239316239,\n",
       " 'test_precision': 0.47096774193548385,\n",
       " 'test_recall': 0.9240506329113924,\n",
       " 'test_f_beta': 0.8910798122065727,\n",
       " 'test_runtime': 289.1268,\n",
       " 'test_samples_per_second': 0.069,\n",
       " 'test_steps_per_second': 0.01}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e15ea-2d50-40c1-af57-cc1314c5fde9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Old Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9a1cdce-1052-4655-801c-99f12735fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 648, 18)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a94a65f9-facc-4baa-9a91-404996a3f425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6239316239316239,\n",
       " 'precision': 0.47096774193548385,\n",
       " 'recall': 0.9240506329113924,\n",
       " 'f_beta': 0.8910798122065727}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics((pred_results.predictions, pred_results.label_ids, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4fd8613-3385-486b-ac9f-9ec3f9017f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb2e8b12-e292-4d45-83d9-5c018bf6ade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.trainer_utils.PredictionOutput"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b383e65b-aadd-438b-85da-25e4a464f69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.0730685442686081,\n",
       " 'test_f1': 0.4098939929328622,\n",
       " 'test_precision': 0.2636363636363636,\n",
       " 'test_recall': 0.9206349206349206,\n",
       " 'test_f_beta': 0.8401114206128134,\n",
       " 'test_runtime': 352.2587,\n",
       " 'test_samples_per_second': 0.057,\n",
       " 'test_steps_per_second': 0.009}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1db1c3c0-8a90-495b-9908-827dbb0e125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.0730685442686081,\n",
       " 'test_f1': 0.017912291537986413,\n",
       " 'test_precision': 0.009044129112739748,\n",
       " 'test_recall': 0.9206349206349206,\n",
       " 'test_runtime': 305.392,\n",
       " 'test_samples_per_second': 0.065,\n",
       " 'test_steps_per_second': 0.01}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ac664b9-7f6d-43df-9750-b1085610fa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.040113478899002075,\n",
       " 'test_f1': 0.028469750889679714,\n",
       " 'test_precision': 0.01444043321299639,\n",
       " 'test_recall': 1.0,\n",
       " 'test_runtime': 30.6181,\n",
       " 'test_samples_per_second': 0.065,\n",
       " 'test_steps_per_second': 0.033}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d8e32ac2-39fe-4a8f-85a3-a9c2beb0fb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 10.082626342773438,\n",
       " 'test_f1': 0.06493506493506493,\n",
       " 'test_precision': 0.03355704697986577,\n",
       " 'test_recall': 1.0,\n",
       " 'test_runtime': 33.6083,\n",
       " 'test_samples_per_second': 0.06,\n",
       " 'test_steps_per_second': 0.03}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4e089c91-8edd-4466-9229-7dca0504a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Create Array B (boolean mask)\n",
    "B = np.array([True, False, True, False, True])\n",
    "\n",
    "# Use boolean mask to select values from Array A where the mask is True\n",
    "selected_values = A[B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7bbc7e28-d59c-4471-b47d-8f2d332fb126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1056e98-e7ee-4662-a0c9-c8a7304feb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 26min 36s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_results = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62639b95-1c05-4046-a60c-1eea65c48443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='205' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 10:42:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07608494907617569,\n",
       " 'eval_f1': 0.7619047619047619,\n",
       " 'eval_precision': 0.6153846153846154,\n",
       " 'eval_recall': 1.0,\n",
       " 'eval_f_beta': 0.9765258215962441,\n",
       " 'eval_runtime': 17.8009,\n",
       " 'eval_samples_per_second': 0.112,\n",
       " 'eval_steps_per_second': 0.056}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset.select(range(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f6bc9b1-548b-4930-9506-668947b754e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_runtime': 276.4769,\n",
       " 'test_samples_per_second': 0.072,\n",
       " 'test_steps_per_second': 0.011}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b09d5aa-de19-4e2d-8c5b-4a63d0a1671d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_runtime': 260.9532,\n",
       " 'test_samples_per_second': 0.077,\n",
       " 'test_steps_per_second': 0.011}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "105716fd-efb5-40e9-b70f-1416237baefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1acb309-5a0d-4fe4-89e6-8987dd8624f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b61eca5e-f3c2-4d47-aed2-6e34aa175a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "\n",
    "# Define your custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    {\"text\": \"This is a sample text.\", \"labels\": 0},\n",
    "    {\"text\": \"Another example here.\", \"labels\": 1}\n",
    "    # Add more samples as needed\n",
    "]\n",
    "\n",
    "# Initialize your custom dataset\n",
    "custom_dataset = CustomDataset(dataset)\n",
    "\n",
    "# Define a function to convert samples to features\n",
    "def convert_to_features(sample):\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(sample[\"text\"], return_tensors=\"pt\")[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": tokenizer(sample[\"text\"], return_tensors=\"pt\")[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": torch.tensor(sample[\"labels\"])\n",
    "    }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = YourTokenizer.from_pretrained(\"tokenizer_name\")\n",
    "\n",
    "# Convert dataset to features\n",
    "features = [convert_to_features(sample) for sample in custom_dataset]\n",
    "\n",
    "# Split the dataset into train and eval\n",
    "train_size = int(0.8 * len(features))\n",
    "eval_size = len(features) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(features, [train_size, eval_size])\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    per_device_eval_batch_size=8,    # batch size per device during evaluation\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    do_eval=True                     # Perform evaluation\n",
    ")\n",
    "\n",
    "# Initialize Trainer with the evaluation only mode\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    eval_dataset=eval_dataset,       # evaluation dataset\n",
    "    data_collator=data_collator      # data collator for evaluation\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd579a5-b638-4a70-8f9a-db14d04b49f6",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51ba47-2a78-4888-82c4-0f556d7d6772",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c53480-6c09-4bf3-8041-f8c6f225fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics:\n",
    "#Precision\n",
    "#Recall\n",
    "#f1_beta \n",
    "#f1_beta word-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c4c8f37c-ea1d-45ff-8239-76418fb4d64a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import reprlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21468e0-c2b8-416a-9136-5157b71356e0",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74152ee1-96b3-4646-b27e-43ce50898935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7979054e-a8fb-4b07-aea6-f476070337c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_callback(eval_preds):\n",
    "    print('------------------ compute_metrics called ------------------')\n",
    "    print(type(eval_preds))\n",
    "    print(reprlib.repr(eval_preds))\n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea189baf-ceb6-4e79-815e-df86095826fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metric(eval_preds):\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    probabilities = np.argmax(logits, axis=-1)\n",
    "    tokens = inputs.tokens()\n",
    "    \n",
    "    # Get highest non-'O' labels for each token with thresholding\n",
    "    token_labels = get_labels(tokens, label_probabilities, model.config.id2label, threshold)\n",
    "    pred_labels = np.array([label for token,label in token_labels])\n",
    "\n",
    "    dict_scores = {}\n",
    "    \n",
    "    f1_score = metric_f1.compute(predictions=preds_id, references=true_labels_id, labels=labels_pos_id, average ='micro')\n",
    "    precision = metric_f1.compute(predictions=preds_id, references=true_labels_id)\n",
    "    recall = metric_f1.compute(predictions=preds_id, references=true_labels_id)\n",
    "    \n",
    "    for score in [f1_score, precision, recall]:\n",
    "        dict_scores.update(score) \n",
    "\n",
    "    return dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d47e96ed-67c7-4fc4-97f9-6dd48897b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752aaba-8525-4201-9de1-73b783b7361a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Metric and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ec81a8e7-de4b-4f32-ad39-70811b6d2cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thorn\\AppData\\Local\\Temp\\ipykernel_14188\\3621208379.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  METRIC_F1 = load_metric(\"f1\",  beta=5)\n",
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# metric_f1 = load_metric(\"leslyarun/fbeta_score\", beta=5)\n",
    "METRIC_F1 = load_metric(\"f1\",  beta=5)\n",
    "METRIC_PRECISION = load_metric(\"precision\")\n",
    "METRIC_RECALL = load_metric(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6ab80a9e-dc4e-4364-bd74-aaa878414e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_id = [classes2id[pred] for pred in pred_bio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4af44862-5e95-47eb-9635-dab62de79d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_id =  [classes2id[label] for label in labels_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "71a2129b-09d1-4944-9879-a2969ab09fbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_pos_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlabels_pos_id\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels_pos_id' is not defined"
     ]
    }
   ],
   "source": [
    "labels_pos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d0e976d7-3dbb-4bfb-96ab-3ce31d6b681b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6972d1c7-2312-4ffb-a936-014db184929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b80e868c-7599-493c-8589-1a4913fdb19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7058823529411765}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRIC_F1.compute(predictions=preds_id, references=true_labels_id, labels=labels_pos_id, average ='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6601e7f5-e859-48e3-8b81-64920fdcf172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "94921b02-4635-4309-9d4b-6385abb7c615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chrf',\n",
       " 'confusion_matrix',\n",
       " 'f1',\n",
       " 'frugalscore',\n",
       " 'BucketHeadP65/confusion_matrix',\n",
       " 'DarrenChensformer/eval_keyphrase',\n",
       " 'DarrenChensformer/relation_extraction',\n",
       " 'LuckiestOne/valid_efficiency_score',\n",
       " 'Muennighoff/code_eval_octopack',\n",
       " 'SpfIo/wer_checker',\n",
       " 'Viona/fuzzy_reordering',\n",
       " 'Viona/infolm',\n",
       " 'angelina-wang/directional_bias_amplification',\n",
       " 'aryopg/roc_auc_skip_uniform_labels',\n",
       " 'bstrai/classification_report',\n",
       " 'danieldux/hierarchical_softmax_loss',\n",
       " 'dgfh76564/accents_unplugged_eval',\n",
       " 'erntkn/dice_coefficient',\n",
       " 'fnvls/bleu1234',\n",
       " 'fnvls/bleu_1234',\n",
       " 'franzi2505/detection_metric',\n",
       " 'fschlatt/ner_eval',\n",
       " 'gjacob/chrf',\n",
       " 'gorkaartola/metric_for_tp_fp_samples',\n",
       " 'kashif/mape',\n",
       " 'leslyarun/fbeta_score',\n",
       " 'maksymdolgikh/seqeval_with_fbeta',\n",
       " 'mfumanelli/geometric_mean',\n",
       " 'mgfrantz/roc_auc_macro',\n",
       " 'mtc/fragments',\n",
       " 'nevikw39/specificity',\n",
       " 'omidf/squad_precision_recall',\n",
       " 'red1bluelost/evaluate_genericify_cpp',\n",
       " 'ronaldahmed/nwentfaithfulness',\n",
       " 'transformersegmentation/segmentation_scores']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in list_metrics if 'f' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6ae0bdb9-6087-49af-a820-652c1f673c6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thorn\\AppData\\Local\\Temp\\ipykernel_15012\\3712819067.py:1: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  list_metrics()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'bertscore',\n",
       " 'bleu',\n",
       " 'bleurt',\n",
       " 'brier_score',\n",
       " 'cer',\n",
       " 'character',\n",
       " 'charcut_mt',\n",
       " 'chrf',\n",
       " 'code_eval',\n",
       " 'comet',\n",
       " 'competition_math',\n",
       " 'confusion_matrix',\n",
       " 'coval',\n",
       " 'cuad',\n",
       " 'exact_match',\n",
       " 'f1',\n",
       " 'frugalscore',\n",
       " 'glue',\n",
       " 'google_bleu',\n",
       " 'indic_glue',\n",
       " 'mae',\n",
       " 'mahalanobis',\n",
       " 'mape',\n",
       " 'mase',\n",
       " 'matthews_correlation',\n",
       " 'mauve',\n",
       " 'mean_iou',\n",
       " 'meteor',\n",
       " 'mse',\n",
       " 'nist_mt',\n",
       " 'pearsonr',\n",
       " 'perplexity',\n",
       " 'poseval',\n",
       " 'precision',\n",
       " 'r_squared',\n",
       " 'recall',\n",
       " 'rl_reliability',\n",
       " 'roc_auc',\n",
       " 'rouge',\n",
       " 'sacrebleu',\n",
       " 'sari',\n",
       " 'seqeval',\n",
       " 'smape',\n",
       " 'spearmanr',\n",
       " 'squad',\n",
       " 'squad_v2',\n",
       " 'super_glue',\n",
       " 'ter',\n",
       " 'trec_eval',\n",
       " 'wer',\n",
       " 'wiki_split',\n",
       " 'xnli',\n",
       " 'xtreme_s',\n",
       " 'Aledade/extraction_evaluation',\n",
       " 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
       " 'BucketHeadP65/confusion_matrix',\n",
       " 'BucketHeadP65/roc_curve',\n",
       " 'CZLC/rouge_raw',\n",
       " 'DaliaCaRo/accents_unplugged_eval',\n",
       " 'DarrenChensformer/eval_keyphrase',\n",
       " 'DarrenChensformer/relation_extraction',\n",
       " 'DoctorSlimm/bangalore_score',\n",
       " 'DoctorSlimm/kaushiks_criteria',\n",
       " 'Drunper/metrica_tesi',\n",
       " 'Felipehonorato/eer',\n",
       " 'Fritz02/execution_accuracy',\n",
       " 'GMFTBY/dailydialog_evaluate',\n",
       " 'GMFTBY/dailydialogevaluate',\n",
       " 'He-Xingwei/sari_metric',\n",
       " 'Ikala-allen/relation_extraction',\n",
       " 'JP-SystemsX/nDCG',\n",
       " 'Josh98/nl2bash_m',\n",
       " 'KevinSpaghetti/accuracyk',\n",
       " 'LottieW/accents_unplugged_eval',\n",
       " 'LuckiestOne/valid_efficiency_score',\n",
       " 'Merle456/accents_unplugged_eval',\n",
       " 'Muennighoff/code_eval_octopack',\n",
       " 'NCSOFT/harim_plus',\n",
       " 'Natooz/ece',\n",
       " 'Ndyyyy/bertscore',\n",
       " 'NikitaMartynov/spell-check-metric',\n",
       " 'NimaBoscarino/weat',\n",
       " 'Ochiroo/rouge_mn',\n",
       " 'Pipatpong/perplexity',\n",
       " 'Qui-nn/accents_unplugged_eval',\n",
       " 'RiciHuggingFace/accents_unplugged_eval',\n",
       " 'SEA-AI/det-metrics',\n",
       " 'SEA-AI/mot-metrics',\n",
       " 'Soroor/cer',\n",
       " 'SpfIo/wer_checker',\n",
       " 'Splend1dchan/cosine_similarity',\n",
       " 'TelEl/accents_unplugged_eval',\n",
       " 'Vallp/ter',\n",
       " 'Vertaix/vendiscore',\n",
       " 'Vickyage/accents_unplugged_eval',\n",
       " 'Viona/fuzzy_reordering',\n",
       " 'Viona/infolm',\n",
       " 'Viona/kendall_tau',\n",
       " 'Vipitis/shadermatch',\n",
       " 'Vlasta/pr_auc',\n",
       " 'Yeshwant123/mcc',\n",
       " 'abdusah/aradiawer',\n",
       " 'abidlabs/mean_iou',\n",
       " 'abidlabs/mean_iou2',\n",
       " 'agkphysics/ccc',\n",
       " 'akki2825/accents_unplugged_eval',\n",
       " 'alvinasvk/accents_unplugged_eval',\n",
       " 'amitness/perplexity',\n",
       " 'andstor/code_perplexity',\n",
       " 'angelasophie/accents_unplugged_eval',\n",
       " 'angelina-wang/directional_bias_amplification',\n",
       " 'anz2/iliauniiccocrevaluation',\n",
       " 'arthurvqin/pr_auc',\n",
       " 'aryopg/roc_auc_skip_uniform_labels',\n",
       " 'bascobasculino/mot-metrics',\n",
       " 'bdsaglam/jer',\n",
       " 'boschar/accents_unplugged_eval',\n",
       " 'brian920128/doc_retrieve_metrics',\n",
       " 'bstrai/classification_report',\n",
       " 'bugbounty1806/accuracy',\n",
       " 'cakiki/ndcg',\n",
       " 'carletoncognitivescience/peak_signal_to_noise_ratio',\n",
       " 'chanelcolgate/average_precision',\n",
       " 'chimene/accents_unplugged_eval',\n",
       " 'ckb/unigram',\n",
       " 'codeparrot/apps_metric',\n",
       " 'cpllab/syntaxgym',\n",
       " 'd-matrix/dmx_perplexity',\n",
       " 'daiyizheng/valid',\n",
       " 'danieldux/hierarchical_softmax_loss',\n",
       " 'dayil100/accents_unplugged_eval',\n",
       " 'dayil100/accents_unplugged_eval_WER',\n",
       " 'dgfh76564/accents_unplugged_eval',\n",
       " 'dvitel/codebleu',\n",
       " 'ecody726/bertscore',\n",
       " 'erntkn/dice_coefficient',\n",
       " 'fnvls/bleu1234',\n",
       " 'fnvls/bleu_1234',\n",
       " 'franzi2505/detection_metric',\n",
       " 'fschlatt/ner_eval',\n",
       " 'gabeorlanski/bc_eval',\n",
       " 'giulio98/code_eval_outputs',\n",
       " 'giulio98/codebleu',\n",
       " 'gjacob/bertimbauscore',\n",
       " 'gjacob/chrf',\n",
       " 'gjacob/google_bleu',\n",
       " 'gjacob/wiki_split',\n",
       " 'gnail/cosine_similarity',\n",
       " 'gorkaartola/metric_for_tp_fp_samples',\n",
       " 'guydav/restrictedpython_code_eval',\n",
       " 'hack/test_metric',\n",
       " 'harshhpareek/bertscore',\n",
       " 'hpi-dhc/FairEval',\n",
       " 'huanghuayu/multiclass_brier_score',\n",
       " 'hynky/sklearn_proxy',\n",
       " 'hyperml/balanced_accuracy',\n",
       " 'iNeil77/code_eval_octopack',\n",
       " 'idsedykh/codebleu',\n",
       " 'idsedykh/codebleu2',\n",
       " 'idsedykh/megaglue',\n",
       " 'idsedykh/metric',\n",
       " 'illorca/FairEval',\n",
       " 'ingyu/klue_mrc',\n",
       " 'jialinsong/apps_metric',\n",
       " 'jjkim0807/code_eval',\n",
       " 'jordyvl/ece',\n",
       " 'jpxkqx/peak_signal_to_noise_ratio',\n",
       " 'jpxkqx/signal_to_reconstruction_error',\n",
       " 'juliakaczor/accents_unplugged_eval',\n",
       " 'jzm-mailchimp/joshs_second_test_metric',\n",
       " 'k4black/codebleu',\n",
       " 'kashif/mape',\n",
       " 'kedudzic/charmatch',\n",
       " 'kyokote/my_metric2',\n",
       " 'langdonholmes/cohen_weighted_kappa',\n",
       " 'leslyarun/fbeta_score',\n",
       " 'lhy/hamming_loss',\n",
       " 'lhy/ranking_loss',\n",
       " 'livvie/accents_unplugged_eval',\n",
       " 'loubnabnl/apps_metric2',\n",
       " 'lvwerra/accuracy_score',\n",
       " 'lvwerra/bary_score',\n",
       " 'lvwerra/test',\n",
       " 'maksymdolgikh/seqeval_with_fbeta',\n",
       " 'manueldeprada/beer',\n",
       " 'mfumanelli/geometric_mean',\n",
       " 'mgfrantz/roc_auc_macro',\n",
       " 'mtc/fragments',\n",
       " 'nevikw39/specificity',\n",
       " 'nlpln/tst',\n",
       " 'ola13/precision_at_k',\n",
       " 'omidf/squad_precision_recall',\n",
       " 'posicube/mean_reciprocal_rank',\n",
       " 'red1bluelost/evaluate_genericify_cpp',\n",
       " 'repllabs/mean_average_precision',\n",
       " 'repllabs/mean_reciprocal_rank',\n",
       " 'ronaldahmed/nwentfaithfulness',\n",
       " 'saicharan2804/my_metric',\n",
       " 'sakusakumura/bertscore',\n",
       " 'shalakasatheesh/squad',\n",
       " 'shalakasatheesh/squad_v2',\n",
       " 'shirayukikun/sescore',\n",
       " 'shunzh/apps_metric',\n",
       " 'sma2023/wil',\n",
       " 'sportlosos/sescore',\n",
       " 'transZ/sbert_cosine',\n",
       " 'transZ/test_parascore',\n",
       " 'transformersegmentation/segmentation_scores',\n",
       " 'unitxt/metric',\n",
       " 'unnati/kendall_tau_distance',\n",
       " 'vichyt/metric-codebleu',\n",
       " 'weiqis/pajm',\n",
       " 'xu1998hz/sescore',\n",
       " 'xu1998hz/sescore_english_coco',\n",
       " 'xu1998hz/sescore_english_mt',\n",
       " 'xu1998hz/sescore_english_webnlg',\n",
       " 'xu1998hz/sescore_german_mt',\n",
       " 'ybelkada/cocoevaluate',\n",
       " 'yonting/average_precision_score',\n",
       " 'yqsong/execution_accuracy',\n",
       " 'yulong-me/yl_metric',\n",
       " 'yuyijiong/quad_match_score',\n",
       " 'yzha/ctc_eval',\n",
       " 'zbeloki/m2']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71790c2-16de-4a5b-8c2c-a9845186a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1], dtype=int64), array([1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Define an equality condition\n",
    "condition = arr == 5\n",
    "\n",
    "# Use np.where() to get the indices where the condition is True\n",
    "indices = np.where(condition)\n",
    "\n",
    "# Print the indices\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "027525d6-3bdb-415a-bb4b-da3a827de73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[3, 7, 1],\n",
    "                [5, 2, 4]])\n",
    "\n",
    "# Perform argsort along axis 1 (sorting each row independently)\n",
    "sorted_indices = np.argsort(arr, axis=1)\n",
    "\n",
    "# Get the index of the nth smallest element along axis 1\n",
    "n = -1  # Change n to the desired value\n",
    "nth_smallest_index = sorted_indices[:, n]\n",
    "\n",
    "# Use fancy indexing to get the nth smallest element along axis 1\n",
    "nth_smallest_elements = arr[np.arange(arr.shape[0]), nth_smallest_index]\n",
    "nth_smallest_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b504aeb-3bf1-4437-98eb-4c3252603728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[3, 2, 4],\n",
    "                [1, 6, 5]])\n",
    "\n",
    "# Perform argsort along an axis (axis=1 in this example)\n",
    "sorted_indices = np.argsort(arr, axis=1)\n",
    "\n",
    "# Get the nth element along the axis\n",
    "n = 1  # Change this to the desired index\n",
    "nth_element = arr[np.arange(arr.shape[0])[:, np.newaxis], sorted_indices][:, n]\n",
    "\n",
    "print(nth_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "43ef8649-2941-4d04-854d-6a586ceee72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "520e4b49-a4c2-4bfc-8565-f53539d7d7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1, 3)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "array[:, :, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceb15831-0559-4a56-a4d3-62d112d31c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_smallest_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3a01970-388b-4758-8800-f7fb0129cb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64), array([2], dtype=int64))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(arr == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "051f1d4b-eb11-46d4-ba09-a616bfb885c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8f1b1-47b9-4159-81ce-e30ed6bda60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7249f1c-fad5-456c-a697-2e2b5173b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_f1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b3316-2114-4901-afa9-a72c9d293d86",
   "metadata": {},
   "source": [
    "### Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf70ea-8734-49f0-9730-14c104a9c26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0058d8dc-6d30-49aa-953a-fa9af79f5ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def compute_metrics_base(eval_preds, model_adapter, threshold = 0.1):\n",
    "    \n",
    "#     global eval_preds_copy\n",
    "#     eval_preds_copy = eval_preds\n",
    "    \n",
    "#     logits, true_labels_id = eval_preds\n",
    "#     np_probs = np.argmax(logits, axis=-1)\n",
    "#     tokens = inputs.tokens()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ## (Vectorized) Get highest non-'O' labels for each token with thresholding\n",
    "#     # Get 'O' index of model\n",
    "#     # Get argsort\n",
    "#     # Get First max\n",
    "#     # Get == 'O' Mask\n",
    "#     # Get threshold mask\n",
    "#     # Get final indices\n",
    "    \n",
    "#     label2id =  model_adapter.model.config.label2id\n",
    "#     o_index = label2id['O']\n",
    "    \n",
    "#     np_sorted_indices = np.argsort(np_probs)\n",
    "#     np_max_indices = np_sorted[:, -1]\n",
    "#     np_max = np_sorted[np.arange(arr.shape[0]), np_max_indices]\n",
    "    \n",
    "#     np_2nd_max_indices = p_sorted[:, -2]\n",
    "#     np_2nd_max = np_sorted[np.arange(arr.shape[0]), np_max_indices]\n",
    "    \n",
    "#     np_O_mask = np_max_indices == o_index\n",
    "#     np_threshold_mask = np_2nd_max > threshold\n",
    "    \n",
    "#     np_replace_mask = np_threshold_mask & np_O_mask\n",
    "    \n",
    "    \n",
    "#     #np_max_indices[np_replace_mask] = np_2nd_max_indices\n",
    "#     np_label_ids = np.where(np_replace_mask, np_max, np_2nd_max)\n",
    "    \n",
    "#     ##\n",
    "#     #Compute model metrics\n",
    "    \n",
    "#     #Define labels_pos_id in model_adapter base class (exclude O)\n",
    "    \n",
    "    \n",
    "#     dict_scores = {}\n",
    "    \n",
    "#     f1_score = metric_f1.compute(predictions=np_label_ids, references=true_labels_id, labels=labels_pos_id, average ='micro')\n",
    "#     precision = metric_f1.compute(predictions=np_label_ids, references=true_labels_id)\n",
    "#     recall = metric_f1.compute(predictions=np_label_ids, references=true_labels_id)\n",
    "    \n",
    "#     for score in [f1_score, precision, recall]:\n",
    "#         dict_scores.update(score) \n",
    "\n",
    "        \n",
    "\n",
    "#     return dict_scores\n",
    "\n",
    "\n",
    "# #     #Test above first\n",
    "    \n",
    "    \n",
    "# #     ##Compute competition metrics\n",
    "    \n",
    "# #     # Step 4: Create a list of which tokens or subwords correspond to a word using the word_ids variable\n",
    "# #     word_subword_mapping = {}\n",
    "# #     for i, word_id in enumerate(word_ids):\n",
    "# #         if word_id is not None:\n",
    "# #             if word_id not in word_subword_mapping:\n",
    "# #                 word_subword_mapping[word_id] = []\n",
    "# #             word_subword_mapping[word_id].append(i)\n",
    "\n",
    "# #     # Step 5: Iterate through pairs of words and subwords to count the majority label\n",
    "# #     word_labels = []\n",
    "# #     for i, word in enumerate(words):\n",
    "# #         if i in word_subword_mapping:\n",
    "# #             subword_labels = pred_labels[word_subword_mapping[i]]\n",
    "# #             majority_label = get_majority(subword_labels)\n",
    "# #             word_labels.append((word, majority_label))\n",
    "# #         else:\n",
    "# #             word_labels.append((word,'O'))\n",
    "\n",
    "\n",
    "# #     token_v, pred_v = zip(*word_labels)\n",
    "\n",
    "# #     pred_conv = [model_id2cur_label[model.config.label2id[pred]] for pred in pred_v]\n",
    "# #     pred_bio = convert_to_bio(token_v, pred_conv)\n",
    "\n",
    "# #     f_beta = fbeta_score(labels, pred_bio, labels = classes_pos,beta=5, average='micro')\n",
    "    \n",
    "    \n",
    "    \n",
    "# #     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "#     #Threshold + postprocessing\n",
    "#     #\n",
    "#     #Compute precision, recall, f1_beta from preds and labels\n",
    "    \n",
    "#     #\n",
    "    \n",
    "#     #Get \n",
    "#     #Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12b82730-6e19-4519-940d-1426a2fa90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "compute_metrics = functools.partial(compute_metrics_base, model_adapter=model_adapter, threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b75050-2ddc-4163-a25d-b41e6180bb55",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b732552-5102-402f-a370-616d0950089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_preds_copy.label_ids[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7ef56-2492-4b01-97d0-e5596bfad3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0cf65bf2-6993-4074-b972-d9d00cee1188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1, 51146,   377, ...,     0,     0,     0],\n",
       "       [    1, 69528,   463, ...,  1105,  1084,     2]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_preds_copy.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ed18e1d-af24-4e4d-a049-73885665d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits(logits, label):\n",
    "    np_probs = torch.softmax(logits, axis=-1) \n",
    "    \n",
    "    return np_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178e733-4a40-4738-943d-c5356467b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_probs = np.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "992997a5-fedc-4f1f-89f7-882b3836b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, logits, true_labels_id = eval_preds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "65772cef-d755-4492-afc0-83dd6d3ec30a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EvalPrediction' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[244], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43meval_preds_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EvalPrediction' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "eval_preds_copy.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d65e7d11-def0-46c7-8691-0b9f3bff9da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EvalPrediction' object has no attribute 'np_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43meval_preds_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_probs\u001b[49m\u001b[38;5;241m.\u001b[39mstart_index\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EvalPrediction' object has no attribute 'np_probs'"
     ]
    }
   ],
   "source": [
    "eval_preds_copy.np_probs.start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "611b72f9-5aee-40ac-bd83-8862329bb36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.5333333333333333,\n",
       " 'precision': 0.36363636363636365,\n",
       " 'recall': 1.0,\n",
       " 'f_beta': 0.9369369369369369}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hreshold=0.5\n",
    "\n",
    "np_probs, true_labels_id, inputs = eval_preds_copy\n",
    "\n",
    "# tokens = inputs.tokens()\n",
    "\n",
    "## (Vectorized) Get highest non-'O' labels for each token with thresholding\n",
    "# Get 'O' index of model\n",
    "# Get argsort\n",
    "# Get First max\n",
    "# Get == 'O' Mask\n",
    "# Get threshold mask\n",
    "# Get final indices\n",
    "\n",
    "label2id =  model_adapter.model.config.label2id\n",
    "o_index = label2id['O']\n",
    "\n",
    "np_sorted_indices = np.argsort(np_probs)\n",
    "np_max_indices = np_sorted_indices[:,:, -1]\n",
    "# np_max = np_sorted_indices[np.arange(np_max_indices.shape[0]), np_max_indices]\n",
    "np_max_prob = np.take_along_axis(np_probs, np_max_indices[:, :, np.newaxis], axis=2).squeeze()\n",
    "\n",
    "np_2nd_max_indices = np_sorted_indices[:, :, -2]\n",
    "# np_2nd_max = np_sorted_indices[np.arange(np_sorted_indices.shape[0]), np_max_indices]\n",
    "np_2nd_max_prob = np.take_along_axis(np_probs, np_2nd_max_indices[:, :, np.newaxis], axis=2).squeeze()\n",
    "\n",
    "np_O_mask = np_max_indices == o_index\n",
    "np_threshold_mask = np_2nd_max_prob > threshold\n",
    "\n",
    "np_replace_mask = np_threshold_mask & np_O_mask\n",
    "\n",
    "np_label_ids = np.where(np_replace_mask, np_2nd_max_indices, np_max_indices)\n",
    "\n",
    "flat_label_ids = np_label_ids.flatten()\n",
    "flat_true_labels_id = true_labels_id.flatten()\n",
    "\n",
    "# Postprocess labels, convert irrelevant labels to 'O'\n",
    "np_labels_irrelevant = np.array(model_adapter.labels_irrelevant)\n",
    "flat_label_ids_mask = np.isin(flat_label_ids, np_labels_irrelevant)\n",
    "flat_label_ids[flat_label_ids_mask] = o_index\n",
    "\n",
    "# Also remove paddings\n",
    "mask_padding_inv = flat_true_labels_id != -100\n",
    "flat_true_labels_id = flat_true_labels_id[mask_padding_inv]\n",
    "flat_label_ids = flat_label_ids[mask_padding_inv]\n",
    "\n",
    "dict_scores = {}\n",
    "\n",
    "f1_score = METRIC_F1.compute(predictions=flat_label_ids, references=flat_true_labels_id, labels=classes_pos_id, average ='micro')\n",
    "f_beta_score = fbeta_score(y_true = flat_true_labels_id, y_pred = flat_label_ids , labels = classes_pos_id,beta=5, average='micro')\n",
    "precision = METRIC_PRECISION.compute(predictions=flat_label_ids, references=flat_true_labels_id,labels=classes_pos_id, average ='micro')\n",
    "recall = METRIC_RECALL.compute(predictions=flat_label_ids, references=flat_true_labels_id,labels=classes_pos_id, average ='micro')\n",
    "\n",
    "input_id = eval_dataset['input_id']\n",
    "word_ids = eval_dataset['word_ids]\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for score in [f1_score, precision, recall]:\n",
    "    dict_scores.update(score) \n",
    "    \n",
    "dict_scores['f_beta'] = f_beta_score\n",
    "\n",
    "\n",
    "dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ce40af54-622c-40b1-bf9a-fa67f66919cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_true_labels_id[flat_true_labels_id == -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0202e8a5-9041-46d6-8ff5-e6925057a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_comp, labels_true_comp = zip(*[(pred,label) for pred,label in zip(list(flat_label_ids),list(flat_true_labels_id)) if (label != 0 or pred != 0 or label != pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e47f8ce-9c26-4ca9-9502-dd715ca4cfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36363636363636365"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(labels_true_comp, pred_comp , labels = classes_pos_id,beta=0, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d77d9a4d-44fd-42d5-8fda-16782ada7103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9369369369369369"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(labels_true_comp, pred_comp , labels = classes_pos_id,beta=5, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "78c88ab3-5cb8-4de3-b61b-aea485c7897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_comp = list(zip(pred_comp,labels_true_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f2e44af9-46bb-4ccc-a232-5f2b31aa1823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2),\n",
       " (2, 2),\n",
       " (2, 0),\n",
       " (2, 0),\n",
       " (12, 0),\n",
       " (12, 0),\n",
       " (12, 0),\n",
       " (12, 0),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 0)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "048af776-27b4-43b6-ba11-810d78d08a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a40ca570-1bf3-491b-a72b-2a632baa1651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comp[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8813ec21-b736-45d5-94d3-bfb467a53ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comp[2][0] == zip_comp[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a40a1dc-7b8b-420e-b418-5f6af1f2e54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "46f0d5f5-4097-46eb-9fc0-850dd6d81c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_comp[2][0] == zip_comp[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16514909-36f3-4bd8-a0fc-519dde33d55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "1859bc0e-5207-4393-9c0d-c457a1682b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 648)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6e0560bb-f9bd-439f-abbc-c0e9b9e8b07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 648, 18)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bd3fbeaa-c0fc-4cec-a0a5-0f4f3637596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.06493506493506493,\n",
       " 'precision': 0.0021240441801189465,\n",
       " 'recall': 0.0021240441801189465}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "220e81f8-3918-49c4-bb42-dd90b5d72935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adapter.O_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "dd64ddab-a96c-4f74-b821-0f22a688999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 10.082626342773438,\n",
       " 'test_f1': 0.06493506493506493,\n",
       " 'test_precision': 0.03355704697986577,\n",
       " 'test_recall': 1.0,\n",
       " 'test_runtime': 34.472,\n",
       " 'test_samples_per_second': 0.058,\n",
       " 'test_steps_per_second': 0.029}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2bd52afb-d63a-41f3-bc47-9ecc758e8982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2c151aa3-c2e8-4835-afbe-f9f2d4fb5306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.05847953216374269,\n",
       " 'precision': 0.0021240441801189465,\n",
       " 'recall': 0.0021240441801189465}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6cb1d791-8818-496f-b609-7273a640e4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  2])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([11,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b070c73f-3d08-41bf-b69b-4f3ebfdf8431",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[230], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokens\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "266b7863-5ae2-465c-9468-31dda203c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_true_labels_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "686dc6a3-5073-4225-aa31-5773c7da3e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_label_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "34efafc7-4b46-4a9b-a04c-be220ce9786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_probs, true_labels_id, inputs = eval_preds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "38e8c46b-77c7-4729-af21-5f5fb52248c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_id[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "fb9d2bbe-a53f-43d1-970d-4e1620207edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_true_labels_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c72a0958-890b-4aad-ba1a-9a1cb9cd8747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.05847953216374269,\n",
       " 'precision': 0.0021240441801189465,\n",
       " 'recall': 0.0021240441801189465}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "07f48e9a-339f-4e36-adf5-5d8bb30019ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "768a90b6-4cd0-43f2-9df6-61bf36dac0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d957daeb-32e8-4a0d-a4dd-069d96b032f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_pos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c9ff6057-d549-4062-8082-8b595e5d94a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "22d9da15-13b0-486f-88ee-da6334a8070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "04310e01-fc03-4167-86db-59d76dadb714",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (2,1) (2,1177,18) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_sorted_indices\u001b[49m\u001b[43m]\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (2,1) (2,1177,18) "
     ]
    }
   ],
   "source": [
    "np_probs[np.arange(np_probs.shape[0])[:, np.newaxis], np_sorted_indices][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "54b56363-d5f2-4a7e-ad5e-4801632b3ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177, 18)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3359be5f-6491-4c81-a9bc-8f285f69f9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1177, 18, 18)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_probs[:, np_sorted_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fe485c2a-0ff2-4ae6-accf-bf30c57f3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2  2]\n",
      "  [ 5  5]]\n",
      "\n",
      " [[ 8  8]\n",
      "  [11 11]]]\n"
     ]
    }
   ],
   "source": [
    "def nth_largest_along_z(array, n):\n",
    "    # Step 1: Use argsort along the z-axis\n",
    "    sorted_indices = np.argsort(array, axis=2)\n",
    "    \n",
    "    # Step 2: Extract the nth largest index along the z-axis\n",
    "    nth_largest_index = sorted_indices[:, :, -n]\n",
    "    \n",
    "    # Step 3: Use the obtained index to extract the corresponding value\n",
    "    nth_largest_values = array[np.arange(array.shape[0])[:, None, None], \n",
    "                               np.arange(array.shape[1])[None, :, None], \n",
    "                               nth_largest_index]\n",
    "    \n",
    "    return nth_largest_values\n",
    "\n",
    "# Example usage:\n",
    "array = np.array([[[1, 2, 3],\n",
    "                   [4, 5, 6]],\n",
    "                  [[7, 8, 9],\n",
    "                   [10, 11, 12]]])\n",
    "\n",
    "# Get the 2nd largest value along the z-axis\n",
    "nth_largest = nth_largest_along_z(array, 2)\n",
    "print(nth_largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "220bd92f-2fa6-40a1-8766-4bc9d20007b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5]\n",
      " [ 8 11]]\n"
     ]
    }
   ],
   "source": [
    "def nth_largest_along_z(array, n):\n",
    "    global nth_largest_index\n",
    "    # Step 1: Use argsort along the z-axis\n",
    "    sorted_indices = np.argsort(array, axis=2)\n",
    "    \n",
    "    # Step 2: Extract the nth largest index along the z-axis\n",
    "    nth_largest_index = sorted_indices[:, :, -n]\n",
    "    \n",
    "    # Step 3: Use the obtained index to extract the corresponding value\n",
    "    nth_largest_values = np.take_along_axis(array, nth_largest_index[:, :, np.newaxis], axis=2)\n",
    "    \n",
    "    # Reshape to 2 dimensions\n",
    "    nth_largest_values = nth_largest_values.squeeze(axis=2)\n",
    "    \n",
    "    return nth_largest_values\n",
    "\n",
    "# Example usage:\n",
    "array = np.array([[[1, 2, 3],\n",
    "                   [4, 5, 6]],\n",
    "                  [[7, 8, 9],\n",
    "                   [10, 11, 12]]])\n",
    "\n",
    "# Get the 2nd largest value along the z-axis\n",
    "nth_largest = nth_largest_along_z(array, 2)\n",
    "print(nth_largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "54727201-2908-438d-a14e-3f9918486fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_largest_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "576d5317-8ae8-44cf-896f-81cc5ad7d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_largest_index[:, :, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2d8d773b-2b35-4800-974d-f07bb65fd7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  5],\n",
       "       [ 8, 11]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "28fb727c-e73f-4f7c-b85d-453cdd0e521b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ec009fc1-cfe7-40fc-ae8a-467cdaa154f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_largest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5e345df9-8883-4d5f-89ee-4d715fbf9bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(np_probs.shape[0])[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "266c5152-1799-40b8-ad84-093862b55847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1177, 18)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_probs[:,np_max_indices].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "753f5db0-c7a8-4c5d-a5f0-5ad680d0e36b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (2,) (2,1177) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp_sorted_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_max_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_max_indices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (2,) (2,1177) "
     ]
    }
   ],
   "source": [
    "np_sorted_indices[np.arange(np_max_indices.shape[0]), np_max_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "24ef11b9-79a5-44d5-b4a7-99994b3529a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (2,) (2,1177) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_max_indices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (2,) (2,1177) "
     ]
    }
   ],
   "source": [
    "np_probs[np.arange(np_probs.shape[0]), np_max_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5329ea11-a303-4c09-8a53-5a19263aef31",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np_max_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp_sorted_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "np_max_indices = np_sorted_indices[:,:, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df13e9c4-bc72-47d6-af36-3298fcad7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177, 18)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "143beec9-293f-434b-92c0-e57e621d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_max_indices = np_sorted_indices[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fd807505-9735-4639-875c-92d189ea60ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np_max_indices = np_sorted_indices[:,:,-1]\n",
    "np_max_indices = np_sorted_indices[:,:,-1]\n",
    "np_max_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "83c0fa86-1b1b-4fe0-9c41-a6408a1e0333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8, 10, 11,  7, 14,  6, 15, 17,  0, 13, 12,  5, 16,  3,  4,  1,\n",
       "         9,  2],\n",
       "       [10,  8, 14, 17, 11,  6,  7, 12, 13, 15,  5,  3,  4,  1, 16,  9,\n",
       "         2,  0]], dtype=int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f888a8b-dd23-44f6-8993-2f1f7ead5214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177, 18)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7027b3c0-d501-4f35-877f-257cbaed683d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 18)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_max_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b20d4a30-1b30-491c-a0d0-3c95efe72dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "452cf0e5-d194-452f-9b57-b016cb92800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 251, 1155], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d6d2186-4cee-46d1-885c-4a7052dfa7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 251, 1155], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3856a39-101f-4b0d-a067-5d1ec6f3067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41458cdb-6b30-4788-90e5-5b411cec4d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72ae0e77-a5f2-488c-874a-79d126b81842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1177, 18)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e63b48d-a2ca-4e02-ba9a-9987ddda51ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 251, 1155], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea4801-2bf0-40e2-b49d-c03a4d7975c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04cc4c32-0d0a-4c25-aac5-a86dfe7743da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3],\n",
    "                   'B': [4, 5, 6],\n",
    "                   'C': [7, 8, 9]})\n",
    "\n",
    "filtered_df = df[df['A'] > 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93334f5d-ff3c-469b-8579-ec42001a98ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "1  2  5  8\n",
       "2  3  6  9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f07e231e-6f8f-46bf-a1e7-388512d69dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Boolean mask:\n",
      "[[False False False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "\n",
      "Resulting array:\n",
      "[4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create a boolean mask selecting elements greater than 3\n",
    "mask = arr > 3\n",
    "\n",
    "# Use the mask to select elements from the original array\n",
    "result = arr[mask]\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(arr)\n",
    "print(\"\\nBoolean mask:\")\n",
    "print(mask)\n",
    "print(\"\\nResulting array:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "841b4fe0-d927-4c23-abc0-4611c9510160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Boolean mask:\n",
      "[[False False False]\n",
      " [False  True  True]\n",
      " [ True  True  True]]\n",
      "\n",
      "Resulting array:\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Create an example array\n",
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create a boolean mask selecting elements greater than 4\n",
    "mask = arr > 4\n",
    "\n",
    "# Use the mask to select elements from the original array\n",
    "result = arr[mask]\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(arr)\n",
    "print(\"\\nBoolean mask:\")\n",
    "print(mask)\n",
    "print(\"\\nResulting array:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1b78016-f9a4-4023-9f63-be4d750974b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Boolean mask:\n",
      "[[False False False]\n",
      " [False  True False]\n",
      " [False False  True]]\n",
      "\n",
      "Resulting array:\n",
      "[5 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an example array\n",
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create a boolean mask selecting elements from the second row and the last column\n",
    "mask = np.array([[False, False, False],\n",
    "                 [False, True, False],\n",
    "                 [False, False, True]])\n",
    "\n",
    "# Use the mask to select elements from the original array\n",
    "result = arr[mask]\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(arr)\n",
    "print(\"\\nBoolean mask:\")\n",
    "print(mask)\n",
    "print(\"\\nResulting array:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47a0470a-6711-4f28-8805-a11559918075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Boolean mask:\n",
      "[False  True  True]\n",
      "\n",
      "Resulting array:\n",
      "[[4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an example array\n",
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create a boolean mask selecting elements from the second and third row\n",
    "mask = np.array([False, True, True])\n",
    "\n",
    "# Use the mask to select rows from the original array\n",
    "result = arr[mask]\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(arr)\n",
    "print(\"\\nBoolean mask:\")\n",
    "print(mask)\n",
    "print(\"\\nResulting array:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "759c95ef-e9b0-49db-bc96-f7392bf9562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Boolean mask:\n",
      "[[False False False]\n",
      " [False  True  True]\n",
      " [ True  True  True]]\n",
      "\n",
      "Resulting array:\n",
      "[[2 3]\n",
      " [5 6]\n",
      " [8 9]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create a boolean mask selecting elements greater than 4 along the horizontal axis (axis=1)\n",
    "mask = arr > 4\n",
    "\n",
    "# Use the mask to select elements from the original array along the horizontal axis\n",
    "result = arr[:, mask.any(axis=1)]\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(arr)\n",
    "print(\"\\nBoolean mask:\")\n",
    "print(mask)\n",
    "print(\"\\nResulting array:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5775b63f-8834-4f29-a7d1-387244ff9752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "828a6881-85e8-459e-a7d2-dfe5a3bf3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5 7 9]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# 2D boolean mask with the same shape as arr\n",
    "mask_2d = np.array([[True, False, True],\n",
    "                    [False, True, False],\n",
    "                    [True, False, True]])\n",
    "\n",
    "result_2d = arr[mask_2d]\n",
    "print(result_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61519e4b-e367-415e-bedf-707e2e75fc33",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Keyword arguments `items`, `like`, or `regex` are mutually exclusive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6400\\3511354435.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\nlp_torch\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, items, like, regex, axis)\u001b[0m\n\u001b[0;32m   5772\u001b[0m         \u001b[0mrabbit\u001b[0m    \u001b[1;36m4\u001b[0m    \u001b[1;36m5\u001b[0m      \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5773\u001b[0m         \"\"\"\n\u001b[0;32m   5774\u001b[0m         \u001b[0mnkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_not_none\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5775\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnkw\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5776\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m   5777\u001b[0m                 \u001b[1;34m\"Keyword arguments `items`, `like`, or `regex` \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5778\u001b[0m                 \u001b[1;34m\"are mutually exclusive\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5779\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Keyword arguments `items`, `like`, or `regex` are mutually exclusive"
     ]
    }
   ],
   "source": [
    "condition = lambda x: x['A'] > 1\n",
    "df.filter(['A'],condition, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e22086-53cb-438c-92dd-2cfe480c3dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56783a0-8e88-492e-bc65-dde8e8e7e903",
   "metadata": {},
   "source": [
    "# Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05dd49-9de3-4791-8657-42df1e2b5675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e2f2b56-67bc-44a5-81d7-f824b3596f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c99ce7f2-33df-4d28-a243-437822d15826",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-beta score: 0.968944099378882\n",
      "         1778966 function calls (1738293 primitive calls) in 14.654 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      690    0.004    0.000    0.041    0.000 1039821328.py:1(get_majority)\n",
      "        1    0.009    0.009   14.654   14.654 1497379277.py:1(run_inference)\n",
      "        1    0.001    0.001    0.001    0.001 1497379277.py:13(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 1497379277.py:29(<listcomp>)\n",
      "        1    0.001    0.001    0.002    0.002 1497379277.py:52(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 2159492214.py:1(convert_to_bio)\n",
      "        1    0.057    0.057    0.078    0.078 2791921769.py:1(get_labels)\n",
      "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:100(acquire)\n",
      "        9    0.000    0.000    0.008    0.001 <frozen importlib._bootstrap>:1022(_find_and_load)\n",
      "        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1038(_gcd_import)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1053(_handle_fromlist)\n",
      "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:125(release)\n",
      "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:165(__init__)\n",
      "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:169(__enter__)\n",
      "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:173(__exit__)\n",
      "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:179(_get_module_lock)\n",
      "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:198(cb)\n",
      "        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:216(_lock_unlock_module)\n",
      "      180    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:244(_verbose_message)\n",
      "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:357(__init__)\n",
      "     1470    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap>:404(parent)\n",
      "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:71(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:746(find_spec)\n",
      "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:826(find_spec)\n",
      "       24    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:893(__enter__)\n",
      "       24    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:897(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:902(_resolve_name)\n",
      "        4    0.000    0.000    0.007    0.002 <frozen importlib._bootstrap>:921(_find_spec)\n",
      "        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:968(_sanity_check)\n",
      "        4    0.000    0.000    0.007    0.002 <frozen importlib._bootstrap>:987(_find_and_load_unlocked)\n",
      "      180    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:119(<listcomp>)\n",
      "       40    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1356(_path_importer_cache)\n",
      "        4    0.000    0.000    0.007    0.002 <frozen importlib._bootstrap_external>:1399(_get_spec)\n",
      "       36    0.000    0.000    0.004    0.000 <frozen importlib._bootstrap_external>:140(_path_stat)\n",
      "        4    0.000    0.000    0.007    0.002 <frozen importlib._bootstrap_external>:1431(find_spec)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1471(find_distributions)\n",
      "       36    0.001    0.000    0.007    0.000 <frozen importlib._bootstrap_external>:1536(find_spec)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1587(_fill_cache)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1616(<setcomp>)\n",
      "       36    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:67(_relax_case)\n",
      "      180    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap_external>:96(_path_join)\n",
      "        2    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
      "       28    0.000    0.000    0.000    0.000 <string>:2(__eq__)\n",
      "        1    0.000    0.000    0.000    0.000 <string>:2(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _VF.py:26(__getattr__)\n",
      "      338    0.000    0.000    0.001    0.000 __init__.py:1007(is_promise)\n",
      "      133    0.058    0.000    0.176    0.001 __init__.py:101(get_all)\n",
      "      127    0.000    0.000    0.000    0.000 __init__.py:1014(<listcomp>)\n",
      "      116    0.000    0.000    0.001    0.000 __init__.py:1019(get_constructor)\n",
      "      116    0.000    0.000    0.000    0.000 __init__.py:1021(<listcomp>)\n",
      "       58    0.000    0.000    0.000    0.000 __init__.py:1030(parse_args)\n",
      "       58    0.002    0.000    0.180    0.003 __init__.py:1044(make_promise_schema)\n",
      "       58    0.000    0.000    0.000    0.000 __init__.py:1056(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:108(import_module)\n",
      "      734    0.001    0.000    0.006    0.000 __init__.py:1091(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 __init__.py:1108(__setitem__)\n",
      "    54837    0.015    0.000    0.015    0.000 __init__.py:112(<genexpr>)\n",
      "       27    0.000    0.000    0.000    0.000 __init__.py:1120(is_scalar_nan)\n",
      "      392    0.001    0.000    0.003    0.000 __init__.py:1146(dtype)\n",
      "      392    0.000    0.000    0.000    0.000 __init__.py:1151(_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:1166(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:1171(_dtype)\n",
      "      133    0.000    0.000    0.089    0.001 __init__.py:118(get_entry_points)\n",
      "        4    0.000    0.000    0.011    0.003 __init__.py:123(use_ops)\n",
      "       90    0.000    0.000    0.065    0.001 __init__.py:128(get_entry_point)\n",
      "      199    0.000    0.000    0.000    0.000 __init__.py:134(get_current_ops)\n",
      "      223    0.001    0.000    0.153    0.001 __init__.py:141(_get_entry_points)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:141(set_current_ops)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:1455(debug)\n",
      "       10    0.000    0.000    0.000    0.000 __init__.py:1467(info)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:155(_get_thread_state)\n",
      "        4    0.000    0.000    0.001    0.000 __init__.py:172(get_configparser)\n",
      "       14    0.000    0.000    0.000    0.000 __init__.py:1724(isEnabledFor)\n",
      "       89    0.000    0.000    0.000    0.000 __init__.py:174(check_exists)\n",
      "       89    0.000    0.000    0.001    0.000 __init__.py:183(_get)\n",
      "       23    0.000    0.000    0.001    0.000 __init__.py:183(dumps)\n",
      "      114    0.001    0.000    0.006    0.000 __init__.py:188(__init__)\n",
      "      356    0.000    0.000    0.000    0.000 __init__.py:190(<genexpr>)\n",
      "    38802    0.050    0.000    0.087    0.000 __init__.py:211(matches)\n",
      "        1    0.000    0.000    0.019    0.019 __init__.py:222(interpolate)\n",
      "        3    0.002    0.001    0.019    0.006 __init__.py:230(interpret_config)\n",
      "    77604    0.019    0.000    0.025    0.000 __init__.py:231(<genexpr>)\n",
      "      141    0.000    0.000    0.000    0.000 __init__.py:237(<lambda>)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:24(_module_matches_namespace)\n",
      "        1    0.000    0.000    1.215    1.215 __init__.py:27(load)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:27(make_lemmatizer)\n",
      "        6    0.003    0.001    0.300    0.050 __init__.py:274(load)\n",
      "    165/3    0.001    0.000    0.003    0.001 __init__.py:278(replace_section_refs)\n",
      "       33    0.000    0.000    0.000    0.000 __init__.py:287(<listcomp>)\n",
      "      534    0.000    0.000    0.003    0.000 __init__.py:293(_interpret_value)\n",
      "        7    0.000    0.000    0.198    0.028 __init__.py:299(loads)\n",
      "      618    0.001    0.000    0.001    0.000 __init__.py:305(_get_section_ref)\n",
      "       38    0.000    0.000    0.010    0.000 __init__.py:339(copy)\n",
      "       15    0.000    0.000    0.009    0.001 __init__.py:351(merge)\n",
      "      223    0.001    0.000    0.147    0.001 __init__.py:354(select)\n",
      "      991    0.052    0.000    0.146    0.000 __init__.py:359(<genexpr>)\n",
      "       50    0.000    0.000    0.000    0.000 __init__.py:36(__get__)\n",
      "      118    0.001    0.000    0.007    0.000 __init__.py:364(_sort)\n",
      "      118    0.000    0.000    0.000    0.000 __init__.py:371(<dictcomp>)\n",
      "      659    0.001    0.000    0.006    0.000 __init__.py:372(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 __init__.py:378(_set_overrides)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:392(_validate_sections)\n",
      "        3    0.000    0.000    0.036    0.012 __init__.py:404(from_str)\n",
      "        1    0.000    0.000    0.006    0.006 __init__.py:426(to_str)\n",
      "       43    0.000    0.000    0.016    0.000 __init__.py:43(__contains__)\n",
      "      223    0.004    0.000    0.004    0.000 __init__.py:463(_all)\n",
      "        2    0.000    0.000    0.023    0.012 __init__.py:477(from_disk)\n",
      "      223    0.001    0.000    0.153    0.001 __init__.py:484(select)\n",
      "      659    0.002    0.000    0.004    0.000 __init__.py:491(_mask_positional_args)\n",
      "      534    0.000    0.000    0.002    0.000 __init__.py:505(try_load_json)\n",
      "        3    0.000    0.000    0.000    0.000 __init__.py:513(is_tensor)\n",
      "      178    0.000    0.000    0.002    0.000 __init__.py:513(try_dump_json)\n",
      "        1    0.000    0.000    0.002    0.002 __init__.py:532(from_name)\n",
      "    64/15    0.000    0.000    0.001    0.000 __init__.py:537(deep_merge_configs)\n",
      "       54    0.000    0.000    0.000    0.000 __init__.py:555(<listcomp>)\n",
      "       50    0.000    0.000    0.000    0.000 __init__.py:557(<listcomp>)\n",
      "       21    0.000    0.000    0.000    0.000 __init__.py:565(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:57(find_spec)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:577(_discover_resolvers)\n",
      "        2    0.000    0.000    0.003    0.002 __init__.py:58(unpack)\n",
      "        5    0.000    0.000    0.000    0.000 __init__.py:580(<genexpr>)\n",
      "       21    0.000    0.000    0.000    0.000 __init__.py:640(update)\n",
      "      178    0.000    0.000    0.001    0.000 __init__.py:65(before_read)\n",
      "      350    0.000    0.000    0.000    0.000 __init__.py:684(alias_generator)\n",
      "       21    0.000    0.000    0.002    0.000 __init__.py:695(copy_model_field)\n",
      "        3    0.000    0.000    0.000    0.000 __init__.py:7(is_available)\n",
      "      128    0.001    0.000    0.195    0.002 __init__.py:70(unpackb)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:746(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:749(path)\n",
      "        8    0.000    0.000    0.133    0.017 __init__.py:750(resolve)\n",
      "        7    0.000    0.000    0.108    0.015 __init__.py:764(fill)\n",
      "       15    0.000    0.000    0.240    0.016 __init__.py:778(_make)\n",
      "        7    0.000    0.000    0.000    0.000 __init__.py:781(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 __init__.py:787(children)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:794(zip_children)\n",
      "        7    0.000    0.000    0.002    0.000 __init__.py:801(search)\n",
      "        7    0.000    0.000    0.001    0.000 __init__.py:804(mtime)\n",
      "        3    0.000    0.000    0.001    0.000 __init__.py:810(lookup)\n",
      "        3    0.000    0.000    0.001    0.000 __init__.py:816(__init__)\n",
      "        2    0.000    0.000    0.008    0.004 __init__.py:82(_import_extra_cpu_backends)\n",
      "      356    0.000    0.000    0.003    0.000 __init__.py:82(before_get)\n",
      "       89    0.001    0.000    0.050    0.001 __init__.py:82(get)\n",
      "    73/15    0.004    0.000    0.240    0.016 __init__.py:825(_fill)\n",
      "        7    0.000    0.000    0.000    0.000 __init__.py:837(search)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:859(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:866(normalize)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:873(legacy_normalize)\n",
      "      356    0.001    0.000    0.002    0.000 __init__.py:88(interpolate)\n",
      "       14    0.000    0.000    0.000    0.000 __init__.py:881(__bool__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:886(find_distributions)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:89(find_spec)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:899(_search_paths)\n",
      "        1    0.000    0.000    1.213    1.213 __init__.py:9(load)\n",
      "        8    0.000    0.000    0.002    0.000 __init__.py:903(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:912(__init__)\n",
      "        2    0.000    0.000    0.011    0.005 __init__.py:93(get_ops)\n",
      "       73    0.001    0.000    0.002    0.000 __init__.py:949(_update_from_parsed)\n",
      "        1    0.000    0.000    0.002    0.002 __init__.py:951(distribution)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:96(<lambda>)\n",
      "     1444    0.001    0.000    0.001    0.000 __init__.py:970(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 __init__.py:98(<dictcomp>)\n",
      "     1444    0.001    0.000    0.001    0.000 __init__.py:980(__getitem__)\n",
      "       15    0.000    0.000    0.000    0.000 __init__.py:981(_validate_overrides)\n",
      "       32    0.000    0.000    0.000    0.000 _array_api.py:111(_is_numpy_namespace)\n",
      "       10    0.000    0.000    0.000    0.000 _array_api.py:123(isdtype)\n",
      "    14/10    0.000    0.000    0.000    0.000 _array_api.py:135(_isdtype_single)\n",
      "        6    0.000    0.000    0.000    0.000 _array_api.py:144(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 _array_api.py:170(supported_float_dtypes)\n",
      "       36    0.000    0.000    0.000    0.000 _array_api.py:212(_check_device_cpu)\n",
      "       80    0.000    0.000    0.000    0.000 _array_api.py:272(__getattr__)\n",
      "       36    0.000    0.000    0.000    0.000 _array_api.py:292(asarray)\n",
      "       12    0.000    0.000    0.002    0.000 _array_api.py:306(unique_values)\n",
      "        7    0.000    0.000    0.000    0.000 _array_api.py:312(reshape)\n",
      "       10    0.000    0.000    0.000    0.000 _array_api.py:327(isdtype)\n",
      "       54    0.000    0.000    0.000    0.000 _array_api.py:334(get_namespace)\n",
      "       30    0.000    0.000    0.004    0.000 _array_api.py:501(_asarray_with_order)\n",
      "        1    0.000    0.000    0.000    0.000 _array_api.py:53(_check_array_api_dispatch)\n",
      "       59    0.000    0.000    0.000    0.000 _base.py:1483(issparse)\n",
      "        2    0.000    0.000    0.000    0.000 _beam_utils.pyx:200(collect_states)\n",
      "        1    0.000    0.000    0.012    0.012 _classification.py:1281(fbeta_score)\n",
      "        3    0.000    0.000    0.000    0.000 _classification.py:1465(_prf_divide)\n",
      "        1    0.000    0.000    0.004    0.004 _classification.py:1518(_check_set_wise_labels)\n",
      "        1    0.000    0.000    0.012    0.012 _classification.py:1559(precision_recall_fscore_support)\n",
      "        1    0.000    0.000    0.007    0.007 _classification.py:398(multilabel_confusion_matrix)\n",
      "        1    0.000    0.000    0.000    0.000 _classification.py:49(_check_zero_division)\n",
      "        2    0.000    0.000    0.006    0.003 _classification.py:58(_check_targets)\n",
      "       13    0.000    0.000    0.000    0.000 _collections.py:20(__missing__)\n",
      "        6    0.000    0.000    0.000    0.000 _collections.py:23(freeze)\n",
      "       13    0.000    0.000    0.000    0.000 _collections.py:24(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 _collections.py:242(__init__)\n",
      "       46    0.000    0.000    0.000    0.000 _collections.py:259(__getitem__)\n",
      "       40    0.000    0.000    0.000    0.000 _collections.py:291(__iter__)\n",
      "       38    0.000    0.000    0.000    0.000 _collections.py:302(add)\n",
      "        2    0.000    0.000    0.000    0.000 _collections.py:337(extend)\n",
      "        2    0.000    0.000    0.000    0.000 _collections.py:95(__getitem__)\n",
      "        6    0.000    0.000    0.000    0.000 _collections_abc.py:1034(__iter__)\n",
      "        2    0.000    0.000    0.000    0.000 _collections_abc.py:1044(__contains__)\n",
      "       49    0.000    0.000    0.000    0.000 _collections_abc.py:816(get)\n",
      "       14    0.000    0.000    0.000    0.000 _collections_abc.py:823(__contains__)\n",
      "        4    0.000    0.000    0.000    0.000 _collections_abc.py:831(keys)\n",
      "      149    0.000    0.000    0.000    0.000 _collections_abc.py:835(items)\n",
      "      153    0.000    0.000    0.000    0.000 _collections_abc.py:857(__init__)\n",
      "      143    0.000    0.000    0.001    0.000 _collections_abc.py:860(__len__)\n",
      "      288    0.000    0.000    0.000    0.000 _collections_abc.py:880(__iter__)\n",
      "      858    0.001    0.000    0.006    0.000 _collections_abc.py:904(__iter__)\n",
      "      748    0.002    0.000    0.005    0.000 _collections_abc.py:986(update)\n",
      "        2    0.000    0.000    0.000    0.000 _config.py:199(config_context)\n",
      "       60    0.000    0.000    0.000    0.000 _config.py:24(_get_threadlocal_config)\n",
      "       58    0.000    0.000    0.000    0.000 _config.py:32(get_config)\n",
      "        2    0.000    0.000    0.000    0.000 _config.py:50(set_config)\n",
      "      494    0.002    0.000    0.002    0.000 _contextlib.py:149(__new__)\n",
      "      733    0.002    0.000    0.008    0.000 _dict_proxies.py:26(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _encode.py:10(_unique)\n",
      "        2    0.000    0.000    0.000    0.000 _encode.py:149(__init__)\n",
      "        2    0.000    0.000    0.002    0.001 _encode.py:162(_map_to_integer)\n",
      "        2    0.000    0.000    0.000    0.000 _encode.py:164(<dictcomp>)\n",
      "        2    0.001    0.001    0.001    0.001 _encode.py:165(<listcomp>)\n",
      "        2    0.000    0.000    0.002    0.001 _encode.py:194(_encode)\n",
      "        1    0.000    0.000    0.000    0.000 _encode.py:51(_unique_np)\n",
      "        4    0.000    0.000    0.000    0.000 _errors.py:230(hf_raise_for_status)\n",
      "        4    0.000    0.000    0.001    0.000 _headers.py:137(get_token_to_send)\n",
      "        4    0.000    0.000    0.000    0.000 _headers.py:169(_validate_token_to_send)\n",
      "        4    0.000    0.000    0.000    0.000 _headers.py:186(_http_user_agent)\n",
      "        4    0.000    0.000    0.000    0.000 _headers.py:230(_deduplicate_user_agent)\n",
      "        4    0.000    0.000    0.000    0.000 _headers.py:234(<dictcomp>)\n",
      "        4    0.000    0.000    0.001    0.000 _headers.py:39(build_hf_headers)\n",
      "        2    0.000    0.000    0.000    0.000 _http.py:134(get_session)\n",
      "        2    0.000    0.000    0.000    0.000 _http.py:51(add_headers)\n",
      "        2    0.000    0.000    2.948    1.474 _http.py:64(send)\n",
      "       10    0.000    0.000    0.000    0.000 _internal_utils.py:25(to_native_string)\n",
      "        2    0.000    0.000    0.000    0.000 _internal_utils.py:38(unicode_is_ascii)\n",
      "      495    0.000    0.000    0.000    0.000 _jit_internal.py:1109(is_scripting)\n",
      "      190    0.000    0.000    0.001    0.000 _json_api.py:10(json_dumps)\n",
      "        8    0.000    0.000    0.000    0.000 _json_api.py:179(is_json_serializable)\n",
      "      714    0.000    0.000    0.002    0.000 _json_api.py:30(json_loads)\n",
      "        9    0.000    0.000    0.016    0.002 _json_api.py:42(read_json)\n",
      "        2    0.000    0.000    0.002    0.001 _label.py:118(transform)\n",
      "        1    0.000    0.000    0.000    0.000 _label.py:84(fit)\n",
      "       10    0.000    0.000    0.001    0.000 _methods.py:101(_mean)\n",
      "       10    0.001    0.000    0.002    0.000 _methods.py:135(_var)\n",
      "        3    0.000    0.000    0.000    0.000 _methods.py:47(_sum)\n",
      "        3    0.000    0.000    0.000    0.000 _methods.py:55(_any)\n",
      "        3    0.000    0.000    0.000    0.000 _methods.py:61(_all)\n",
      "       20    0.000    0.000    0.000    0.000 _methods.py:67(_count_reduce_items)\n",
      "      128    0.001    0.000    0.196    0.002 _msgpack_api.py:17(msgpack_loads)\n",
      "        2    0.000    0.000    0.004    0.002 _msgpack_api.py:43(read_msgpack)\n",
      "     4283    0.001    0.000    0.002    0.000 _msgpack_numpy.py:65(decode_numpy)\n",
      "      192    0.000    0.000    0.000    0.000 _param_server.py:16(__init__)\n",
      "       68    0.000    0.000    0.000    0.000 _param_server.py:38(has_param)\n",
      "       28    0.000    0.000    0.000    0.000 _param_server.py:41(has_grad)\n",
      "       68    0.000    0.000    0.000    0.000 _param_server.py:44(get_param)\n",
      "       85    0.000    0.000    0.000    0.000 _param_server.py:54(set_param)\n",
      "       18    0.000    0.000    0.000    0.000 _param_validation.py:101(make_constraint)\n",
      "      3/1    0.000    0.000    0.012    0.012 _param_validation.py:182(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:195(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:201(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:25(validate_parameter_constraints)\n",
      "       16    0.000    0.000    0.000    0.000 _param_validation.py:259(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 _param_validation.py:291(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:295(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:305(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:315(is_satisfied_by)\n",
      "        3    0.000    0.000    0.000    0.000 _param_validation.py:363(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:481(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:497(is_satisfied_by)\n",
      "        4    0.000    0.000    0.000    0.000 _param_validation.py:525(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 _param_validation.py:584(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 _param_validation.py:72(<listcomp>)\n",
      "       50    0.000    0.000    0.000    0.000 _policybase.py:281(_sanitize_header)\n",
      "       38    0.000    0.000    0.000    0.000 _policybase.py:293(header_source_parse)\n",
      "       50    0.000    0.000    0.000    0.000 _policybase.py:311(header_fetch_parse)\n",
      "        2    0.000    0.000    0.004    0.002 _precomputable_affine.py:19(forward)\n",
      "        2    0.000    0.000    0.000    0.000 _precomputable_affine.py:6(PrecomputableAffine)\n",
      "      179    0.003    0.000    0.003    0.000 _retokenize.pyx:443(normalize_token_attrs)\n",
      "     1064    0.001    0.000    0.001    0.000 _retokenize.pyx:459(set_token_attrs)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:105(is_fastcore_available)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:215(is_tf_available)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:219(get_tf_version)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:224(is_torch_available)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:228(get_torch_version)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:260(is_google_colab)\n",
      "       24    0.000    0.000    0.000    0.000 _runtime.py:68(_get_version)\n",
      "       16    0.000    0.000    0.000    0.000 _runtime.py:72(_is_available)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:77(get_python_version)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:82(get_hf_hub_version)\n",
      "        4    0.000    0.000    0.000    0.000 _runtime.py:96(is_fastai_available)\n",
      "      786    0.000    0.000    0.000    0.000 _tensor.py:1002(__hash__)\n",
      "        1    0.000    0.000    0.000    0.000 _tensor.py:964(__len__)\n",
      "        4    0.000    0.000    0.000    0.000 _token.py:110(_get_token_from_environment)\n",
      "        4    0.000    0.000    0.000    0.000 _token.py:115(_get_token_from_file)\n",
      "        4    0.000    0.000    0.000    0.000 _token.py:122(_clean_token)\n",
      "        4    0.000    0.000    0.001    0.000 _token.py:30(get_token)\n",
      "        4    0.000    0.000    0.000    0.000 _token.py:47(_get_token_from_google_colab)\n",
      "        1    0.001    0.001    0.001    0.001 _trace.py:1110(is_tracing)\n",
      "        6    0.000    0.000    0.000    0.000 _ufunc_config.py:132(geterr)\n",
      "        6    0.000    0.000    0.000    0.000 _ufunc_config.py:33(seterr)\n",
      "        3    0.000    0.000    0.000    0.000 _ufunc_config.py:426(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 _ufunc_config.py:430(__enter__)\n",
      "        3    0.000    0.000    0.000    0.000 _ufunc_config.py:435(__exit__)\n",
      "       60    0.000    0.000    0.000    0.000 _ufunc_config.py:452(_no_nep50_warning)\n",
      "      393    0.001    0.000    0.011    0.000 _utils.py:178(_rebuild_tensor)\n",
      "      393    0.001    0.000    0.012    0.000 _utils.py:199(_rebuild_tensor_v2)\n",
      "        1    0.000    0.000    0.000    0.000 _utils.py:227(_validate_loaded_sparse_tensors)\n",
      "      393    0.001    0.000    0.001    0.000 _utils.py:806(_element_size)\n",
      "  786/393    0.001    0.000    0.003    0.000 _utils.py:828(__get__)\n",
      "    22/12    0.000    0.000    2.964    0.247 _validators.py:102(_inner_fn)\n",
      "       16    0.000    0.000    0.000    0.000 _validators.py:123(validate_repo_id)\n",
      "        8    0.000    0.000    0.000    0.000 _validators.py:177(smoothly_deprecate_use_auth_token)\n",
      "        1    0.000    0.000    0.000    0.000 _weights_only_unpickler.py:114(__init__)\n",
      "        1    0.071    0.071    0.122    0.122 _weights_only_unpickler.py:120(load)\n",
      "      787    0.001    0.000    0.001    0.000 _weights_only_unpickler.py:283(pop_mark)\n",
      "       58    0.000    0.000    0.003    0.000 abc.py:105(__new__)\n",
      "      981    0.001    0.000    0.002    0.000 abc.py:117(__instancecheck__)\n",
      "      376    0.000    0.000    0.001    0.000 abc.py:121(__subclasscheck__)\n",
      "       24    0.000    0.000    0.001    0.000 activations.py:203(__getitem__)\n",
      "       24    0.000    0.000    0.001    0.000 activations.py:68(__init__)\n",
      "       24    0.000    0.000    0.057    0.002 activations.py:78(forward)\n",
      "        2    0.000    0.000    0.001    0.001 adapters.py:237(cert_verify)\n",
      "        2    0.000    0.000    0.001    0.000 adapters.py:294(build_response)\n",
      "        2    0.000    0.000    0.001    0.000 adapters.py:331(get_connection)\n",
      "        2    0.000    0.000    0.000    0.000 adapters.py:370(request_url)\n",
      "        2    0.000    0.000    0.000    0.000 adapters.py:399(add_headers)\n",
      "        2    0.000    0.000    2.948    1.474 adapters.py:434(send)\n",
      "        1    0.000    0.000    0.000    0.000 arc_eager.pyx:577(__init__)\n",
      "      106    0.000    0.000    0.000    0.000 arc_eager.pyx:718(init_transition)\n",
      "        1    0.001    0.001    0.001    0.001 arc_eager.pyx:750(set_annotations)\n",
      "       15    0.000    0.000    0.000    0.000 array_getitem.py:30(ints_getitem)\n",
      "       10    0.000    0.000    0.000    0.000 array_getitem.py:38(forward)\n",
      "      706    0.000    0.000    0.001    0.000 arraysetops.py:125(_unpack_tuple)\n",
      "      706    0.000    0.000    0.000    0.000 arraysetops.py:133(_unique_dispatcher)\n",
      "      706    0.002    0.000    0.034    0.000 arraysetops.py:138(unique)\n",
      "      706    0.016    0.000    0.031    0.000 arraysetops.py:323(_unique1d)\n",
      "        1    0.000    0.000    0.000    0.000 arraysetops.py:519(_in1d_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 arraysetops.py:524(in1d)\n",
      "        2    0.000    0.000    0.000    0.000 arraysetops.py:630(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 arraysetops.py:935(_setdiff1d_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 arraysetops.py:939(setdiff1d)\n",
      "        1    0.000    0.000    0.000    0.000 ast.py:33(parse)\n",
      "        1    0.000    0.000    0.000    0.000 ast.py:54(literal_eval)\n",
      "      9/1    0.000    0.000    0.000    0.000 ast.py:84(_convert)\n",
      "        1    0.000    0.000    0.035    0.035 attributeruler.py:132(__call__)\n",
      "        1    0.000    0.000    0.026    0.026 attributeruler.py:148(match)\n",
      "        1    0.001    0.001    0.001    0.001 attributeruler.py:151(<listcomp>)\n",
      "        1    0.004    0.004    0.009    0.009 attributeruler.py:157(set_annotations)\n",
      "      179    0.001    0.000    0.008    0.000 attributeruler.py:224(add)\n",
      "        1    0.000    0.000    0.009    0.009 attributeruler.py:248(add_patterns)\n",
      "        1    0.000    0.000    0.000    0.000 attributeruler.py:25(make_attribute_ruler)\n",
      "        1    0.000    0.000    0.011    0.011 attributeruler.py:323(from_disk)\n",
      "        1    0.000    0.000    0.011    0.011 attributeruler.py:335(load_patterns)\n",
      "        1    0.000    0.000    0.000    0.000 attributeruler.py:57(make_attribute_ruler_scorer)\n",
      "        1    0.000    0.000    0.000    0.000 attributeruler.py:69(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 auto_factory.py:386(_get_model_class)\n",
      "        1    0.174    0.174    2.892    2.892 auto_factory.py:448(from_pretrained)\n",
      "        1    0.000    0.000    0.000    0.000 auto_factory.py:464(<dictcomp>)\n",
      "  115/113    0.000    0.000    0.000    0.000 auto_factory.py:693(getattribute_from_module)\n",
      "        1    0.000    0.000    0.000    0.000 auto_factory.py:734(__getitem__)\n",
      "      113    0.000    0.000    0.001    0.000 auto_factory.py:750(_load_attr_from_module)\n",
      "        2    0.000    0.000    0.001    0.001 auto_factory.py:756(keys)\n",
      "        2    0.000    0.000    0.001    0.000 auto_factory.py:757(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 auto_factory.py:795(__contains__)\n",
      "       43    0.000    0.000    0.001    0.000 chain.py:18(chain)\n",
      "      144    0.000    0.000    0.000    0.000 chain.py:39(<genexpr>)\n",
      "     47/4    0.000    0.000    0.061    0.015 chain.py:48(forward)\n",
      "       58    0.000    0.000    0.000    0.000 class_validators.py:162(__init__)\n",
      "      246    0.001    0.000    0.001    0.000 class_validators.py:166(get_validators)\n",
      "       58    0.000    0.000    0.000    0.000 class_validators.py:176(check_for_unused)\n",
      "       58    0.000    0.000    0.000    0.000 class_validators.py:178(<genexpr>)\n",
      "       58    0.000    0.000    0.000    0.000 class_validators.py:195(extract_validators)\n",
      "       58    0.000    0.000    0.001    0.000 class_validators.py:209(extract_root_validators)\n",
      "       58    0.000    0.000    0.000    0.000 class_validators.py:234(inherit_validators)\n",
      "      288    0.003    0.000    0.021    0.000 class_validators.py:242(make_generic_validator)\n",
      "      313    0.000    0.000    0.021    0.000 class_validators.py:281(prep_validators)\n",
      "      313    0.001    0.000    0.021    0.000 class_validators.py:282(<listcomp>)\n",
      "      288    0.001    0.000    0.001    0.000 class_validators.py:322(_generic_validator_basic)\n",
      "      304    0.000    0.000    0.001    0.000 class_validators.py:337(<lambda>)\n",
      "       36    0.000    0.000    0.000    0.000 class_validators.py:349(<lambda>)\n",
      "       14    0.000    0.000    0.000    0.000 client.py:1007(_output)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1028(_send_output)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1082(putrequest)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1209(_encode_request)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1213(_validate_method)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1222(_validate_path)\n",
      "       12    0.000    0.000    0.000    0.000 client.py:1238(putheader)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:1266(endheaders)\n",
      "        2    0.000    0.000    2.943    1.471 client.py:1330(getresponse)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:206(_read_headers)\n",
      "        2    0.000    0.000    0.001    0.001 client.py:224(parse_headers)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:248(__init__)\n",
      "        2    0.000    0.000    2.941    1.471 client.py:278(_read_status)\n",
      "        2    0.000    0.000    2.943    1.471 client.py:311(begin)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:383(_check_close)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:412(_close_conn)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:417(close)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:429(flush)\n",
      "       10    0.000    0.000    0.000    0.000 client.py:440(isclosed)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:450(read)\n",
      "        2    0.000    0.000    0.000    0.000 client.py:967(send)\n",
      "        3    0.000    0.000    0.003    0.001 clone.py:12(clone)\n",
      "       20    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "       18    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "       18    0.000    0.000    0.020    0.001 codecs.py:319(decode)\n",
      "        3    0.000    0.000    0.000    0.000 concatenate.py:28(concatenate)\n",
      "        6    0.000    0.000    0.000    0.000 concatenate.py:44(<genexpr>)\n",
      "       18    0.000    0.000    0.000    0.000 concatenate.py:48(<genexpr>)\n",
      "        2    0.000    0.000    0.005    0.003 concatenate.py:56(forward)\n",
      "        2    0.000    0.000    0.004    0.002 concatenate.py:57(<listcomp>)\n",
      "        2    0.000    0.000    0.001    0.001 concatenate.py:69(_array_forward)\n",
      "        2    0.000    0.000    0.000    0.000 concatenate.py:72(<listcomp>)\n",
      "      350    0.001    0.000    0.002    0.000 config.py:117(get_field_info)\n",
      "      371    0.000    0.000    0.000    0.000 config.py:142(prepare_field)\n",
      "      116    0.004    0.000    0.004    0.000 config.py:169(inherit_config)\n",
      "       58    0.000    0.000    0.000    0.000 config.py:186(prepare_config)\n",
      "      807    0.000    0.000    0.000    0.000 configparser.py:1024(<dictcomp>)\n",
      "        3    0.001    0.000    0.002    0.001 configparser.py:1119(_join_multiline_values)\n",
      "     1444    0.002    0.000    0.002    0.000 configparser.py:1143(_unify_values)\n",
      "      224    0.000    0.000    0.000    0.000 configparser.py:1170(_validate_value_types)\n",
      "      188    0.000    0.000    0.000    0.000 configparser.py:1191(converters)\n",
      "      178    0.001    0.000    0.001    0.000 configparser.py:1201(set)\n",
      "       46    0.000    0.000    0.001    0.000 configparser.py:1207(add_section)\n",
      "      188    0.001    0.000    0.003    0.000 configparser.py:1244(__init__)\n",
      "      534    0.001    0.000    0.005    0.000 configparser.py:1256(__getitem__)\n",
      "      138    0.000    0.000    0.001    0.000 configparser.py:1273(__len__)\n",
      "      138    0.000    0.000    0.001    0.000 configparser.py:1276(__iter__)\n",
      "      276    0.000    0.000    0.001    0.000 configparser.py:1279(_options)\n",
      "        4    0.000    0.000    0.001    0.000 configparser.py:1321(__init__)\n",
      "      188    0.000    0.000    0.000    0.000 configparser.py:1363(__iter__)\n",
      "      712    0.000    0.000    0.000    0.000 configparser.py:363(before_get)\n",
      "      534    0.000    0.000    0.000    0.000 configparser.py:369(before_read)\n",
      "      178    0.000    0.000    0.000    0.000 configparser.py:372(before_write)\n",
      "      178    0.000    0.000    0.000    0.000 configparser.py:459(before_set)\n",
      "        4    0.000    0.000    0.001    0.000 configparser.py:601(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 configparser.py:641(defaults)\n",
      "       46    0.000    0.000    0.000    0.000 configparser.py:649(add_section)\n",
      "      184    0.000    0.000    0.000    0.000 configparser.py:663(has_section)\n",
      "      276    0.000    0.000    0.001    0.000 configparser.py:670(options)\n",
      "        3    0.000    0.000    0.014    0.005 configparser.py:706(read_file)\n",
      "        3    0.000    0.000    0.014    0.005 configparser.py:721(read_string)\n",
      "1444/1068    0.002    0.000    0.008    0.000 configparser.py:766(get)\n",
      "        3    0.000    0.000    0.000    0.000 configparser.py:832(items)\n",
      "      534    0.000    0.000    0.000    0.000 configparser.py:878(has_option)\n",
      "      178    0.000    0.000    0.001    0.000 configparser.py:892(set)\n",
      "        1    0.000    0.000    0.001    0.001 configparser.py:906(write)\n",
      "       46    0.000    0.000    0.001    0.000 configparser.py:926(_write_section)\n",
      "      141    0.000    0.000    0.000    0.000 configparser.py:962(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 configparser.py:990(__len__)\n",
      "        3    0.000    0.000    0.000    0.000 configparser.py:993(__iter__)\n",
      "        3    0.007    0.002    0.014    0.005 configparser.py:997(_read)\n",
      "      115    0.000    0.000    0.000    0.000 configuration_auto.py:774(model_type_to_module_name)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_auto.py:809(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_auto.py:838(__contains__)\n",
      "        1    0.000    0.000    0.005    0.005 configuration_auto.py:998(from_pretrained)\n",
      "        2    0.000    0.000    0.001    0.000 configuration_deberta_v2.py:113(__init__)\n",
      "     15/4    0.000    0.000    0.000    0.000 configuration_utils.py:1005(dict_torch_dtype_to_str)\n",
      "        3    0.000    0.000    0.000    0.000 configuration_utils.py:1043(_get_generation_defaults)\n",
      "  230/224    0.000    0.000    0.000    0.000 configuration_utils.py:257(__setattr__)\n",
      "14827/14813    0.024    0.000    0.024    0.000 configuration_utils.py:262(__getattribute__)\n",
      "        3    0.000    0.000    0.001    0.000 configuration_utils.py:267(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_utils.py:310(<dictcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 configuration_utils.py:379(name_or_path)\n",
      "        2    0.000    0.000    0.000    0.000 configuration_utils.py:383(name_or_path)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_utils.py:387(use_return_dict)\n",
      "        2    0.000    0.000    0.000    0.000 configuration_utils.py:395(num_labels)\n",
      "        2    0.000    0.000    0.000    0.000 configuration_utils.py:402(num_labels)\n",
      "        2    0.000    0.000    0.000    0.000 configuration_utils.py:405(<dictcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 configuration_utils.py:408(_attn_implementation)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_utils.py:484(_set_token_in_kwargs)\n",
      "        1    0.000    0.000    0.001    0.001 configuration_utils.py:614(get_config_dict)\n",
      "        1    0.000    0.000    0.001    0.001 configuration_utils.py:647(_get_config_dict)\n",
      "        1    0.000    0.000    0.003    0.003 configuration_utils.py:737(from_dict)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_utils.py:767(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 configuration_utils.py:814(_dict_from_json_file)\n",
      "        1    0.000    0.000    0.003    0.003 configuration_utils.py:823(__repr__)\n",
      "        1    0.000    0.000    0.002    0.002 configuration_utils.py:826(to_diff_dict)\n",
      "        3    0.000    0.000    0.001    0.000 configuration_utils.py:883(to_dict)\n",
      "        1    0.000    0.000    0.003    0.003 configuration_utils.py:925(to_json_string)\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:15(is_connection_dropped)\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:168(host)\n",
      "        4    0.000    0.000    0.000    0.000 connection.py:256(is_closed)\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:260(is_connected)\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:285(putrequest)\n",
      "       12    0.000    0.000    0.000    0.000 connection.py:305(putheader)\n",
      "       24    0.000    0.000    0.000    0.000 connection.py:307(<genexpr>)\n",
      "        2    0.000    0.000    0.001    0.000 connection.py:319(request)\n",
      "       12    0.000    0.000    0.000    0.000 connection.py:354(<genexpr>)\n",
      "        2    0.000    0.000    2.944    1.472 connection.py:435(getresponse)\n",
      "        2    0.000    0.000    0.000    0.000 connectionpool.py:1088(_validate_conn)\n",
      "        2    0.000    0.000    0.000    0.000 connectionpool.py:259(_get_conn)\n",
      "        2    0.000    0.000    0.000    0.000 connectionpool.py:297(_put_conn)\n",
      "        2    0.000    0.000    0.000    0.000 connectionpool.py:340(_validate_conn)\n",
      "        4    0.000    0.000    0.000    0.000 connectionpool.py:349(_get_timeout)\n",
      "        2    0.000    0.000    2.944    1.472 connectionpool.py:380(_make_request)\n",
      "        2    0.000    0.000    2.945    1.473 connectionpool.py:595(urlopen)\n",
      "        1    0.000    0.000    0.000    0.000 container.py:274(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 container.py:309(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 container.py:313(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 container.py:317(__iadd__)\n",
      "        1    0.000    0.000    0.000    0.000 container.py:389(extend)\n",
      "       38    0.000    0.000    0.000    0.000 contextlib.py:102(__init__)\n",
      "       38    0.000    0.000    0.011    0.000 contextlib.py:130(__enter__)\n",
      "       38    0.000    0.000    0.000    0.000 contextlib.py:139(__exit__)\n",
      "       38    0.000    0.000    0.000    0.000 contextlib.py:279(helper)\n",
      "       11    0.000    0.000    0.000    0.000 contextlib.py:420(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 contextlib.py:423(__enter__)\n",
      "       11    0.000    0.000    0.000    0.000 contextlib.py:426(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:442(_create_exit_wrapper)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:452(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:482(enter_context)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:509(_push_cm_exit)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:514(_push_exit_callback)\n",
      "        1    0.000    0.000    0.000    0.000 contextlib.py:530(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 contextlib.py:533(__exit__)\n",
      "       10    0.000    0.000    0.000    0.000 cookiejar.py:1227(vals_sorted_by_key)\n",
      "       10    0.000    0.000    0.000    0.000 cookiejar.py:1231(deepvalues)\n",
      "        6    0.000    0.000    0.000    0.000 cookiejar.py:1266(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:1295(_cookies_for_request)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:1302(_cookie_attrs)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:1361(add_cookie_header)\n",
      "        4    0.000    0.000    0.000    0.000 cookiejar.py:1604(make_cookies)\n",
      "        4    0.000    0.000    0.000    0.000 cookiejar.py:1685(extract_cookies)\n",
      "        2    0.000    0.000    0.000    0.000 cookiejar.py:1739(clear_expired_cookies)\n",
      "       10    0.000    0.000    0.000    0.000 cookiejar.py:1758(__iter__)\n",
      "        6    0.000    0.000    0.000    0.000 cookiejar.py:44(_debug)\n",
      "        6    0.000    0.000    0.000    0.000 cookiejar.py:885(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:110(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 cookies.py:117(info)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:124(extract_cookies_to_jar)\n",
      "        2    0.000    0.000    0.000    0.000 cookies.py:140(get_cookie_header)\n",
      "        6    0.000    0.000    0.000    0.000 cookies.py:35(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:358(update)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:521(cookiejar_from_dict)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:534(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 cookies.py:542(merge_cookies)\n",
      "        2    0.000    0.000    0.000    0.000 cookies.py:87(get_new_headers)\n",
      "        2    0.000    0.000    0.000    0.000 copy.py:107(_copy_immutable)\n",
      " 3815/214    0.007    0.000    0.013    0.000 copy.py:128(deepcopy)\n",
      "     3254    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)\n",
      "       87    0.000    0.000    0.001    0.000 copy.py:201(_deepcopy_list)\n",
      "  424/291    0.002    0.000    0.009    0.000 copy.py:227(_deepcopy_dict)\n",
      "      556    0.001    0.000    0.001    0.000 copy.py:243(_keep_alive)\n",
      "       45    0.001    0.000    0.009    0.000 copy.py:259(_reconstruct)\n",
      "       90    0.000    0.000    0.000    0.000 copy.py:264(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 copy.py:66(copy)\n",
      "       45    0.000    0.000    0.000    0.000 copyreg.py:100(__newobj__)\n",
      "        2    0.000    0.000    0.000    0.000 cp1252.py:22(decode)\n",
      "        3    0.000    0.000    0.000    0.000 dataclasses.py:1188(fields)\n",
      "       13    0.000    0.000    0.000    0.000 dataclasses.py:1203(<genexpr>)\n",
      "      246    0.000    0.000    0.001    0.000 dataclasses.py:1211(is_dataclass)\n",
      "       28    0.000    0.000    0.000    0.000 dataclasses.py:283(handle_extra_init)\n",
      "       28    0.000    0.000    0.000    0.000 dataclasses.py:320(new_init)\n",
      "      246    0.000    0.000    0.001    0.000 dataclasses.py:443(is_builtin_dataclass)\n",
      "        7    0.000    0.000    0.198    0.028 decoder.py:332(decode)\n",
      "        7    0.191    0.027    0.198    0.028 decoder.py:343(raw_decode)\n",
      "      350    0.000    0.000    0.000    0.000 deepspeed.py:261(is_deepspeed_zero3_enabled)\n",
      "        1    0.000    0.000    0.000    0.000 dep_parser.pyx:250(make_parser_scorer)\n",
      "        1    0.000    0.000    0.000    0.000 dep_parser.pyx:261(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 dep_parser.pyx:297(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 dep_parser.pyx:315(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 dep_parser.pyx:343(_ensure_labels_are_added)\n",
      "        1    0.000    0.000    0.000    0.000 dep_parser.pyx:62(make_parser)\n",
      "        3    0.000    0.000    0.000    0.000 distributed_c10d.py:437(default_pg)\n",
      "        3    0.000    0.000    0.000    0.000 distributed_c10d.py:555(WORLD)\n",
      "        3    0.000    0.000    0.000    0.000 distributed_c10d.py:908(is_initialized)\n",
      "        4    0.000    0.000    0.000    0.000 doc.pxd:44(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 doc.pxd:44(__set__)\n",
      "        6    0.000    0.000    0.000    0.000 doc.pyx:1050(_realloc)\n",
      "        1    0.000    0.000    0.000    0.000 doc.pyx:126(values)\n",
      "        2    0.000    0.000    0.000    0.000 doc.pyx:1817(set_children_from_heads)\n",
      "        2    0.000    0.000    0.000    0.000 doc.pyx:1848(_set_lr_kids_and_edges)\n",
      "      733    0.010    0.000    0.019    0.000 doc.pyx:202(__init__)\n",
      "     3660    0.009    0.000    0.010    0.000 doc.pyx:431(has_annotation)\n",
      "     3660    0.001    0.000    0.001    0.000 doc.pyx:472(genexpr)\n",
      "6390/6288    0.003    0.000    0.005    0.000 doc.pyx:474(__getitem__)\n",
      "     3016    0.001    0.000    0.001    0.000 doc.pyx:507(__iter__)\n",
      "     2619    0.000    0.000    0.000    0.000 doc.pyx:520(__len__)\n",
      "     6356    0.000    0.000    0.000    0.000 doc.pyx:67(bounds_check)\n",
      "        1    0.000    0.000    0.000    0.000 doc.pyx:797(set_ents)\n",
      "     2347    0.000    0.000    0.001    0.000 doc.pyx:945(__pyx_fuse_0push_back)\n",
      "       34    0.000    0.000    0.000    0.000 doc.pyx:945(__pyx_fuse_1push_back)\n",
      "        2    0.000    0.000    0.000    0.000 doc.pyx:969(to_array (wrapper))\n",
      "        2    0.000    0.000    0.000    0.000 doc.pyx:969(to_array)\n",
      "        6    0.000    0.000    0.000    0.000 dropout.py:10(Dropout)\n",
      "        1    0.000    0.000    0.000    0.000 dropout.py:13(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 dropout.py:19(forward)\n",
      "        1    0.000    0.000    0.000    0.000 dropout.py:57(forward)\n",
      "        3    0.000    0.000    0.000    0.000 dynamic_module_utils.py:589(resolve_trust_remote_code)\n",
      "       23    0.000    0.000    0.000    0.000 encoder.py:104(__init__)\n",
      "       23    0.000    0.000    0.001    0.000 encoder.py:182(encode)\n",
      "       20    0.000    0.000    0.000    0.000 encoder.py:204(iterencode)\n",
      "        4    0.000    0.000    0.000    0.000 encoder.py:223(floatstr)\n",
      "        1    0.000    0.000    0.000    0.000 encoder.py:259(_make_iterencode)\n",
      "        9    0.000    0.000    0.000    0.000 encoder.py:277(_iterencode_list)\n",
      "  429/277    0.000    0.000    0.000    0.000 encoder.py:333(_iterencode_dict)\n",
      "      277    0.000    0.000    0.001    0.000 encoder.py:413(_iterencode)\n",
      "      906    0.001    0.000    0.002    0.000 enum.py:359(__call__)\n",
      "        3    0.000    0.000    0.000    0.000 enum.py:451(__members__)\n",
      "      906    0.000    0.000    0.000    0.000 enum.py:678(__new__)\n",
      "        3    0.000    0.000    0.000    0.000 enum.py:801(value)\n",
      "       30    0.000    0.000    0.000    0.000 error_wrappers.py:31(__init__)\n",
      "       30    0.000    0.000    0.000    0.000 errors.py:123(__init__)\n",
      "       39    0.000    0.000    0.000    0.000 errors.py:7(__getattribute__)\n",
      "        3    0.000    0.000    0.000    0.000 expand_window.py:10(expand_window)\n",
      "        8    0.000    0.000    0.002    0.000 expand_window.py:18(forward)\n",
      "        8    0.000    0.000    0.002    0.000 expand_window.py:25(_expand_window_floats)\n",
      "        1    0.000    0.000    0.000    0.000 external_utils.py:6(is_compiling)\n",
      "        3    0.000    0.000    0.000    0.000 extmath.py:1214(_nanaverage)\n",
      "        2    0.000    0.000    0.000    0.000 featureextractor.py:14(forward)\n",
      "        3    0.000    0.000    0.000    0.000 featureextractor.py:9(FeatureExtractor)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:101(push)\n",
      "        4    0.000    0.000    0.000    0.000 feedparser.py:122(pushlines)\n",
      "        4    0.000    0.000    0.000    0.000 feedparser.py:125(__iter__)\n",
      "       44    0.000    0.000    0.000    0.000 feedparser.py:128(__next__)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:139(__init__)\n",
      "        2    0.000    0.000    0.001    0.000 feedparser.py:173(feed)\n",
      "        4    0.000    0.000    0.001    0.000 feedparser.py:178(_call_parse)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:184(close)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:197(_new_message)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:210(_pop_message)\n",
      "        4    0.000    0.000    0.001    0.000 feedparser.py:218(_parsegen)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:471(_parse_headers)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:53(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 feedparser.py:70(close)\n",
      "       44    0.000    0.000    0.000    0.000 feedparser.py:78(readline)\n",
      "  408/280    0.000    0.000    0.002    0.000 fields.py:1056(_validate_singleton)\n",
      "      320    0.001    0.000    0.001    0.000 fields.py:1152(_apply_validators)\n",
      "      246    0.000    0.000    0.002    0.000 fields.py:1255(is_finalvar_with_default_val)\n",
      "      371    0.004    0.000    0.004    0.000 fields.py:151(__init__)\n",
      "      246    0.001    0.000    0.002    0.000 fields.py:190(get_constraints)\n",
      "      246    0.001    0.000    0.001    0.000 fields.py:196(<setcomp>)\n",
      "      350    0.000    0.000    0.000    0.000 fields.py:217(_validate)\n",
      "  371/267    0.002    0.000    0.068    0.000 fields.py:394(__init__)\n",
      "     1074    0.001    0.000    0.001    0.000 fields.py:438(get_default)\n",
      "      350    0.002    0.000    0.010    0.000 fields.py:441(_get_field_info)\n",
      "      246    0.003    0.000    0.079    0.000 fields.py:485(infer)\n",
      "      703    0.000    0.000    0.000    0.000 fields.py:535(alt_alias)\n",
      "  371/267    0.001    0.000    0.063    0.000 fields.py:539(prepare)\n",
      "      371    0.000    0.000    0.001    0.000 fields.py:559(_set_default_and_type)\n",
      "  399/267    0.003    0.000    0.028    0.000 fields.py:583(_type_analysis)\n",
      "        6    0.000    0.000    0.003    0.000 fields.py:663(<listcomp>)\n",
      "       20    0.000    0.000    0.012    0.001 fields.py:751(<listcomp>)\n",
      "   104/52    0.001    0.000    0.017    0.000 fields.py:788(_create_sub_type)\n",
      "      104    0.000    0.000    0.000    0.000 fields.py:793(<dictcomp>)\n",
      "      371    0.003    0.000    0.041    0.000 fields.py:816(populate_validators)\n",
      "      371    0.000    0.000    0.000    0.000 fields.py:822(<genexpr>)\n",
      "      313    0.000    0.000    0.000    0.000 fields.py:830(<listcomp>)\n",
      "      313    0.000    0.000    0.000    0.000 fields.py:832(<listcomp>)\n",
      "  383/255    0.001    0.000    0.003    0.000 fields.py:852(validate)\n",
      "       14    0.000    0.000    0.001    0.000 fields.py:901(_validate_sequence_like)\n",
      "       10    0.001    0.000    0.007    0.001 file_download.py:1495(try_to_load_from_cache)\n",
      "        2    0.000    0.000    2.952    1.476 file_download.py:1588(get_hf_file_metadata)\n",
      "        2    0.000    0.000    0.000    0.000 file_download.py:1656(_int_or_none)\n",
      "        2    0.000    0.000    0.001    0.000 file_download.py:1688(_get_pointer_path)\n",
      "        2    0.000    0.000    0.000    0.000 file_download.py:184(hf_hub_url)\n",
      "      4/2    0.000    0.000    2.952    1.476 file_download.py:365(_request_wrapper)\n",
      "        2    0.000    0.000    0.000    0.000 file_download.py:826(_normalize_etag)\n",
      "        2    0.000    0.000    0.001    0.001 file_download.py:941(_cache_commit_hash_for_specific_revision)\n",
      "        2    0.000    0.000    0.000    0.000 file_download.py:956(repo_folder_name)\n",
      "        2    0.000    0.000    2.957    1.478 file_download.py:993(hf_hub_download)\n",
      "        1    0.000    0.000    0.000    0.000 format.py:196(_check_version)\n",
      "        1    0.000    0.000    0.000    0.000 format.py:223(read_magic)\n",
      "        1    0.000    0.000    0.000    0.000 format.py:282(descr_to_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 format.py:587(_read_array_header)\n",
      "        3    0.000    0.000    0.000    0.000 format.py:652(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 format.py:738(read_array)\n",
      "        3    0.000    0.000    0.000    0.000 format.py:951(_read_bytes)\n",
      "      690    0.000    0.000    0.000    0.000 fromnumeric.py:1136(_argmax_dispatcher)\n",
      "      690    0.002    0.000    0.004    0.000 fromnumeric.py:1140(argmax)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1328(_searchsorted_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1332(searchsorted)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1487(_squeeze_dispatcher)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1491(squeeze)\n",
      "      690    0.000    0.000    0.000    0.000 fromnumeric.py:1877(_nonzero_dispatcher)\n",
      "      690    0.001    0.000    0.002    0.000 fromnumeric.py:1881(nonzero)\n",
      "        7    0.000    0.000    0.000    0.000 fromnumeric.py:195(_reshape_dispatcher)\n",
      "        7    0.000    0.000    0.000    0.000 fromnumeric.py:200(reshape)\n",
      "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2172(_sum_dispatcher)\n",
      "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2177(sum)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2317(_any_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2322(any)\n",
      "        5    0.000    0.000    0.000    0.000 fromnumeric.py:2508(_cumsum_dispatcher)\n",
      "        5    0.000    0.000    0.000    0.000 fromnumeric.py:2512(cumsum)\n",
      "     1408    0.002    0.000    0.005    0.000 fromnumeric.py:53(_wrapfunc)\n",
      "       15    0.000    0.000    0.000    0.000 fromnumeric.py:533(_swapaxes_dispatcher)\n",
      "       15    0.000    0.000    0.000    0.000 fromnumeric.py:537(swapaxes)\n",
      "        9    0.000    0.000    0.000    0.000 fromnumeric.py:71(_wrapreduction)\n",
      "        9    0.000    0.000    0.000    0.000 fromnumeric.py:72(<dictcomp>)\n",
      "       24    0.000    0.000    0.000    0.000 function.py:27(save_for_backward)\n",
      "       24    0.000    0.000    1.017    0.042 function.py:534(apply)\n",
      "      690    0.000    0.000    0.000    0.000 function_base.py:1320(_diff_dispatcher)\n",
      "      690    0.008    0.000    0.008    0.000 function_base.py:1324(diff)\n",
      "        1    0.000    0.000    0.000    0.000 functional.py:1249(dropout)\n",
      "        1    0.000    0.000    0.000    0.000 functional.py:2122(embedding)\n",
      "       50    0.001    0.000    0.043    0.001 functional.py:2528(layer_norm)\n",
      "      288    0.002    0.000    0.003    0.000 functools.py:35(update_wrapper)\n",
      "      288    0.000    0.000    0.000    0.000 functools.py:65(wraps)\n",
      "      118    0.000    0.000    0.001    0.000 functools.py:818(dispatch)\n",
      "      118    0.000    0.000    0.159    0.001 functools.py:884(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:106(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:107(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:110(is_tensor)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:145(_is_torch)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:151(is_torch_tensor)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:330(__post_init__)\n",
      "       10    0.000    0.000    0.000    0.000 generic.py:340(<genexpr>)\n",
      "        7    0.000    0.000    0.000    0.000 generic.py:344(<genexpr>)\n",
      "      5/2    0.000    0.000    0.000    0.000 generic.py:398(__getitem__)\n",
      "       10    0.000    0.000    0.000    0.000 generic.py:405(__setattr__)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:411(__setitem__)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:424(to_tuple)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:428(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:493(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:497(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:501(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:74(infer_framework_from_repr)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:90(_get_frameworks_and_test_func)\n",
      "       24    0.000    0.000    0.003    0.000 genericpath.py:16(exists)\n",
      "       32    0.000    0.000    0.002    0.000 genericpath.py:27(isfile)\n",
      "       43    0.000    0.000    0.004    0.000 genericpath.py:39(isdir)\n",
      "        1    0.000    0.000    0.000    0.000 genericpath.py:48(getsize)\n",
      "      988    0.003    0.000    0.005    0.000 grad_mode.py:182(__init__)\n",
      "      494    0.002    0.000    0.003    0.000 grad_mode.py:73(__init__)\n",
      "      494    0.001    0.000    0.004    0.000 grad_mode.py:78(__enter__)\n",
      "      494    0.003    0.000    0.006    0.000 grad_mode.py:82(__exit__)\n",
      "       15    0.000    0.000    0.003    0.000 hashembed.py:15(HashEmbed)\n",
      "       10    0.000    0.000    0.004    0.000 hashembed.py:59(forward)\n",
      "        4    0.000    0.000    0.000    0.000 hooks.py:15(default_hooks)\n",
      "        4    0.000    0.000    0.000    0.000 hooks.py:16(<dictcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 hooks.py:22(dispatch_hook)\n",
      "        3    0.000    0.000    0.000    0.000 hub.py:149(is_remote_url)\n",
      "        2    0.000    0.000    0.000    0.000 hub.py:219(http_user_agent)\n",
      "        9    0.000    0.000    0.000    0.000 hub.py:242(extract_commit_hash)\n",
      "       12    0.000    0.000    2.966    0.247 hub.py:256(cached_file)\n",
      "       14    0.000    0.000    0.000    0.000 hub.py:80(is_offline_mode)\n",
      "        1    0.000    0.000    0.000    0.000 import_utils.py:1307(is_torch_fx_proxy)\n",
      "        2    0.000    0.000    0.000    0.000 import_utils.py:1348(__getattr__)\n",
      "       10    0.000    0.000    0.000    0.000 import_utils.py:258(is_torch_available)\n",
      "        3    0.000    0.000    0.000    0.000 import_utils.py:266(is_torch_sdpa_available)\n",
      "        1    0.000    0.000    0.000    0.000 import_utils.py:409(is_torch_fx_available)\n",
      "        2    0.000    0.000    0.000    0.000 import_utils.py:413(is_peft_available)\n",
      "        2    0.000    0.000    0.000    0.000 import_utils.py:421(is_tf_available)\n",
      "        1    0.000    0.000    0.000    0.000 import_utils.py:520(is_torchdynamo_compiling)\n",
      "        1    0.000    0.000    0.000    0.000 import_utils.py:615(is_bitsandbytes_available)\n",
      "        2    0.000    0.000    0.000    0.000 import_utils.py:723(is_safetensors_available)\n",
      "        2    0.000    0.000    0.000    0.000 import_utils.py:833(is_training_run_on_sagemaker)\n",
      "       50    0.000    0.000    0.002    0.000 init.py:202(ones_)\n",
      "       50    0.000    0.000    0.001    0.000 init.py:215(zeros_)\n",
      "      145    0.000    0.000    0.001    0.000 init.py:291(_calculate_fan_in_and_fan_out)\n",
      "       50    0.000    0.000    0.002    0.000 init.py:57(_no_grad_fill_)\n",
      "       50    0.000    0.000    0.001    0.000 init.py:62(_no_grad_zero_)\n",
      "       36    0.000    0.000    0.000    0.000 inspect.py:191(isclass)\n",
      "       34    0.000    0.000    0.000    0.000 inspect.py:2005(_signature_is_functionlike)\n",
      "      405    0.005    0.000    0.014    0.000 inspect.py:2279(_signature_from_function)\n",
      "      405    0.004    0.000    0.021    0.000 inspect.py:2374(_signature_from_callable)\n",
      "      844    0.002    0.000    0.004    0.000 inspect.py:2633(__init__)\n",
      "     1347    0.000    0.000    0.000    0.000 inspect.py:2683(name)\n",
      "      267    0.000    0.000    0.000    0.000 inspect.py:2687(default)\n",
      "      372    0.000    0.000    0.000    0.000 inspect.py:2691(annotation)\n",
      "      692    0.000    0.000    0.000    0.000 inspect.py:2695(kind)\n",
      "       58    0.000    0.000    0.001    0.000 inspect.py:2699(replace)\n",
      "      810    0.000    0.000    0.001    0.000 inspect.py:277(isfunction)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2775(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2836(apply_defaults)\n",
      "      463    0.003    0.000    0.004    0.000 inspect.py:2916(__init__)\n",
      "      934    0.001    0.000    0.001    0.000 inspect.py:2965(<genexpr>)\n",
      "      405    0.001    0.000    0.021    0.000 inspect.py:2994(from_callable)\n",
      "      429    0.000    0.000    0.000    0.000 inspect.py:3002(parameters)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3046(_bind)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3177(bind)\n",
      "      405    0.001    0.000    0.022    0.000 inspect.py:3248(signature)\n",
      "      405    0.001    0.000    0.002    0.000 inspect.py:612(unwrap)\n",
      "      405    0.000    0.000    0.000    0.000 inspect.py:632(_is_wrapper)\n",
      "      405    0.001    0.000    0.002    0.000 inspect.py:66(get_annotations)\n",
      "        1    0.000    0.000    0.000    0.000 interactiveshell.py:661(get_ipython)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:259(schedule)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:577(_schedule_flush)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:655(write)\n",
      "      460    0.003    0.000    0.009    0.000 ipkernel.py:770(_clean_thread_parent_frames)\n",
      "      230    0.001    0.000    0.002    0.000 ipkernel.py:785(<setcomp>)\n",
      "        1    0.000    0.000    0.203    0.203 language.py:1016(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:104(create_tokenizer)\n",
      "        1    0.000    0.000    0.266    0.266 language.py:110(tokenizer_factory)\n",
      "        1    0.000    0.000    0.033    0.033 language.py:1110(make_doc)\n",
      "        1    0.000    0.000    0.033    0.033 language.py:1122(_ensure_doc)\n",
      "        1    0.000    0.000    0.269    0.269 language.py:155(__init__)\n",
      "        8    0.000    0.000    0.005    0.001 language.py:1711(_link_components)\n",
      "        1    0.000    0.000    0.546    0.546 language.py:1733(from_config)\n",
      "        2    0.000    0.000    0.000    0.000 language.py:1953(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:2097(_resolve_component_status)\n",
      "        1    0.000    0.000    0.651    0.651 language.py:2132(from_disk)\n",
      "        1    0.000    0.000    0.001    0.001 language.py:2150(deserialize_meta)\n",
      "        1    0.000    0.000    0.129    0.129 language.py:2158(deserialize_vocab)\n",
      "        1    0.000    0.000    0.011    0.011 language.py:2165(<lambda>)\n",
      "        1    0.000    0.000    0.245    0.245 language.py:2170(<lambda>)\n",
      "        7    0.004    0.001    0.263    0.038 language.py:2178(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:233(meta)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:275(config)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:310(config)\n",
      "        2    0.000    0.000    0.000    0.000 language.py:314(disabled)\n",
      "        2    0.000    0.000    0.000    0.000 language.py:322(<listcomp>)\n",
      "       10    0.000    0.000    0.000    0.000 language.py:343(component_names)\n",
      "       10    0.000    0.000    0.000    0.000 language.py:350(<listcomp>)\n",
      "       26    0.000    0.000    0.001    0.000 language.py:353(pipeline)\n",
      "       26    0.000    0.000    0.000    0.000 language.py:361(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:364(pipe_names)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:370(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 language.py:384(pipe_labels)\n",
      "        7    0.000    0.000    0.000    0.000 language.py:400(has_factory)\n",
      "       28    0.000    0.000    0.000    0.000 language.py:406(get_factory_name)\n",
      "       14    0.000    0.000    0.000    0.000 language.py:417(get_factory_meta)\n",
      "        7    0.000    0.000    0.000    0.000 language.py:440(get_pipe_meta)\n",
      "        7    0.000    0.000    0.000    0.000 language.py:450(get_pipe_config)\n",
      "        7    0.000    0.000    0.242    0.035 language.py:652(create_pipe)\n",
      "        7    0.000    0.000    0.247    0.035 language.py:764(add_pipe)\n",
      "        7    0.000    0.000    0.000    0.000 language.py:834(_get_pipe_index)\n",
      "       35    0.000    0.000    0.000    0.000 language.py:851(<genexpr>)\n",
      "       28    0.000    0.000    0.000    0.000 language.py:855(<genexpr>)\n",
      "        6    0.000    0.000    0.000    0.000 layernorm.py:12(LayerNorm)\n",
      "       10    0.001    0.000    0.006    0.001 layernorm.py:23(forward)\n",
      "       10    0.001    0.000    0.001    0.000 layernorm.py:58(_begin_update_scale_shift)\n",
      "       10    0.000    0.000    0.004    0.000 layernorm.py:72(_get_moments)\n",
      "        3    0.000    0.000    0.000    0.000 lemmatizer.py:117(mode)\n",
      "        1    0.002    0.002    0.039    0.039 lemmatizer.py:121(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 lemmatizer.py:168(_validate_tables)\n",
      "      698    0.029    0.000    0.036    0.000 lemmatizer.py:196(rule_lemmatize)\n",
      "        1    0.000    0.000    0.012    0.012 lemmatizer.py:289(from_disk)\n",
      "        1    0.000    0.000    0.012    0.012 lemmatizer.py:302(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 lemmatizer.py:47(make_lemmatizer_scorer)\n",
      "        1    0.000    0.000    0.000    0.000 lemmatizer.py:60(get_lookups_config)\n",
      "        1    0.000    0.000    0.000    0.000 lemmatizer.py:75(__init__)\n",
      "      533    0.002    0.000    0.004    0.000 lemmatizer.py:8(is_base_form)\n",
      "     1790    0.010    0.000    0.014    0.000 lex_attrs.py:117(word_shape)\n",
      "     3539    0.002    0.000    0.003    0.000 lex_attrs.py:144(lower)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:148(prefix)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:152(suffix)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:156(is_alpha)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:160(is_digit)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:164(is_lower)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:168(is_space)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:172(is_title)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:176(is_upper)\n",
      "     1790    0.002    0.000    0.002    0.000 lex_attrs.py:180(is_stop)\n",
      "     1792    0.000    0.000    0.000    0.000 lex_attrs.py:184(get_lang)\n",
      "     1790    0.009    0.000    0.014    0.000 lex_attrs.py:22(like_num)\n",
      "     1790    0.003    0.000    0.005    0.000 lex_attrs.py:25(is_punct)\n",
      "     1790    0.003    0.000    0.003    0.000 lex_attrs.py:32(is_ascii)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:53(is_bracket)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:58(is_quote)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:65(is_left_punct)\n",
      "     1790    0.001    0.000    0.001    0.000 lex_attrs.py:72(is_right_punct)\n",
      "     1790    0.001    0.000    0.002    0.000 lex_attrs.py:77(is_currency)\n",
      "     1790    0.001    0.000    0.003    0.000 lex_attrs.py:85(like_email)\n",
      "     1790    0.006    0.000    0.008    0.000 lex_attrs.py:89(like_url)\n",
      "      145    0.002    0.000    0.003    0.000 linear.py:103(reset_parameters)\n",
      "      193    0.007    0.000    4.488    0.023 linear.py:113(forward)\n",
      "        4    0.000    0.000    0.000    0.000 linear.py:13(Linear)\n",
      "        2    0.000    0.000    0.001    0.000 linear.py:35(forward)\n",
      "      145    0.002    0.000    0.031    0.000 linear.py:90(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 list2array.py:15(list2array)\n",
      "        2    0.000    0.000    0.000    0.000 list2array.py:24(forward)\n",
      "        2    0.000    0.000    0.000    0.000 list2array.py:25(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 list2ragged.py:11(list2ragged)\n",
      "        2    0.000    0.000    0.000    0.000 list2ragged.py:20(forward)\n",
      "        2    0.000    0.000    0.000    0.000 list2ragged.py:24(<listcomp>)\n",
      "       21    0.000    0.000    0.000    0.000 lookups.py:100(__getitem__)\n",
      "     1245    0.001    0.000    0.002    0.000 lookups.py:109(get)\n",
      "     1026    0.001    0.000    0.003    0.000 lookups.py:119(__contains__)\n",
      "        2    0.000    0.000    0.000    0.000 lookups.py:170(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 lookups.py:177(__contains__)\n",
      "     1000    0.000    0.000    0.000    0.000 lookups.py:220(get_table)\n",
      "        3    0.000    0.000    0.000    0.000 lookups.py:248(has_table)\n",
      "        2    0.001    0.001    0.020    0.010 lookups.py:267(from_bytes)\n",
      "        2    0.000    0.000    0.021    0.011 lookups.py:297(from_disk)\n",
      "        4    0.000    0.000    0.006    0.002 lookups.py:64(__init__)\n",
      "     3523    0.004    0.000    0.004    0.000 lookups.py:81(__setitem__)\n",
      "       58    0.000    0.000    0.000    0.000 main.py:102(generate_hash_function)\n",
      "       92    0.003    0.000    0.007    0.000 main.py:1032(validate_model)\n",
      "       58    0.007    0.000    0.119    0.002 main.py:122(__new__)\n",
      "       58    0.002    0.000    0.003    0.000 main.py:148(<setcomp>)\n",
      "       58    0.000    0.000    0.000    0.000 main.py:153(<dictcomp>)\n",
      "      666    0.001    0.000    0.001    0.000 main.py:174(is_untouched)\n",
      "       58    0.000    0.000    0.000    0.000 main.py:255(<dictcomp>)\n",
      "       58    0.000    0.000    0.000    0.000 main.py:259(<dictcomp>)\n",
      "       58    0.000    0.000    0.000    0.000 main.py:279(<dictcomp>)\n",
      "     1040    0.000    0.000    0.001    0.000 main.py:298(__instancecheck__)\n",
      "       92    0.000    0.000    0.008    0.000 main.py:332(__init__)\n",
      "       92    0.000    0.000    0.000    0.000 main.py:421(_init_private_attributes)\n",
      "       92    0.001    0.000    0.010    0.000 main.py:427(dict)\n",
      "       73    0.000    0.000    0.000    0.000 main.py:506(_enforce_dict_if_root)\n",
      "       73    0.000    0.000    0.006    0.000 main.py:517(parse_obj)\n",
      " 1040/972    0.001    0.000    0.004    0.000 main.py:727(_get_value)\n",
      "       82    0.000    0.000    0.000    0.000 main.py:778(<genexpr>)\n",
      "       58    0.000    0.000    0.001    0.000 main.py:802(__try_update_forward_refs__)\n",
      "     1064    0.002    0.000    0.009    0.000 main.py:823(_iter)\n",
      "       92    0.000    0.000    0.000    0.000 main.py:880(_calculate_keys)\n",
      "       73    0.000    0.000    0.000    0.000 main.py:903(<setcomp>)\n",
      "       58    0.002    0.000    0.123    0.002 main.py:952(create_model)\n",
      "      188    0.001    0.000    0.001    0.000 matcher.pyx:1064(_get_extra_predicates)\n",
      "       22    0.000    0.000    0.001    0.000 matcher.pyx:1110(_get_extra_predicates_dict)\n",
      "      188    0.000    0.000    0.000    0.000 matcher.pyx:1169(_get_operators)\n",
      "      188    0.000    0.000    0.000    0.000 matcher.pyx:1206(_get_extensions)\n",
      "      179    0.000    0.000    0.000    0.000 matcher.pyx:128(genexpr)\n",
      "        1    0.000    0.000    0.000    0.000 matcher.pyx:151(_require_patterns)\n",
      "        1    0.001    0.001    0.024    0.024 matcher.pyx:214(__call__)\n",
      "      179    0.000    0.000    0.000    0.000 matcher.pyx:337(_normalize_key)\n",
      "        1    0.002    0.002    0.023    0.023 matcher.pyx:352(find_matches)\n",
      "        1    0.000    0.000    0.000    0.000 matcher.pyx:39(__init__)\n",
      "      753    0.005    0.000    0.021    0.000 matcher.pyx:430(transition_states)\n",
      "    12801    0.007    0.000    0.016    0.000 matcher.pyx:572(update_predicate_cache)\n",
      "        1    0.000    0.000    0.000    0.000 matcher.pyx:596(finish_states)\n",
      "        1    0.000    0.000    0.000    0.000 matcher.pyx:62(__len__)\n",
      "      180    0.000    0.000    0.000    0.000 matcher.pyx:769(init_pattern)\n",
      "      179    0.001    0.000    0.005    0.000 matcher.pyx:79(add)\n",
      "      180    0.001    0.000    0.003    0.000 matcher.pyx:818(_preprocess_pattern)\n",
      "      188    0.001    0.000    0.001    0.000 matcher.pyx:851(_get_attr_values)\n",
      "       22    0.000    0.000    0.000    0.000 matcher.pyx:887(_predicate_cache_key)\n",
      "        3    0.000    0.000    0.000    0.000 matcher.pyx:926(__init__)\n",
      "     1506    0.002    0.000    0.002    0.000 matcher.pyx:937(__call__)\n",
      "       19    0.000    0.000    0.000    0.000 matcher.pyx:948(__init__)\n",
      "       19    0.000    0.000    0.000    0.000 matcher.pyx:966(genexpr)\n",
      "    12048    0.007    0.000    0.007    0.000 matcher.pyx:973(__call__)\n",
      "        6    0.000    0.000    0.001    0.000 maxout.py:16(Maxout)\n",
      "       10    0.001    0.000    0.044    0.004 maxout.py:45(forward)\n",
      "        4    0.000    0.000    0.000    0.000 message.py:120(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 message.py:181(is_multipart)\n",
      "        2    0.000    0.000    0.000    0.000 message.py:213(get_payload)\n",
      "        8    0.000    0.000    0.000    0.000 message.py:29(_splitparam)\n",
      "        2    0.000    0.000    0.000    0.000 message.py:303(set_payload)\n",
      "        2    0.000    0.000    0.000    0.000 message.py:451(items)\n",
      "        2    0.000    0.000    0.000    0.000 message.py:459(<listcomp>)\n",
      "       16    0.000    0.000    0.000    0.000 message.py:462(get)\n",
      "       38    0.000    0.000    0.000    0.000 message.py:479(set_raw)\n",
      "        8    0.000    0.000    0.000    0.000 message.py:497(get_all)\n",
      "        8    0.000    0.000    0.000    0.000 message.py:564(get_content_type)\n",
      "        6    0.000    0.000    0.000    0.000 message.py:588(get_content_maintype)\n",
      "      846    0.000    0.000    0.000    0.000 model.py:124(layers)\n",
      "       56    0.000    0.000    0.000    0.000 model.py:131(shims)\n",
      "      403    0.000    0.000    0.000    0.000 model.py:135(attrs)\n",
      "      112    0.000    0.000    0.000    0.000 model.py:142(param_names)\n",
      "       56    0.000    0.000    0.000    0.000 model.py:147(grad_names)\n",
      "       56    0.000    0.000    0.000    0.000 model.py:150(<listcomp>)\n",
      "       11    0.000    0.000    0.000    0.000 model.py:152(dim_names)\n",
      "      170    0.000    0.000    0.000    0.000 model.py:176(has_dim)\n",
      "      103    0.000    0.000    0.000    0.000 model.py:187(get_dim)\n",
      "      246    0.001    0.000    0.001    0.000 model.py:198(set_dim)\n",
      "      458    0.000    0.000    0.000    0.000 model.py:205(<genexpr>)\n",
      "       21    0.000    0.000    0.000    0.000 model.py:214(maybe_get_dim)\n",
      "       28    0.000    0.000    0.000    0.000 model.py:218(has_param)\n",
      "       68    0.000    0.000    0.000    0.000 model.py:230(get_param)\n",
      "       85    0.000    0.000    0.000    0.000 model.py:244(set_param)\n",
      "       28    0.000    0.000    0.000    0.000 model.py:252(has_grad)\n",
      "        2    0.000    0.000    0.000    0.000 model.py:283(get_ref)\n",
      "       30    0.000    0.000    0.000    0.000 model.py:298(set_ref)\n",
      "    135/8    0.000    0.000    0.066    0.008 model.py:307(__call__)\n",
      "        4    0.000    0.000    0.066    0.017 model.py:330(predict)\n",
      "       56    0.000    0.000    0.000    0.000 model.py:374(walk)\n",
      "      810    0.001    0.000    0.002    0.000 model.py:389(_walk_bfs)\n",
      "        7    0.000    0.000    0.003    0.000 model.py:478(copy)\n",
      "     56/7    0.001    0.000    0.003    0.000 model.py:486(_copy)\n",
      "        5    0.000    0.000    0.198    0.040 model.py:628(from_bytes)\n",
      "        5    0.002    0.000    0.167    0.033 model.py:649(from_dict)\n",
      "      118    0.000    0.000    0.158    0.001 model.py:843(deserialize_attr)\n",
      "      192    0.004    0.000    0.005    0.000 model.py:85(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 model.py:868(set_dropout_rate)\n",
      "        1    0.000    0.000    8.108    8.108 modeling_deberta_v2.py:1022(forward)\n",
      "       24    0.009    0.000    1.014    0.042 modeling_deberta_v2.py:107(forward)\n",
      "        1    0.000    0.000    0.112    0.112 modeling_deberta_v2.py:1369(__init__)\n",
      "        1    0.000    0.000    8.109    8.109 modeling_deberta_v2.py:1380(forward)\n",
      "       97    0.000    0.000    0.005    0.000 modeling_deberta_v2.py:220(__init__)\n",
      "       97    0.000    0.000    0.000    0.000 modeling_deberta_v2.py:226(forward)\n",
      "       24    0.000    0.000    0.011    0.000 modeling_deberta_v2.py:263(__init__)\n",
      "       24    0.014    0.001    0.374    0.016 modeling_deberta_v2.py:269(forward)\n",
      "       24    0.000    0.000    0.039    0.002 modeling_deberta_v2.py:278(__init__)\n",
      "       24    0.001    0.000    5.387    0.224 modeling_deberta_v2.py:284(forward)\n",
      "       24    0.000    0.000    0.006    0.000 modeling_deberta_v2.py:315(__init__)\n",
      "       24    0.001    0.000    1.351    0.056 modeling_deberta_v2.py:323(forward)\n",
      "       24    0.000    0.000    0.063    0.003 modeling_deberta_v2.py:331(__init__)\n",
      "       24    0.012    0.001    1.320    0.055 modeling_deberta_v2.py:338(forward)\n",
      "       24    0.000    0.000    0.109    0.005 modeling_deberta_v2.py:347(__init__)\n",
      "       24    0.002    0.000    8.061    0.336 modeling_deberta_v2.py:353(forward)\n",
      "        1    0.000    0.000    0.111    0.111 modeling_deberta_v2.py:419(__init__)\n",
      "        1    0.000    0.000    0.109    0.109 modeling_deberta_v2.py:422(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_deberta_v2.py:438(<listcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 modeling_deberta_v2.py:446(get_rel_embedding)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_deberta_v2.py:452(get_attention_mask)\n",
      "        1    0.000    0.000    0.012    0.012 modeling_deberta_v2.py:461(get_rel_pos)\n",
      "        1    0.001    0.001    8.077    8.077 modeling_deberta_v2.py:473(forward)\n",
      "        1    0.006    0.006    0.011    0.011 modeling_deberta_v2.py:549(make_log_bucket_position)\n",
      "        1    0.001    0.001    0.012    0.012 modeling_deberta_v2.py:564(build_relative_position)\n",
      "       24    0.001    0.000    0.027    0.001 modeling_deberta_v2.py:623(__init__)\n",
      "      120    0.004    0.000    0.069    0.001 modeling_deberta_v2.py:661(transpose_for_scores)\n",
      "       24    0.165    0.007    5.010    0.209 modeling_deberta_v2.py:666(forward)\n",
      "       24    0.644    0.027    2.073    0.086 modeling_deberta_v2.py:750(disentangled_attention_bias)\n",
      "        1    0.000    0.000    0.001    0.001 modeling_deberta_v2.py:833(__init__)\n",
      "        1    0.029    0.029    0.030    0.030 modeling_deberta_v2.py:859(forward)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_deberta_v2.py:918(_init_weights)\n",
      "        1    0.000    0.000    0.111    0.111 modeling_deberta_v2.py:999(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:1236(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:1258(post_init)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:1266(_backward_compatibility_gradient_checkpointing)\n",
      "        3    0.000    0.000    0.000    0.000 modeling_utils.py:131(is_fsdp_enabled)\n",
      "        3    0.000    0.000    0.000    0.000 modeling_utils.py:1344(_autoset_attn_implementation)\n",
      "        3    0.000    0.000    0.000    0.000 modeling_utils.py:1440(can_generate)\n",
      "        3    0.000    0.000    0.000    0.000 modeling_utils.py:1543(_check_and_enable_sdpa)\n",
      "        3    0.000    0.000    0.000    0.000 modeling_utils.py:1616(get_output_embeddings)\n",
      "      468    0.000    0.000    0.000    0.000 modeling_utils.py:1634(_initialize_weights)\n",
      "        3    0.001    0.000    0.015    0.005 modeling_utils.py:1643(tie_weights)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:177(no_init_weights)\n",
      "      292    0.000    0.000    0.000    0.000 modeling_utils.py:190(_skip_init)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:2070(init_weights)\n",
      "        1    0.001    0.001    1.215    1.215 modeling_utils.py:2619(from_pretrained)\n",
      "        1    0.002    0.002    0.961    0.961 modeling_utils.py:3952(_load_pretrained_model)\n",
      "      393    0.000    0.000    0.000    0.000 modeling_utils.py:4001(_fix_key)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_utils.py:4009(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:4012(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 modeling_utils.py:4013(<genexpr>)\n",
      "        1    0.000    0.000    0.003    0.003 modeling_utils.py:4034(<setcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_utils.py:4049(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_utils.py:4071(<listcomp>)\n",
      "      392    0.000    0.000    0.000    0.000 modeling_utils.py:4135(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 modeling_utils.py:4155(_find_mismatched_keys)\n",
      "        1    0.000    0.000    0.001    0.001 modeling_utils.py:4505(warn_if_padding_and_no_attention_mask)\n",
      "        1    0.000    0.000    0.125    0.125 modeling_utils.py:503(load_state_dict)\n",
      "        1    0.006    0.006    0.203    0.203 modeling_utils.py:561(set_initialized_submodules)\n",
      "      468    0.080    0.000    0.132    0.000 modeling_utils.py:568(<setcomp>)\n",
      "        1    0.000    0.000    0.718    0.718 modeling_utils.py:576(_load_state_dict_into_model)\n",
      "    468/1    0.008    0.000    0.718    0.718 modeling_utils.py:602(load)\n",
      "      468    0.051    0.000    0.101    0.000 modeling_utils.py:607(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 modeling_utils.py:846(_add_variant)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:106(_encode_params)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:207(register_hook)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:216(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:258(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:335(__init__)\n",
      "        2    0.000    0.000    0.001    0.000 models.py:352(prepare)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:394(prepare_method)\n",
      "        2    0.000    0.000    0.001    0.000 models.py:410(prepare_url)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:484(prepare_headers)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:495(prepare_body)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:573(prepare_content_length)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:589(prepare_auth)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:611(prepare_cookies)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:631(prepare_hooks)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:659(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:770(is_redirect)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:795(iter_content)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:812(generate)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:85(path_url)\n",
      "        2    0.000    0.000    0.000    0.000 models.py:887(content)\n",
      "        4    0.000    0.000    0.000    0.000 models.py:994(raise_for_status)\n",
      "    514/1    0.003    0.000    8.109    8.109 module.py:1514(_wrapped_call_impl)\n",
      "    514/1    0.007    0.000    8.109    8.109 module.py:1520(_call_impl)\n",
      "     3539    0.012    0.000    0.012    0.000 module.py:1682(__getattr__)\n",
      "     2880    0.012    0.000    0.027    0.000 module.py:1697(__setattr__)\n",
      "      835    0.001    0.000    0.001    0.000 module.py:1698(remove_from)\n",
      "     4237    0.016    0.000    0.034    0.000 module.py:1788(_save_to_state_dict)\n",
      " 4237/470    0.031    0.000    0.068    0.000 module.py:1825(state_dict)\n",
      "      346    0.065    0.000    0.608    0.002 module.py:1954(_load_from_state_dict)\n",
      "      346    0.000    0.000    0.000    0.000 module.py:1993(<dictcomp>)\n",
      "      346    0.001    0.000    0.001    0.000 module.py:1995(<dictcomp>)\n",
      "      395    0.002    0.000    0.008    0.000 module.py:2156(_named_members)\n",
      "      393    0.000    0.000    0.005    0.000 module.py:2195(named_parameters)\n",
      "      468    0.000    0.000    0.000    0.000 module.py:2224(<lambda>)\n",
      "        2    0.000    0.000    0.003    0.002 module.py:2251(named_buffers)\n",
      "      468    0.000    0.000    0.000    0.000 module.py:2274(<lambda>)\n",
      "     1870    0.001    0.000    0.003    0.000 module.py:2278(children)\n",
      "     1870    0.002    0.000    0.002    0.000 module.py:2287(named_children)\n",
      "     1407    0.001    0.000    0.008    0.000 module.py:2308(modules)\n",
      "22614/2814    0.016    0.000    0.017    0.000 module.py:2335(named_modules)\n",
      "    468/1    0.001    0.000    0.007    0.007 module.py:2379(train)\n",
      "        1    0.000    0.000    0.007    0.007 module.py:2401(eval)\n",
      "      468    0.008    0.000    0.009    0.000 module.py:451(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 module.py:496(register_buffer)\n",
      "      392    0.002    0.000    0.004    0.000 module.py:554(register_parameter)\n",
      "       24    0.000    0.000    0.000    0.000 module.py:599(add_module)\n",
      "    468/1    0.001    0.000    0.003    0.003 module.py:861(apply)\n",
      "        3    0.000    0.000    0.000    0.000 multiarray.py:1080(copyto)\n",
      "      702    0.000    0.000    0.000    0.000 multiarray.py:153(concatenate)\n",
      "        2    0.000    0.000    0.000    0.000 multiarray.py:346(where)\n",
      "        3    0.000    0.000    0.000    0.000 multiarray.py:892(bincount)\n",
      "        6    0.000    0.000    0.001    0.000 multiclass.py:113(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 multiclass.py:115(<genexpr>)\n",
      "        8    0.000    0.000    0.002    0.000 multiclass.py:128(is_multilabel)\n",
      "        4    0.000    0.000    0.001    0.000 multiclass.py:21(_unique_multiclass)\n",
      "        8    0.000    0.000    0.006    0.001 multiclass.py:228(type_of_target)\n",
      "        2    0.000    0.000    0.003    0.001 multiclass.py:43(unique_labels)\n",
      "        6    0.000    0.000    0.002    0.000 multiclass.py:80(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 nanfunctions.py:187(_divide_by_count)\n",
      "        3    0.000    0.000    0.000    0.000 nanfunctions.py:68(_replace_nan)\n",
      "        3    0.000    0.000    0.000    0.000 nanfunctions.py:947(_nanmean_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 nanfunctions.py:952(nanmean)\n",
      "        1    0.000    0.000    0.000    0.000 ner.pyx:187(make_ner_scorer)\n",
      "        1    0.000    0.000    0.000    0.000 ner.pyx:198(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 ner.pyx:245(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 ner.pyx:248(genexpr)\n",
      "        1    0.000    0.000    0.000    0.000 ner.pyx:52(make_ner)\n",
      "        1    0.000    0.000    0.000    0.000 nonproj.pyx:176(deprojectivize (wrapper))\n",
      "        1    0.000    0.000    0.000    0.000 nonproj.pyx:176(deprojectivize)\n",
      "       50    0.001    0.000    0.061    0.001 normalization.py:167(__init__)\n",
      "       50    0.000    0.000    0.003    0.000 normalization.py:189(reset_parameters)\n",
      "       50    0.001    0.000    0.045    0.001 normalization.py:195(forward)\n",
      "        1    0.000    0.000    0.001    0.001 npyio.py:282(load)\n",
      "       95    0.001    0.000    0.001    0.000 ntpath.py:103(join)\n",
      "      227    0.001    0.000    0.001    0.000 ntpath.py:150(splitdrive)\n",
      "       13    0.000    0.000    0.000    0.000 ntpath.py:206(split)\n",
      "        3    0.000    0.000    0.000    0.000 ntpath.py:240(basename)\n",
      "        4    0.000    0.000    0.000    0.000 ntpath.py:247(dirname)\n",
      "        4    0.000    0.000    0.000    0.000 ntpath.py:315(expanduser)\n",
      "       17    0.000    0.000    0.000    0.000 ntpath.py:35(_get_bothseps)\n",
      "        4    0.000    0.000    0.000    0.000 ntpath.py:489(normpath)\n",
      "        4    0.000    0.000    0.000    0.000 ntpath.py:563(abspath)\n",
      "        3    0.000    0.000    0.000    0.000 numeric.py:1855(isscalar)\n",
      "       10    0.006    0.001    0.007    0.001 numpy_ops.pyx:165(__pyx_fuse_0maxout)\n",
      "       10    0.000    0.000    0.000    0.000 numpy_ops.pyx:165(maxout)\n",
      "        8    0.000    0.000    0.002    0.000 numpy_ops.pyx:228(seq2col)\n",
      "       10    0.000    0.000    0.000    0.000 numpy_ops.pyx:304(hash)\n",
      "       10    0.003    0.000    0.003    0.000 numpy_ops.pyx:460(__pyx_fuse_0_1gather_add)\n",
      "       10    0.000    0.000    0.000    0.000 numpy_ops.pyx:460(gather_add)\n",
      "        4    0.000    0.000    0.000    0.000 numpy_ops.pyx:52(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 numpy_ops.pyx:535(check_seq2col_lengths)\n",
      "      198    0.000    0.000    0.000    0.000 numpy_ops.pyx:65(asarray)\n",
      "       40    0.002    0.000    0.002    0.000 numpy_ops.pyx:82(alloc)\n",
      "       12    0.000    0.000    0.000    0.000 numpy_ops.pyx:88(cblas)\n",
      "       15    0.040    0.003    0.040    0.003 numpy_ops.pyx:91(gemm)\n",
      "        1    0.000    0.000    0.000    0.000 ops.py:259(affine)\n",
      "        7    0.000    0.000    0.000    0.000 ops.py:319(flatten)\n",
      "        7    0.000    0.000    0.000    0.000 ops.py:331(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 ops.py:364(unflatten)\n",
      "        2    0.000    0.000    0.000    0.000 ops.py:375(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 ops.py:508(alloc2f)\n",
      "       10    0.000    0.000    0.000    0.000 ops.py:629(reshape1f)\n",
      "       10    0.000    0.000    0.000    0.000 ops.py:632(reshape2f)\n",
      "       10    0.000    0.000    0.000    0.000 ops.py:635(reshape3f)\n",
      "       30    0.000    0.000    0.000    0.000 ops.py:661(reshape)\n",
      "       10    0.000    0.000    0.000    0.000 ops.py:699(asarray_f)\n",
      "       15    0.000    0.000    0.000    0.000 ops.py:707(asarray1i)\n",
      "        2    0.000    0.000    0.000    0.000 ops.py:712(asarray2i)\n",
      "       42    0.000    0.000    0.000    0.000 ops.py:763(as_contig)\n",
      "        6    0.000    0.000    0.002    0.000 os.py:200(makedirs)\n",
      "       35    0.000    0.000    0.000    0.000 os.py:675(__getitem__)\n",
      "      288    0.000    0.000    0.000    0.000 os.py:698(__iter__)\n",
      "       35    0.000    0.000    0.000    0.000 os.py:741(check_str)\n",
      "       35    0.000    0.000    0.000    0.000 os.py:747(encodekey)\n",
      "        1    0.000    0.000    0.000    0.000 os.py:772(getenv)\n",
      "      392    0.001    0.000    0.001    0.000 overrides.py:1776(is_tensor_like)\n",
      "      392    0.000    0.000    0.001    0.000 parameter.py:164(is_lazy)\n",
      "      392    0.000    0.000    0.003    0.000 parameter.py:33(__new__)\n",
      "     2488    0.005    0.000    0.007    0.000 parameter.py:8(__instancecheck__)\n",
      "       56    0.000    0.000    0.000    0.000 parse.py:103(_noop)\n",
      "       56    0.000    0.000    0.000    0.000 parse.py:114(_coerce_args)\n",
      "        2    0.000    0.000    0.000    0.000 parse.py:151(username)\n",
      "       16    0.000    0.000    0.000    0.000 parse.py:159(hostname)\n",
      "        2    0.000    0.000    0.000    0.000 parse.py:189(_userinfo)\n",
      "       16    0.000    0.000    0.000    0.000 parse.py:201(_hostinfo)\n",
      "        2    0.000    0.000    0.000    0.000 parse.py:336(geturl)\n",
      "       23    0.000    0.000    0.000    0.000 parse.py:372(urlparse)\n",
      "       25    0.000    0.000    0.000    0.000 parse.py:437(urlsplit)\n",
      "        4    0.000    0.000    0.000    0.000 parse.py:494(urlunparse)\n",
      "        4    0.000    0.000    0.000    0.000 parse.py:505(urlunsplit)\n",
      "        2    0.000    0.000    0.000    0.000 parse.py:644(unquote)\n",
      "        6    0.000    0.000    0.000    0.000 parse.py:818(quote)\n",
      "        6    0.000    0.000    0.000    0.000 parse.py:889(quote_from_bytes)\n",
      "        2    0.000    0.000    0.000    0.000 parse.py:911(urlencode)\n",
      "        2    0.000    0.000    0.000    0.000 parser.py:102(resize_output)\n",
      "        2    0.000    0.000    0.000    0.000 parser.py:108(_resize_upper)\n",
      "        2    0.000    0.000    0.011    0.006 parser.py:14(build_tb_parser_model)\n",
      "        2    0.000    0.000    0.000    0.000 parser.py:17(__init__)\n",
      "        2    0.000    0.000    0.001    0.000 parser.py:41(parse)\n",
      "        2    0.000    0.000    0.001    0.000 parser.py:59(parsestr)\n",
      "        2    0.000    0.000    0.000    0.000 parser.py:94(_define_upper)\n",
      "        2    0.000    0.000    0.000    0.000 parser.py:98(_define_lower)\n",
      "       34    0.000    0.000    0.005    0.000 pathlib.py:1092(stat)\n",
      "       26    0.000    0.000    0.004    0.000 pathlib.py:1111(open)\n",
      "        6    0.000    0.000    0.001    0.000 pathlib.py:1129(read_text)\n",
      "        2    0.000    0.000    0.000    0.000 pathlib.py:1170(mkdir)\n",
      "       30    0.000    0.000    0.005    0.000 pathlib.py:1285(exists)\n",
      "        2    0.000    0.000    0.000    0.000 pathlib.py:1300(is_dir)\n",
      "        2    0.000    0.000    0.000    0.000 pathlib.py:1316(is_file)\n",
      "       65    0.000    0.000    0.000    0.000 pathlib.py:147(splitroot)\n",
      "        6    0.000    0.000    0.000    0.000 pathlib.py:189(casefold_parts)\n",
      "        6    0.000    0.000    0.000    0.000 pathlib.py:190(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 pathlib.py:38(_ignore_error)\n",
      "        2    0.000    0.000    0.000    0.000 pathlib.py:512(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 pathlib.py:519(__len__)\n",
      "        4    0.000    0.000    0.000    0.000 pathlib.py:525(__getitem__)\n",
      "       57    0.000    0.000    0.001    0.000 pathlib.py:56(parse_parts)\n",
      "       57    0.000    0.000    0.001    0.000 pathlib.py:569(_parse_args)\n",
      "       14    0.000    0.000    0.001    0.000 pathlib.py:589(_from_parts)\n",
      "       51    0.000    0.000    0.000    0.000 pathlib.py:600(_from_parsed_parts)\n",
      "       41    0.000    0.000    0.000    0.000 pathlib.py:608(_format_parsed_parts)\n",
      "       43    0.000    0.000    0.001    0.000 pathlib.py:615(_make_child)\n",
      "       69    0.000    0.000    0.000    0.000 pathlib.py:621(__str__)\n",
      "       65    0.000    0.000    0.000    0.000 pathlib.py:631(__fspath__)\n",
      "        2    0.000    0.000    0.000    0.000 pathlib.py:634(as_posix)\n",
      "        8    0.000    0.000    0.000    0.000 pathlib.py:654(_cparts)\n",
      "       13    0.000    0.000    0.000    0.000 pathlib.py:663(__eq__)\n",
      "       43    0.000    0.000    0.001    0.000 pathlib.py:853(__truediv__)\n",
      "        4    0.000    0.000    0.000    0.000 pathlib.py:865(parent)\n",
      "        2    0.000    0.000    0.000    0.000 pathlib.py:875(parents)\n",
      "       43    0.000    0.000    0.000    0.000 pathlib.py:94(join_parsed_parts)\n",
      "       14    0.000    0.000    0.001    0.000 pathlib.py:957(__new__)\n",
      "      732    0.012    0.000    0.038    0.000 phrasematcher.pyx:158(add)\n",
      "       19    0.000    0.000    0.006    0.000 phrasematcher.pyx:31(__init__)\n",
      "      732    0.001    0.000    0.001    0.000 phrasematcher.pyx:340(_convert_to_array)\n",
      "       73    0.000    0.000    0.000    0.000 pipe.pxd:2(__get__)\n",
      "       39    0.000    0.000    0.000    0.000 pipe.pxd:2(__set__)\n",
      "        6    0.000    0.000    0.000    0.000 pipe.pyx:104(__get__)\n",
      "       12    0.000    0.000    0.000    0.000 pipe.pyx:108(__get__)\n",
      "       12    0.000    0.000    0.000    0.000 pipe.pyx:135(get_error_handler)\n",
      "        3    0.000    0.000    0.002    0.001 pipe.pyx:147(deserialize_config)\n",
      "        2    0.000    0.000    0.000    0.000 poolmanager.py:277(connection_from_host)\n",
      "        2    0.000    0.000    0.000    0.000 poolmanager.py:306(connection_from_context)\n",
      "        2    0.000    0.000    0.000    0.000 poolmanager.py:331(connection_from_pool_key)\n",
      "        2    0.000    0.000    0.001    0.000 poolmanager.py:357(connection_from_url)\n",
      "        2    0.000    0.000    0.000    0.000 poolmanager.py:375(_merge_pool_kwargs)\n",
      "        2    0.000    0.000    0.000    0.000 poolmanager.py:96(_default_key_normalizer)\n",
      "        2    0.000    0.000    0.000    0.000 proxy.py:11(connection_requires_http_tunnel)\n",
      "        1    0.000    0.000    0.000    0.000 py3k.py:49(isfileobj)\n",
      "      392    0.001    0.000    0.002    0.000 pytorch_utils.py:277(id_tensor_storage)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:122(put)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:154(get)\n",
      "        4    0.000    0.000    0.000    0.000 queue.py:248(_qsize)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:251(_put)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:254(_get)\n",
      "        3    0.000    0.000    0.000    0.000 ragged2list.py:11(ragged2list)\n",
      "        2    0.000    0.000    0.000    0.000 ragged2list.py:17(forward)\n",
      "        2    0.000    0.000    0.000    0.000 re.py:197(search)\n",
      "      341    0.000    0.000    0.001    0.000 re.py:202(sub)\n",
      "       10    0.000    0.000    0.000    0.000 re.py:249(compile)\n",
      "      353    0.000    0.000    0.000    0.000 re.py:288(_compile)\n",
      "        2    0.000    0.000    0.000    0.000 request.py:134(set_file_position)\n",
      "        2    0.000    0.000    0.000    0.000 request.py:189(body_to_chunks)\n",
      "        4    0.000    0.000    0.000    0.000 request.py:2487(getproxies_environment)\n",
      "        2    0.000    0.000    0.000    0.000 request.py:2661(getproxies_registry)\n",
      "        2    0.000    0.000    0.000    0.000 request.py:2710(getproxies)\n",
      "        3    0.000    0.000    0.000    0.000 residual.py:14(residual)\n",
      "        8    0.001    0.000    0.041    0.005 residual.py:28(forward)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:242(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 response.py:246(__len__)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:299(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 response.py:325(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 response.py:376(retries)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:40(assert_header_parsing)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:412(_init_decoder)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:539(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:599(release_conn)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:643(_init_length)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:670(<setcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 response.py:699(_error_catcher)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:755(_fp_read)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:79(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:799(_raw_read)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:841(read)\n",
      "        4    0.000    0.000    0.000    0.000 response.py:9(is_fp_closed)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:912(stream)\n",
      "        2    0.000    0.000    0.000    0.000 response.py:953(closed)\n",
      "        2    0.000    0.000    0.000    0.000 retry.py:375(_is_method_retryable)\n",
      "        2    0.000    0.000    0.000    0.000 retry.py:383(is_retry)\n",
      "      246    0.000    0.000    0.003    0.000 schema.py:1002(get_annotation_from_field_info)\n",
      "        2    0.000    0.000    0.000    0.000 senter.pyx:102(hide_labels)\n",
      "        1    0.000    0.000    0.000    0.000 senter.pyx:43(make_senter)\n",
      "        1    0.000    0.000    0.000    0.000 senter.pyx:57(make_senter_scorer)\n",
      "        1    0.000    0.000    0.000    0.000 senter.pyx:66(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:109(_is_zipfile)\n",
      "     1179    0.000    0.000    0.001    0.000 serialization.py:1275(_maybe_decode_ascii)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:1287(_get_restore_location)\n",
      "      393    0.000    0.000    0.001    0.000 serialization.py:1295(restore_location)\n",
      "        1    0.000    0.000    0.122    0.122 serialization.py:1317(_load)\n",
      "      393    0.014    0.000    0.020    0.000 serialization.py:1351(load_tensor)\n",
      "      393    0.002    0.000    0.027    0.000 serialization.py:1375(persistent_load)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:1404(UnpicklerWrapper)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:1431(_is_torchscript_zip)\n",
      "      393    0.000    0.000    0.000    0.000 serialization.py:249(_cpu_deserialize)\n",
      "      393    0.000    0.000    0.001    0.000 serialization.py:379(default_restore_location)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:399(_is_path)\n",
      "        2    0.000    0.000    0.000    0.000 serialization.py:404(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 serialization.py:407(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:410(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:415(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:418(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:433(_open_file_like)\n",
      "        1    0.001    0.001    0.001    0.001 serialization.py:446(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 serialization.py:538(_check_dill_version)\n",
      "        3    0.000    0.000    0.000    0.000 serialization.py:80(get_default_load_endianness)\n",
      "        1    0.001    0.001    0.125    0.125 serialization.py:856(load)\n",
      "        2    0.000    0.000    0.000    0.000 sessions.py:107(get_redirect_target)\n",
      "        2    0.000    0.000    0.000    0.000 sessions.py:159(resolve_redirects)\n",
      "        2    0.000    0.000    0.002    0.001 sessions.py:459(prepare_request)\n",
      "        2    0.000    0.000    2.952    1.476 sessions.py:502(request)\n",
      "       14    0.000    0.000    0.000    0.000 sessions.py:61(merge_setting)\n",
      "        2    0.000    0.000    2.949    1.475 sessions.py:673(send)\n",
      "        2    0.000    0.000    0.001    0.000 sessions.py:751(merge_environment_settings)\n",
      "        2    0.000    0.000    0.000    0.000 sessions.py:782(get_adapter)\n",
      "        6    0.000    0.000    0.000    0.000 sessions.py:84(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 sessions.py:91(merge_hooks)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:19(_atleast_1d_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:215(_vhstack_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 shape_base.py:23(atleast_1d)\n",
      "        3    0.001    0.000    0.001    0.000 shape_base.py:292(hstack)\n",
      "        5    0.000    0.000    0.000    0.000 shape_base.py:727(_array_split_dispatcher)\n",
      "        5    0.000    0.000    0.000    0.000 shape_base.py:731(array_split)\n",
      "        5    0.000    0.000    0.000    0.000 shape_base.py:787(_split_dispatcher)\n",
      "        5    0.000    0.000    0.000    0.000 shape_base.py:791(split)\n",
      "        4    0.000    0.000    0.000    0.000 six.py:194(find_spec)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:302(makefile)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:488(_decref_socketios)\n",
      "        1    0.000    0.000    0.000    0.000 socket.py:621(send)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:679(__init__)\n",
      "        2    0.000    0.000    2.941    1.471 socket.py:691(readinto)\n",
      "        4    0.000    0.000    0.000    0.000 socket.py:730(readable)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:768(close)\n",
      "        3    0.000    0.000    0.000    0.000 softmax.py:112(validate_temperature)\n",
      "        2    0.000    0.000    0.000    0.000 softmax.py:35(Softmax_v2)\n",
      "        1    0.000    0.000    0.000    0.000 softmax.py:63(forward)\n",
      "        2    0.000    0.000    0.001    0.000 sparse.py:123(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 sparse.py:152(reset_parameters)\n",
      "        2    0.000    0.000    0.000    0.000 sparse.py:156(_fill_padding_idx_with_zero)\n",
      "        1    0.000    0.000    0.000    0.000 sparse.py:161(forward)\n",
      "        4    0.000    0.000    0.000    0.000 specifiers.py:230(__init__)\n",
      "        7    0.000    0.000    0.000    0.000 specifiers.py:284(operator)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:293(version)\n",
      "        4    0.000    0.000    0.000    0.000 specifiers.py:330(_canonical_spec)\n",
      "        4    0.000    0.000    0.000    0.000 specifiers.py:338(__hash__)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:34(_coerce_version)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:370(_get_operator)\n",
      "        1    0.000    0.000    0.000    0.000 specifiers.py:448(_compare_greater_than_equal)\n",
      "        1    0.000    0.000    0.000    0.000 specifiers.py:455(_compare_less_than)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:535(contains)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:688(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:708(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 specifiers.py:723(prereleases)\n",
      "        2    0.000    0.000    0.000    0.000 specifiers.py:740(prereleases)\n",
      "        1    0.000    0.000    0.000    0.000 specifiers.py:843(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 specifiers.py:853(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 specifiers.py:874(contains)\n",
      "        3    0.000    0.000    0.000    0.000 specifiers.py:929(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 ssl.py:1109(_checkClosed)\n",
      "        2    0.000    0.000    2.941    1.471 ssl.py:1121(read)\n",
      "        2    0.000    0.000    0.000    0.000 ssl.py:1199(send)\n",
      "        2    0.000    0.000    0.000    0.000 ssl.py:1226(sendall)\n",
      "        2    0.000    0.000    2.941    1.471 ssl.py:1263(recv_into)\n",
      "      393    0.003    0.000    0.003    0.000 storage.py:311(__getitem__)\n",
      "      393    0.000    0.000    0.000    0.000 storage.py:408(_get_always_warn_typed_storage_removal)\n",
      "      393    0.001    0.000    0.001    0.000 storage.py:416(_warn_typed_storage_removal)\n",
      "      393    0.000    0.000    0.000    0.000 storage.py:419(is_first_time)\n",
      "      393    0.001    0.000    0.001    0.000 storage.py:458(__new__)\n",
      "      393    0.001    0.000    0.001    0.000 storage.py:530(__init__)\n",
      "      393    0.000    0.000    0.000    0.000 storage.py:912(_data_ptr)\n",
      "       16    0.000    0.000    0.000    0.000 stringsource:1001(memoryview_fromslice)\n",
      "        3    0.000    0.000    0.000    0.000 stringsource:108(__pyx_convert_set_from_py_int)\n",
      "       56    0.000    0.000    0.000    0.000 stringsource:346(__cinit__)\n",
      "       56    0.000    0.000    0.000    0.000 stringsource:374(__dealloc__)\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:561(__get__)\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:596(__get__)\n",
      "       48    0.000    0.000    0.000    0.000 stringsource:659(memoryview_cwrapper)\n",
      "       48    0.000    0.000    0.000    0.000 stringsource:665(memoryview_check)\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:978(__dealloc__)\n",
      "       10    0.000    0.000    0.000    0.000 structures.py:40(__init__)\n",
      "       70    0.000    0.000    0.000    0.000 structures.py:46(__setitem__)\n",
      "       58    0.000    0.000    0.000    0.000 structures.py:51(__getitem__)\n",
      "       10    0.000    0.000    0.000    0.000 structures.py:57(__iter__)\n",
      "       54    0.000    0.000    0.000    0.000 structures.py:58(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 structures.py:60(__len__)\n",
      "        2    0.000    0.000    0.000    0.000 structures.py:76(copy)\n",
      "        2    0.000    0.000    0.000    0.000 tagger.py:10(build_tagger_model)\n",
      "        3    0.000    0.000    0.000    0.000 tagger.pyx:108(labels)\n",
      "        1    0.000    0.000    0.001    0.001 tagger.pyx:124(predict)\n",
      "        1    0.000    0.000    0.000    0.000 tagger.pyx:132(genexpr)\n",
      "        1    0.000    0.000    0.000    0.000 tagger.pyx:144(_scores2guesses)\n",
      "        1    0.001    0.001    0.001    0.001 tagger.pyx:153(set_annotations)\n",
      "        1    0.000    0.000    0.000    0.000 tagger.pyx:44(make_tagger)\n",
      "        1    0.000    0.000    0.000    0.000 tagger.pyx:68(make_tagger_scorer)\n",
      "        1    0.000    0.000    0.000    0.000 tagger.pyx:77(__init__)\n",
      "        2    0.000    0.000    0.030    0.015 tb_framework.py:33(forward)\n",
      "        2    0.000    0.000    0.000    0.000 tb_framework.py:7(TransitionModel)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1102(_wait_for_tstate_lock)\n",
      "     1150    0.001    0.000    0.001    0.000 threading.py:1145(ident)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1169(is_alive)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:1430(current_thread)\n",
      "      230    0.002    0.000    0.003    0.000 threading.py:1478(enumerate)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:264(__enter__)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:267(__exit__)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:279(_is_owned)\n",
      "        4    0.000    0.000    0.000    0.000 threading.py:359(notify)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:553(is_set)\n",
      "        6    0.000    0.000    0.000    0.000 threading.py:90(RLock)\n",
      "        6    0.000    0.000    0.000    0.000 timeout.py:113(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 timeout.py:130(resolve_default_timeout)\n",
      "       18    0.000    0.000    0.000    0.000 timeout.py:134(_validate_timeout)\n",
      "        4    0.000    0.000    0.000    0.000 timeout.py:192(clone)\n",
      "        2    0.000    0.000    0.000    0.000 timeout.py:206(start_connect)\n",
      "        4    0.000    0.000    0.000    0.000 timeout.py:231(connect_timeout)\n",
      "        2    0.000    0.000    0.000    0.000 timeout.py:249(read_timeout)\n",
      "        3    0.000    0.000    0.000    0.000 tok2vec.py:105(build_Tok2Vec_model)\n",
      "        1    0.000    0.000    0.036    0.036 tok2vec.py:113(predict)\n",
      "        2    0.000    0.000    0.000    0.000 tok2vec.py:122(<genexpr>)\n",
      "        3    0.000    0.000    0.004    0.001 tok2vec.py:126(MultiHashEmbed)\n",
      "        1    0.000    0.000    0.000    0.000 tok2vec.py:129(set_annotations)\n",
      "       15    0.000    0.000    0.003    0.000 tok2vec.py:169(make_hash_embed)\n",
      "        3    0.000    0.000    0.003    0.001 tok2vec.py:174(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 tok2vec.py:238(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 tok2vec.py:27(make_tok2vec)\n",
      "        3    0.000    0.000    0.004    0.001 tok2vec.py:281(MaxoutWindowEncoder)\n",
      "        2    0.000    0.000    0.000    0.000 tok2vec.py:284(forward)\n",
      "        2    0.000    0.000    0.000    0.000 tok2vec.py:32(tok2vec_listener_v1)\n",
      "        1    0.000    0.000    0.000    0.000 tok2vec.py:50(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 tok2vec.py:67(listeners)\n",
      "        8    0.000    0.000    0.000    0.000 tok2vec.py:72(<listcomp>)\n",
      "       16    0.000    0.000    0.000    0.000 tok2vec.py:74(listening_components)\n",
      "       13    0.000    0.000    0.000    0.000 tok2vec.py:81(add_listener)\n",
      "       26    0.000    0.000    0.002    0.000 tok2vec.py:98(find_listeners)\n",
      "     9368    0.002    0.000    0.002    0.000 token.pxd:23(cinit)\n",
      "        2    0.000    0.000    0.000    0.000 tokenization_auto.py:478(tokenizer_class_from_name)\n",
      "        1    0.000    0.000    1.461    1.461 tokenization_auto.py:506(get_tokenizer_config)\n",
      "        1    0.000    0.000    2.080    2.080 tokenization_auto.py:630(from_pretrained)\n",
      "        1    0.000    0.000    0.302    0.302 tokenization_deberta_v2_fast.py:118(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1119(bos_token)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1125(eos_token)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1131(unk_token)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1137(sep_token)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1143(pad_token)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1149(cls_token)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1155(mask_token)\n",
      "        2    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1297(special_tokens_map_extended)\n",
      "        2    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1313(all_special_tokens_extended)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1334(all_special_tokens)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1341(<listcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 tokenization_utils_base.py:1564(__init__)\n",
      "        1    0.010    0.010    0.618    0.618 tokenization_utils_base.py:1803(from_pretrained)\n",
      "        2    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2012(<genexpr>)\n",
      "        1    0.000    0.000    0.001    0.001 tokenization_utils_base.py:203(__init__)\n",
      "        1    0.000    0.000    0.602    0.602 tokenization_utils_base.py:2042(_from_pretrained)\n",
      "     23/1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2283(convert_added_tokens)\n",
      "      3/1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2299(<dictcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 tokenization_utils_base.py:242(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2591(_get_padding_truncation_strategies)\n",
      "        1    0.000    0.000    0.012    0.012 tokenization_utils_base.py:2729(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:280(keys)\n",
      "        1    0.000    0.000    0.012    0.012 tokenization_utils_base.py:2818(_call_one)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2840(_is_valid_text_input)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:286(items)\n",
      "        1    0.000    0.000    0.012    0.012 tokenization_utils_base.py:2930(encode_plus)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:301(tokens)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:367(word_ids)\n",
      "        2    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3828(_eventual_warn_about_too_long_sequence)\n",
      "        2    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3848(_switch_to_input_mode)\n",
      "        1    0.000    0.000    0.001    0.001 tokenization_utils_base.py:680(convert_to_tensors)\n",
      "        3    0.000    0.000    0.001    0.000 tokenization_utils_base.py:717(as_tensor)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:834(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:162(<listcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:164(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:167(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:169(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:206(vocab_size)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:220(added_tokens_encoder)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:226(<dictcomp>)\n",
      "        5    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:226(<lambda>)\n",
      "        6    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:228(added_tokens_decoder)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:267(_convert_encoding)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:342(_add_tokens)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:398(set_truncation_and_padding)\n",
      "        1    0.000    0.000    0.011    0.011 tokenization_utils_fast.py:469(_batch_encode_plus)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:516(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:538(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:540(<listcomp>)\n",
      "        1    0.000    0.000    0.012    0.012 tokenization_utils_fast.py:554(_encode_plus)\n",
      "        1    0.000    0.000    0.302    0.302 tokenization_utils_fast.py:94(__init__)\n",
      "        3    0.000    0.000    0.001    0.000 tokenizer.pyx:101(__set__)\n",
      "     6990    0.001    0.000    0.001    0.000 tokenizer.pyx:106(__get__)\n",
      "        3    0.000    0.000    0.001    0.000 tokenizer.pyx:109(__set__)\n",
      "        2    0.000    0.000    0.236    0.118 tokenizer.pyx:117(__set__)\n",
      "     2694    0.001    0.000    0.001    0.000 tokenizer.pyx:126(__get__)\n",
      "        2    0.000    0.000    0.001    0.000 tokenizer.pyx:129(__set__)\n",
      "        1    0.000    0.000    0.033    0.033 tokenizer.pyx:143(__call__)\n",
      "      733    0.006    0.000    0.111    0.000 tokenizer.pyx:156(_tokenize_affixes)\n",
      "     2712    0.010    0.000    0.011    0.000 tokenizer.pyx:216(_flush_cache)\n",
      "     2712    0.001    0.000    0.001    0.000 tokenizer.pyx:219(_reset_cache)\n",
      "       18    0.001    0.000    0.007    0.000 tokenizer.pyx:226(_flush_specials)\n",
      "        1    0.000    0.000    0.001    0.001 tokenizer.pyx:234(_apply_special_cases)\n",
      "        1    0.000    0.000    0.001    0.001 tokenizer.pyx:294(_prepare_special_spans)\n",
      "        1    0.000    0.000    0.265    0.265 tokenizer.pyx:31(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:314(_retokenize_special_spans)\n",
      "     2484    0.001    0.000    0.001    0.000 tokenizer.pyx:355(_try_specials_and_cache)\n",
      "     1168    0.002    0.000    0.086    0.000 tokenizer.pyx:384(_tokenize)\n",
      "     1168    0.007    0.000    0.028    0.000 tokenizer.pyx:396(_split_affixes)\n",
      "     1168    0.006    0.000    0.055    0.000 tokenizer.pyx:444(_attach_tokens)\n",
      "     1168    0.001    0.000    0.001    0.000 tokenizer.pyx:504(_save_cached)\n",
      "     3495    0.026    0.000    0.026    0.000 tokenizer.pyx:525(find_infix)\n",
      "     4662    0.013    0.000    0.014    0.000 tokenizer.pyx:539(find_prefix)\n",
      "     4366    0.031    0.000    0.032    0.000 tokenizer.pyx:553(find_suffix)\n",
      "       19    0.004    0.000    0.497    0.026 tokenizer.pyx:567(_load_special_cases)\n",
      "     2694    0.107    0.000    0.107    0.000 tokenizer.pyx:573(_validate_special_case)\n",
      "     2694    0.021    0.000    0.493    0.000 tokenizer.pyx:591(add_special_case)\n",
      "       16    0.000    0.000    0.006    0.000 tokenizer.pyx:617(_reload_special_cases)\n",
      "     3055    0.000    0.000    0.000    0.000 tokenizer.pyx:74(__get__)\n",
      "        1    0.000    0.000    0.245    0.245 tokenizer.pyx:758(from_disk)\n",
      "        2    0.000    0.000    0.001    0.000 tokenizer.pyx:77(__set__)\n",
      "        1    0.000    0.000    0.245    0.245 tokenizer.pyx:794(from_bytes)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:806(lambda12)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:807(lambda13)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:808(lambda14)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:809(lambda15)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:810(lambda16)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:811(lambda17)\n",
      "        1    0.000    0.000    0.000    0.000 tokenizer.pyx:812(lambda18)\n",
      "     2174    0.000    0.000    0.000    0.000 tokenizer.pyx:82(__get__)\n",
      "        3    0.000    0.000    0.001    0.000 tokenizer.pyx:85(__set__)\n",
      "     9324    0.001    0.000    0.001    0.000 tokenizer.pyx:90(__get__)\n",
      "        3    0.000    0.000    0.001    0.000 tokenizer.pyx:93(__set__)\n",
      "     8732    0.001    0.000    0.001    0.000 tokenizer.pyx:98(__get__)\n",
      "      392    0.000    0.000    0.001    0.000 torch.py:11(storage_ptr)\n",
      "      392    0.000    0.000    0.001    0.000 torch.py:31(storage_size)\n",
      "      756    0.000    0.000    0.000    0.000 trainable_pipe.pxd:6(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 trainable_pipe.pxd:6(__set__)\n",
      "       70    0.000    0.000    0.000    0.000 trainable_pipe.pxd:7(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 trainable_pipe.pxd:7(__set__)\n",
      "        7    0.000    0.000    0.000    0.000 trainable_pipe.pxd:8(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 trainable_pipe.pxd:8(__set__)\n",
      "        2    0.000    0.000    0.000    0.000 trainable_pipe.pxd:9(__set__)\n",
      "        3    0.000    0.000    0.000    0.000 trainable_pipe.pyx:254(_validate_serialization_attrs)\n",
      "        3    0.000    0.000    0.139    0.046 trainable_pipe.pyx:320(from_disk)\n",
      "        3    0.005    0.002    0.137    0.046 trainable_pipe.pyx:331(load_model)\n",
      "        3    0.000    0.000    0.002    0.001 trainable_pipe.pyx:340(lambda8)\n",
      "        4    0.028    0.007    0.096    0.024 trainable_pipe.pyx:40(__call__)\n",
      "       58    0.000    0.000    0.000    0.000 types.py:100(prepare_class)\n",
      "        6    0.000    0.000    0.000    0.000 types.py:1179(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 types.py:1188(dataXd)\n",
      "       58    0.000    0.000    0.000    0.000 types.py:132(_calculate_meta)\n",
      "        3    0.000    0.000    0.000    0.000 types.py:176(__get__)\n",
      "        9    0.000    0.000    0.000    0.000 types.py:433(validate)\n",
      "       52    0.000    0.000    0.000    0.000 types.py:617(__get_validators__)\n",
      "       58    0.000    0.000    0.000    0.000 types.py:79(resolve_bases)\n",
      "       41    0.000    0.000    0.000    0.000 typing.py:1031(__eq__)\n",
      "        5    0.000    0.000    0.000    0.000 typing.py:1037(__hash__)\n",
      "     3945    0.005    0.000    0.014    0.000 typing.py:111(get_origin)\n",
      "      706    0.001    0.000    0.002    0.000 typing.py:1154(__subclasscheck__)\n",
      "       28    0.000    0.000    0.000    0.000 typing.py:1242(__eq__)\n",
      "      659    0.001    0.000    0.001    0.000 typing.py:1247(__hash__)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:1272(_value_and_type_iter)\n",
      "       20    0.000    0.000    0.000    0.000 typing.py:1273(<genexpr>)\n",
      "      373    0.000    0.000    0.000    0.000 typing.py:1278(__eq__)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:137(_type_convert)\n",
      "       68    0.001    0.000    0.002    0.000 typing.py:1408(_get_protocol_attrs)\n",
      "       34    0.000    0.000    0.001    0.000 typing.py:1425(_is_callable_members_only)\n",
      "       68    0.000    0.000    0.000    0.000 typing.py:1427(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:146(_type_check)\n",
      "       34    0.000    0.000    0.003    0.000 typing.py:1490(__instancecheck__)\n",
      "      138    0.000    0.000    0.000    0.000 typing.py:1506(<genexpr>)\n",
      "      294    0.000    0.000    0.001    0.000 typing.py:164(get_args)\n",
      "     1139    0.000    0.000    0.000    0.000 typing.py:1737(cast)\n",
      "     3945    0.004    0.000    0.006    0.000 typing.py:1902(get_origin)\n",
      "      294    0.000    0.000    0.001    0.000 typing.py:1929(get_args)\n",
      "  591/371    0.001    0.000    0.004    0.000 typing.py:197(convert_generics)\n",
      "  376/278    0.000    0.000    0.002    0.000 typing.py:218(<genexpr>)\n",
      "      364    0.000    0.000    0.000    0.000 typing.py:249(is_union)\n",
      "     1417    0.001    0.000    0.002    0.000 typing.py:306(inner)\n",
      "  422/246    0.001    0.000    0.002    0.000 typing.py:320(_eval_type)\n",
      "  278/180    0.000    0.000    0.001    0.000 typing.py:329(<genexpr>)\n",
      "      417    0.000    0.000    0.001    0.000 typing.py:354(is_none_type)\n",
      "       12    0.000    0.000    0.000    0.000 typing.py:358(display_as_type)\n",
      "      141    0.000    0.000    0.000    0.000 typing.py:371(__getattr__)\n",
      "       58    0.001    0.000    0.003    0.000 typing.py:376(resolve_annotations)\n",
      "      262    0.000    0.000    0.001    0.000 typing.py:408(is_callable_type)\n",
      "      636    0.000    0.000    0.002    0.000 typing.py:412(is_literal_type)\n",
      "        4    0.000    0.000    0.000    0.000 typing.py:416(literal_values)\n",
      "     12/4    0.000    0.000    0.000    0.000 typing.py:420(all_literal_values)\n",
      "       12    0.000    0.000    0.000    0.000 typing.py:430(<genexpr>)\n",
      "      260    0.001    0.000    0.002    0.000 typing.py:433(is_namedtuple)\n",
      "      616    0.003    0.000    0.004    0.000 typing.py:443(is_typeddict)\n",
      "      740    0.000    0.000    0.000    0.000 typing.py:453(_check_typeddict_special)\n",
      "      370    0.000    0.000    0.002    0.000 typing.py:457(is_typeddict_special)\n",
      "      399    0.000    0.000    0.000    0.000 typing.py:467(is_new_type)\n",
      "      492    0.000    0.000    0.000    0.000 typing.py:480(_check_classvar)\n",
      "     1232    0.001    0.000    0.001    0.000 typing.py:487(_check_finalvar)\n",
      "      246    0.000    0.000    0.002    0.000 typing.py:497(is_classvar)\n",
      "      616    0.001    0.000    0.003    0.000 typing.py:509(is_finalvar)\n",
      "  350/246    0.000    0.000    0.000    0.000 typing.py:513(update_field_forward_refs)\n",
      "       58    0.000    0.000    0.001    0.000 typing.py:535(update_model_forward_refs)\n",
      "      246    0.000    0.000    0.001    0.000 typing.py:574(get_class)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:664(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 typing.py:679(_evaluate)\n",
      "      108    0.000    0.000    0.000    0.000 typing.py:935(_is_dunder)\n",
      "      108    0.000    0.000    0.001    0.000 typing.py:976(__getattr__)\n",
      "      402    0.000    0.000    0.002    0.000 typing.py:993(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "        1    0.000    0.000    0.000    0.000 ufunclike.py:14(_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 ufunclike.py:71(isposinf)\n",
      "        6    0.000    0.000    0.000    0.000 url.py:100(__new__)\n",
      "        8    0.001    0.000    0.001    0.000 url.py:227(_encode_invalid_chars)\n",
      "        6    0.000    0.000    0.000    0.000 url.py:263(_remove_path_dot_segments)\n",
      "        6    0.000    0.000    0.000    0.000 url.py:303(_normalize_host)\n",
      "        4    0.000    0.000    0.000    0.000 url.py:326(<listcomp>)\n",
      "        8    0.000    0.000    0.000    0.000 url.py:332(_idna_encode)\n",
      "        2    0.000    0.000    0.000    0.000 url.py:351(_encode_target)\n",
      "        6    0.000    0.000    0.001    0.000 url.py:369(parse_url)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1079(is_in_jupyter)\n",
      "     3305    0.002    0.000    0.004    0.000 util.py:112(is_xp_array)\n",
      "        2    0.000    0.000    0.000    0.000 util.py:1130(get_cuda_stream)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1161(compile_prefix_regex)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1168(<listcomp>)\n",
      "     3220    0.000    0.000    0.000    0.000 util.py:117(is_cupy_array)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1172(compile_suffix_regex)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1179(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1183(compile_infix_regex)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1190(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 util.py:1194(add_lookups)\n",
      "2795/1790    0.004    0.000    0.007    0.000 util.py:1207(_get_attr_unless_lookup)\n",
      "       34    0.000    0.000    0.000    0.000 util.py:1264(normalize_slice)\n",
      "     3312    0.001    0.000    0.002    0.000 util.py:127(is_numpy_array)\n",
      "        1    0.000    0.000    0.005    0.005 util.py:1317(from_bytes)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1336(from_dict)\n",
      "      9/1    0.027    0.003    0.649    0.649 util.py:1363(from_disk)\n",
      "       88    0.000    0.000    0.050    0.001 util.py:144(get)\n",
      "        8    0.000    0.000    0.004    0.000 util.py:1455(copy_config)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1485(dict_to_dot)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1494(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1538(walk_dict)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1568(combine_score_weights)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1583(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1587(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:1711(warn_if_jupyter_cupy)\n",
      "        5    0.000    0.000    0.000    0.000 util.py:182(to_numpy)\n",
      "       24    0.000    0.000    0.000    0.000 util.py:19(to_str)\n",
      "       11    0.000    0.000    0.002    0.000 util.py:20(force_path)\n",
      "       28    0.000    0.000    0.016    0.001 util.py:205(has)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:223(__init__)\n",
      "       39    0.000    0.000    0.000    0.000 util.py:250(__init__)\n",
      "   3305/5    0.005    0.000    0.013    0.003 util.py:342(convert_recursive)\n",
      "        1    0.000    0.000    0.001    0.001 util.py:349(get_lang_class)\n",
      "   210/20    0.004    0.000    0.013    0.001 util.py:363(<listcomp>)\n",
      "       20    0.000    0.000    0.000    0.000 util.py:389(ensure_path)\n",
      "        1    0.000    0.000    1.215    1.215 util.py:433(load_model)\n",
      "        1    0.000    0.000    1.214    1.214 util.py:475(load_model_from_package)\n",
      "        1    0.000    0.000    1.211    1.211 util.py:504(load_model_from_path)\n",
      "       66    0.000    0.000    0.000    0.000 util.py:522(partial)\n",
      "        1    0.000    0.000    0.547    0.547 util.py:550(load_model_from_config)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:631(from_array)\n",
      "        1    0.000    0.000    1.213    1.213 util.py:651(load_model_from_init_py)\n",
      "        1    0.000    0.000    0.013    0.013 util.py:693(load_config)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:755(is_compatible_version)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:781(is_unconstrained_version)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:792(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 util.py:797(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 util.py:799(<genexpr>)\n",
      "        7    0.000    0.000    0.000    0.000 util.py:80(get_array_module)\n",
      "        3    0.000    0.000    0.000    0.000 util.py:800(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:814(get_minor_version_range)\n",
      "        1    0.000    0.000    0.002    0.002 util.py:879(load_meta)\n",
      "        1    0.000    0.000    0.002    0.002 util.py:922(get_model_meta)\n",
      "        1    0.000    0.000    0.002    0.002 util.py:932(is_package)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:1016(get_auth_from_url)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:1027(safe_eval)\n",
      "        8    0.000    0.000    0.000    0.000 utils.py:1032(check_header_validity)\n",
      "       16    0.000    0.000    0.000    0.000 utils.py:1043(_validate_header_part)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:112(proxy_bypass)\n",
      "     1054    0.000    0.000    0.001    0.000 utils.py:157(sequence_like)\n",
      "      246    0.000    0.000    0.001    0.000 utils.py:161(validate_field_name)\n",
      "     1674    0.001    0.000    0.002    0.000 utils.py:180(lenient_issubclass)\n",
      "       24    0.000    0.000    0.001    0.000 utils.py:19(unwrap_dead_wrappers)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:199(get_netrc_auth)\n",
      "      246    0.000    0.000    0.000    0.000 utils.py:201(is_valid_identifier)\n",
      "        6    0.000    0.000    0.000    0.000 utils.py:206(<genexpr>)\n",
      "       96    0.000    0.000    0.000    0.000 utils.py:21(<genexpr>)\n",
      "       58    0.002    0.000    0.008    0.000 utils.py:235(generate_model_signature)\n",
      "        2    0.000    0.000    0.001    0.000 utils.py:263(extract_zipped_paths)\n",
      "       58    0.000    0.000    0.000    0.000 utils.py:285(<listcomp>)\n",
      "      116    0.000    0.000    0.000    0.000 utils.py:327(unique_list)\n",
      "       14    0.000    0.000    0.000    0.000 utils.py:340(to_key_val_list)\n",
      "       73    0.000    0.000    0.001    0.000 utils.py:489(__init__)\n",
      "       52    0.000    0.000    0.000    0.000 utils.py:51(_has_surrogates)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:513(_parse_content_type_header)\n",
      "      269    0.000    0.000    0.000    0.000 utils.py:513(for_element)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:538(get_encoding_from_headers)\n",
      "        4    0.000    0.000    0.000    0.000 utils.py:55(canonicalize_version)\n",
      "       73    0.000    0.000    0.002    0.000 utils.py:561(merge)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:580(iter_slices)\n",
      "      146    0.000    0.000    0.002    0.000 utils.py:600(_coerce_items)\n",
      "      146    0.000    0.000    0.001    0.000 utils.py:614(_coerce_value)\n",
      "      561    0.000    0.000    0.000    0.000 utils.py:620(is_true)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:635(unquote_unreserved)\n",
      "       58    0.000    0.000    0.000    0.000 utils.py:638(__init__)\n",
      "       28    0.000    0.000    0.000    0.000 utils.py:642(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:659(requote_uri)\n",
      "     1074    0.000    0.000    0.000    0.000 utils.py:675(smart_deepcopy)\n",
      "      912    0.001    0.000    0.001    0.000 utils.py:696(is_valid_field)\n",
      "        4    0.000    0.000    0.000    0.000 utils.py:742(set_environ)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:76(proxy_bypass_registry)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:764(should_bypass_proxies)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:772(get_proxy)\n",
      "       16    0.000    0.000    0.000    0.000 utils.py:78(<genexpr>)\n",
      "        2    0.000    0.000    0.001    0.000 utils.py:824(get_environ_proxies)\n",
      "        4    0.000    0.000    0.000    0.000 utils.py:836(select_proxy)\n",
      "        2    0.000    0.000    0.000    0.000 uuid.py:138(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 uuid.py:279(__str__)\n",
      "        2    0.000    0.000    0.000    0.000 uuid.py:713(uuid4)\n",
      "        7    0.000    0.000    0.002    0.000 validation.py:1238(column_or_1d)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:1433(_is_fitted)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:1465(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:1471(check_is_fitted)\n",
      "        8    0.000    0.000    0.000    0.000 validation.py:2071(_is_pandas_df)\n",
      "        4    0.000    0.000    0.000    0.000 validation.py:271(_is_arraylike)\n",
      "        4    0.000    0.000    0.000    0.000 validation.py:276(_is_arraylike_not_scalar)\n",
      "        8    0.000    0.000    0.000    0.000 validation.py:281(_use_interchange_protocol)\n",
      "        8    0.000    0.000    0.000    0.000 validation.py:344(_num_samples)\n",
      "        3    0.000    0.000    0.000    0.000 validation.py:416(check_consistent_length)\n",
      "        3    0.000    0.000    0.000    0.000 validation.py:427(<listcomp>)\n",
      "       23    0.000    0.000    0.000    0.000 validation.py:617(_ensure_no_complex_data)\n",
      "       23    0.000    0.000    0.000    0.000 validation.py:627(_check_estimator_name)\n",
      "       23    0.000    0.000    0.000    0.000 validation.py:675(_is_extension_array_dtype)\n",
      "       23    0.000    0.000    0.005    0.000 validation.py:680(check_array)\n",
      "       24    0.000    0.000    0.000    0.000 validators.py:106(bool_validator)\n",
      "      109    0.000    0.000    0.000    0.000 validators.py:127(int_validator)\n",
      "        2    0.000    0.000    0.000    0.000 validators.py:152(float_validator)\n",
      "        8    0.000    0.000    0.000    0.000 validators.py:454(callable_validator)\n",
      "        4    0.000    0.000    0.000    0.000 validators.py:480(make_literal_validator)\n",
      "        4    0.000    0.000    0.000    0.000 validators.py:486(<dictcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 validators.py:488(literal_validator)\n",
      "        9    0.000    0.000    0.000    0.000 validators.py:497(constr_length_validator)\n",
      "        9    0.000    0.000    0.000    0.000 validators.py:511(constr_strip_whitespace)\n",
      "        9    0.000    0.000    0.000    0.000 validators.py:519(constr_upper)\n",
      "        9    0.000    0.000    0.000    0.000 validators.py:527(constr_lower)\n",
      "       64    0.000    0.000    0.000    0.000 validators.py:549(make_arbitrary_type_validator)\n",
      "       29    0.000    0.000    0.000    0.000 validators.py:550(arbitrary_type_validator)\n",
      "      110    0.000    0.000    0.000    0.000 validators.py:59(str_validator)\n",
      "      354    0.000    0.000    0.001    0.000 validators.py:643(check)\n",
      "      796    0.000    0.000    0.001    0.000 validators.py:644(<genexpr>)\n",
      "      549    0.004    0.000    0.014    0.000 validators.py:698(find_validators)\n",
      "        9    0.000    0.000    0.000    0.000 validators.py:74(strict_str_validator)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:102(create_mode_vectors)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:103(vectors_factory)\n",
      "        4    0.000    0.000    0.000    0.000 vectors.pyx:126(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 vectors.pyx:126(__set__)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:127(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:127(__set__)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:128(__get__)\n",
      "     3580    0.000    0.000    0.000    0.000 vectors.pyx:130(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 vectors.pyx:140(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 vectors.pyx:214(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:247(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:307(__len__)\n",
      "        2    0.000    0.000    0.000    0.000 vectors.pyx:41(values)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:618(to_ops)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:638(_set_cfg)\n",
      "        1    0.000    0.000    0.003    0.003 vectors.pyx:678(from_disk)\n",
      "        1    0.000    0.000    0.002    0.002 vectors.pyx:687(load_key2row)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:694(load_keys)\n",
      "        1    0.000    0.000    0.001    0.001 vectors.pyx:700(load_vectors)\n",
      "        1    0.000    0.000    0.001    0.001 vectors.pyx:706(load_settings)\n",
      "        1    0.000    0.000    0.000    0.000 vectors.pyx:783(_sync_unset)\n",
      "       17    0.000    0.000    0.001    0.000 version.py:186(__init__)\n",
      "       68    0.000    0.000    0.000    0.000 version.py:205(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 version.py:232(__str__)\n",
      "        4    0.000    0.000    0.000    0.000 version.py:245(<genexpr>)\n",
      "        5    0.000    0.000    0.000    0.000 version.py:265(epoch)\n",
      "        6    0.000    0.000    0.000    0.000 version.py:276(release)\n",
      "        9    0.000    0.000    0.000    0.000 version.py:292(pre)\n",
      "        5    0.000    0.000    0.000    0.000 version.py:307(post)\n",
      "        9    0.000    0.000    0.000    0.000 version.py:318(dev)\n",
      "        5    0.000    0.000    0.000    0.000 version.py:329(local)\n",
      "        1    0.000    0.000    0.000    0.000 version.py:343(public)\n",
      "        4    0.000    0.000    0.000    0.000 version.py:381(is_prerelease)\n",
      "        8    0.000    0.000    0.000    0.000 version.py:45(parse)\n",
      "       51    0.000    0.000    0.000    0.000 version.py:452(_parse_letter_version)\n",
      "       17    0.000    0.000    0.000    0.000 version.py:491(_parse_local_version)\n",
      "       17    0.000    0.000    0.000    0.000 version.py:503(_cmpkey)\n",
      "       25    0.000    0.000    0.000    0.000 version.py:518(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 version.py:76(__lt__)\n",
      "        5    0.000    0.000    0.000    0.000 version.py:94(__ge__)\n",
      "     2177    0.000    0.000    0.000    0.000 vocab.pxd:28(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 vocab.pyx:103(__get__)\n",
      "     2144    0.004    0.000    0.046    0.000 vocab.pyx:148(get)\n",
      "     6048    0.003    0.000    0.083    0.000 vocab.pyx:167(get_by_orth)\n",
      "     1790    0.046    0.000    0.122    0.000 vocab.pyx:181(_new_lexeme)\n",
      "     1790    0.001    0.000    0.001    0.000 vocab.pyx:213(_add_lex_to_vocab)\n",
      "        1    0.000    0.000    0.000    0.000 vocab.pyx:23(create_vocab)\n",
      "      732    0.002    0.000    0.016    0.000 vocab.pyx:248(__getitem__)\n",
      "     2694    0.119    0.000    0.187    0.000 vocab.pyx:272(make_fused_token)\n",
      "        1    0.000    0.000    0.000    0.000 vocab.pyx:291(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 vocab.pyx:468(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 vocab.pyx:471(__set__)\n",
      "        1    0.103    0.103    0.129    0.129 vocab.pyx:498(from_disk)\n",
      "        1    0.000    0.000    0.000    0.000 vocab.pyx:50(__init__)\n",
      "     5379    0.001    0.000    0.001    0.000 vocab.pyx:92(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 vocab.pyx:95(__set__)\n",
      "        2    0.000    0.000    0.000    0.000 wait.py:113(wait_for_read)\n",
      "        2    0.000    0.000    0.000    0.000 wait.py:33(select_wait_for_socket)\n",
      "       39    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
      "       39    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
      "       39    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
      "       39    0.000    0.000    0.000    0.000 warnings.py:458(__enter__)\n",
      "       39    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
      "      118    0.000    0.000    0.000    0.000 weakref.py:415(__getitem__)\n",
      "       11    0.000    0.000    0.000    0.000 with_array.py:15(with_array)\n",
      "       11    0.000    0.000    0.000    0.000 with_array.py:27(<dictcomp>)\n",
      "        7    0.000    0.000    0.060    0.009 with_array.py:32(forward)\n",
      "        3    0.000    0.000    0.042    0.014 with_array.py:70(_list_forward)\n",
      "        3    0.000    0.000    0.000    0.000 with_array.py:75(<listcomp>)\n",
      "        4    0.000    0.000    0.018    0.004 with_array.py:87(_ragged_forward)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:1216(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:1817(__del__)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:1821(close)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:190(_check_zipfile)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:198(is_zipfile)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:214(_EndRecData64)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:2200(make)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:2319(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:257(_EndRecData)\n",
      "        1    0.000    0.000    0.000    0.000 zipfile.py:663(_check_compression)\n",
      "     1107    0.003    0.000    0.003    0.000 {built-in method __new__ of type object at 0x00007FF8EB414D30}\n",
      "       58    0.001    0.000    0.001    0.000 {built-in method _abc._abc_init}\n",
      "      981    0.001    0.000    0.002    0.000 {built-in method _abc._abc_instancecheck}\n",
      "      376    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _codecs.charmap_decode}\n",
      "       18    0.020    0.001    0.020    0.001 {built-in method _codecs.utf_8_decode}\n",
      "       52    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
      "       52    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
      "       97    0.000    0.000    0.000    0.000 {built-in method _json.encode_basestring_ascii}\n",
      "      392    0.002    0.000    0.002    0.000 {built-in method _make_subclass}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _operator.gt}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _operator.lt}\n",
      "       30    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISDIR}\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISREG}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _struct.calcsize}\n",
      "     4765    0.002    0.000    0.002    0.000 {built-in method _struct.unpack}\n",
      "       28    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "       34    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "      117    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "       24    0.002    0.000    1.016    0.042 {built-in method apply}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
      "    66106    0.021    0.000    0.051    0.000 {built-in method builtins.all}\n",
      "     1725    0.001    0.000    0.002    0.000 {built-in method builtins.any}\n",
      "     1222    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       62    0.002    0.000    0.002    0.000 {built-in method builtins.dir}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.eval}\n",
      "        1    0.000    0.000   14.735   14.735 {built-in method builtins.exec}\n",
      "59982/59953    0.016    0.000    0.017    0.000 {built-in method builtins.getattr}\n",
      "    11051    0.007    0.000    0.012    0.000 {built-in method builtins.hasattr}\n",
      "      668    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "     7945    0.001    0.000    0.001    0.000 {built-in method builtins.id}\n",
      "82550/79612    0.018    0.000    0.030    0.000 {built-in method builtins.isinstance}\n",
      "5017/4713    0.001    0.000    0.002    0.000 {built-in method builtins.issubclass}\n",
      "      193    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "99113/98961    0.011    0.000    0.012    0.000 {built-in method builtins.len}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
      "      118    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       77    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "       85    0.000    0.000    0.013    0.000 {built-in method builtins.next}\n",
      "     7395    0.001    0.000    0.001    0.000 {built-in method builtins.ord}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
      "     2505    0.001    0.000    0.001    0.000 {built-in method builtins.setattr}\n",
      "      140    0.001    0.000    0.007    0.000 {built-in method builtins.sorted}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.vars}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
      "        1    0.301    0.301    0.301    0.301 {built-in method from_file}\n",
      "      284    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "      155    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
      "      130    0.000    0.000    0.000    0.000 {built-in method gc.disable}\n",
      "      130    0.000    0.000    0.000    0.000 {built-in method gc.enable}\n",
      "       37    0.005    0.000    0.005    0.000 {built-in method io.open}\n",
      "       23    0.000    0.000    0.000    0.000 {built-in method io.text_encoding}\n",
      "      290    0.000    0.000    0.000    0.000 {built-in method math.sqrt}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method nt._getfullpathname}\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method nt.fspath}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method nt.getcwd}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "       12    0.002    0.000    0.002    0.000 {built-in method nt.listdir}\n",
      "        8    0.001    0.000    0.001    0.000 {built-in method nt.mkdir}\n",
      "      177    0.018    0.000    0.019    0.000 {built-in method nt.stat}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method nt.urandom}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "     2147    0.001    0.000    0.001    0.000 {built-in method numpy.asanyarray}\n",
      "       69    0.003    0.000    0.003    0.000 {built-in method numpy.asarray}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method numpy.ascontiguousarray}\n",
      "      710    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "      707    0.001    0.000    0.001    0.000 {built-in method numpy.empty}\n",
      "       85    0.000    0.000    0.000    0.000 {built-in method numpy.frombuffer}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.fromfile}\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "      176    0.001    0.000    0.001    0.000 {built-in method srsly.ujson.ujson.dumps}\n",
      "      714    0.002    0.000    0.002    0.000 {built-in method srsly.ujson.ujson.loads}\n",
      "        9    0.013    0.001    0.013    0.001 {built-in method srsly.ujson.ujson.load}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys.audit}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys.exc_info}\n",
      "      405    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method sys.intern}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method time.perf_counter}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method torch._C._are_functorch_transforms_active}\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method torch._C._functorch.unwrap_if_dead}\n",
      "       50    0.000    0.000    0.000    0.000 {built-in method torch._C._get_cudnn_enabled}\n",
      "      515    0.004    0.000    0.004    0.000 {built-in method torch._C._get_tracing_state}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "       51    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._is_tracing}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_metadata}\n",
      "      469    0.001    0.000    0.001    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "       24    0.057    0.002    0.057    0.002 {built-in method torch._C._nn.gelu}\n",
      "      193    4.480    0.023    4.480    0.023 {built-in method torch._C._nn.linear}\n",
      "      988    0.002    0.000    0.002    0.000 {built-in method torch._C._set_grad_enabled}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method torch.abs}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method torch.arange}\n",
      "       96    1.134    0.012    1.134    0.012 {built-in method torch.bmm}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.ceil}\n",
      "       48    0.031    0.001    0.031    0.001 {built-in method torch.clamp}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.dropout}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.embedding}\n",
      "      392    0.067    0.000    0.067    0.000 {built-in method torch.empty}\n",
      "       48    0.324    0.007    0.324    0.007 {built-in method torch.gather}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.get_default_dtype}\n",
      "     1482    0.001    0.000    0.001    0.000 {built-in method torch.is_grad_enabled}\n",
      "       50    0.042    0.001    0.042    0.001 {built-in method torch.layer_norm}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.log}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method torch.sign}\n",
      "       25    0.729    0.029    0.729    0.029 {built-in method torch.softmax}\n",
      "       72    0.001    0.000    0.001    0.000 {built-in method torch.sqrt}\n",
      "      494    0.014    0.000    0.014    0.000 {built-in method torch.tensor}\n",
      "        2    0.002    0.001    0.002    0.001 {built-in method torch.where}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.zeros_like}\n",
      "     4000    0.001    0.000    0.001    0.000 {built-in method unicodedata.category}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method winreg.OpenKey}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method winreg.QueryValueEx}\n",
      "       45    0.000    0.000    0.000    0.000 {function ClassInstantier.__getitem__ at 0x0000024FFDC14D30}\n",
      "        2    0.000    0.000    0.000    0.000 {function HTTPResponse.flush at 0x0000024FE5254AF0}\n",
      "        4    0.000    0.000    0.000    0.000 {function SocketIO.close at 0x0000024FE1A39630}\n",
      "       21    0.000    0.000    0.000    0.000 {function Table.__contains__ at 0x0000024FF0906830}\n",
      "     2488    0.001    0.000    0.001    0.000 {function _ParameterMeta.__instancecheck__ at 0x0000024FEBADE200}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'Close' of 'PyHKEY' objects}\n",
      "      186    0.000    0.000    0.000    0.000 {method '__contains__' of 'frozenset' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__enter__' of '_io._IOBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "       25    0.001    0.000    0.001    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "      238    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "      224    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method '__exit__' of 'memoryview' objects}\n",
      "       58    0.000    0.000    0.000    0.000 {method '__prepare__' of 'type' objects}\n",
      "       48    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method '__setstate__' of 'tokenizers.AddedToken' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "     6823    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\n",
      "      179    0.000    0.000    0.000    0.000 {method 'add' of 'spacy.strings.StringStore' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'add_special_tokens' of 'tokenizers.Tokenizer' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.generic' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "    20853    0.004    0.000    0.004    0.000 {method 'append' of 'list' objects}\n",
      "      690    0.002    0.000    0.002    0.000 {method 'argmax' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'cache_clear' of 'functools._lru_cache_wrapper' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'cast' of 'memoryview' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'clear' of 'list' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
      "      144    0.067    0.000    0.067    0.000 {method 'contiguous' of 'torch._C._TensorBase' objects}\n",
      "      538    0.001    0.000    0.001    0.000 {method 'copy' of 'dict' objects}\n",
      "       88    0.003    0.000    0.003    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "      392    0.485    0.001    0.485    0.001 {method 'copy_' of 'torch._C._TensorBase' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'count' of 'bytes' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'count' of 'list' objects}\n",
      "     1774    0.001    0.000    0.001    0.000 {method 'count' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "      785    0.000    0.000    0.000    0.000 {method 'data_ptr' of 'torch._C.StorageBase' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'decode' of 'bytearray' objects}\n",
      "     1190    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}\n",
      "     3740    0.015    0.000    0.015    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n",
      "      344    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "      108    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "        1    0.011    0.011    0.011    0.011 {method 'encode_batch' of 'tokenizers.Tokenizer' objects}\n",
      "       34    0.000    0.000    0.000    0.000 {method 'end' of 're.Match' objects}\n",
      "     4662    0.002    0.000    0.002    0.000 {method 'endswith' of 'str' objects}\n",
      "       49    0.001    0.000    0.001    0.000 {method 'expand' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'extend' of 'collections.deque' objects}\n",
      "      796    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'fileno' of '_io.BufferedReader' objects}\n",
      "       51    0.001    0.000    0.001    0.000 {method 'fill_' of 'torch._C._TensorBase' objects}\n",
      "      398    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
      "      706    0.001    0.000    0.001    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "      334    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        1    0.000    0.000    0.129    0.129 {method 'from_disk' of 'spacy.vocab.Vocab' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method 'fullmatch' of 're.Pattern' objects}\n",
      "      402    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "    24741    0.004    0.000    0.004    0.000 {method 'get' of 'dict' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'get_added_tokens_decoder' of 'tokenizers.Tokenizer' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get_vocab_size' of 'tokenizers.Tokenizer' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
      "      882    0.001    0.000    0.001    0.000 {method 'group' of 're.Match' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'groups' of 're.Match' objects}\n",
      "       45    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
      "       50    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "     8824    0.001    0.000    0.001    0.000 {method 'isalpha' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'isascii' of 'str' objects}\n",
      "     5414    0.001    0.000    0.001    0.000 {method 'isdigit' of 'str' objects}\n",
      "     1090    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
      "     1790    0.000    0.000    0.000    0.000 {method 'islower' of 'str' objects}\n",
      "     1790    0.000    0.000    0.000    0.000 {method 'isspace' of 'str' objects}\n",
      "      288    0.000    0.000    0.000    0.000 {method 'issubset' of 'set' objects}\n",
      "      468    0.001    0.000    0.001    0.000 {method 'issuperset' of 'set' objects}\n",
      "     1790    0.000    0.000    0.000    0.000 {method 'istitle' of 'str' objects}\n",
      "     6959    0.001    0.000    0.001    0.000 {method 'isupper' of 'str' objects}\n",
      "    18567    0.003    0.000    0.003    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
      "     6442    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'join' of 'bytes' objects}\n",
      "     3130    0.001    0.000    0.001    0.000 {method 'join' of 'str' objects}\n",
      "      473    0.000    0.000    0.000    0.000 {method 'keys' of 'collections.OrderedDict' objects}\n",
      "     2635    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\n",
      "      356    0.000    0.000    0.000    0.000 {method 'keys' of 'mappingproxy' objects}\n",
      "       24    0.000    0.000    0.000    0.000 {method 'long' of 'torch._C._TensorBase' objects}\n",
      "     9520    0.002    0.000    0.002    0.000 {method 'lower' of 'str' objects}\n",
      "       61    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
      "       24    0.188    0.008    0.188    0.008 {method 'masked_fill' of 'torch._C._TensorBase' objects}\n",
      "       24    0.077    0.003    0.077    0.003 {method 'masked_fill_' of 'torch._C._TensorBase' objects}\n",
      "     3572    0.004    0.000    0.004    0.000 {method 'match' of 're.Pattern' objects}\n",
      "       10    0.000    0.000    0.001    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
      "      392    0.000    0.000    0.000    0.000 {method 'nbytes' of 'torch._C.StorageBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'no_truncation' of 'tokenizers.Tokenizer' objects}\n",
      "      690    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
      "       85    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}\n",
      "      144    0.003    0.000    0.003    0.000 {method 'permute' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'collections.OrderedDict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'collections.deque' objects}\n",
      "     9678    0.001    0.000    0.001    0.000 {method 'pop' of 'dict' objects}\n",
      "     2259    0.001    0.000    0.001    0.000 {method 'pop' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'pop' of 'set' objects}\n",
      "       40    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "       13    0.001    0.000    0.001    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "    20653    0.004    0.000    0.004    0.000 {method 'read' of '_io.BytesIO' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'read' of '_io.StringIO' objects}\n",
      "       11    0.080    0.007    0.099    0.009 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        2    2.941    1.471    2.941    1.471 {method 'read' of '_ssl._SSLSocket' objects}\n",
      "       42    0.000    0.000    2.941    0.070 {method 'readline' of '_io.BufferedReader' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'readline' of '_io.BytesIO' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'readlines' of '_io._IOBase' objects}\n",
      "       49    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
      "       39    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "       48    0.011    0.000    0.011    0.000 {method 'repeat' of 'torch._C._TensorBase' objects}\n",
      "     7146    0.003    0.000    0.003    0.000 {method 'replace' of 'str' objects}\n",
      "       30    0.000    0.000    0.000    0.000 {method 'reset' of '_contextvars.ContextVar' objects}\n",
      "      139    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "       57    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "     1532    0.001    0.000    0.001    0.000 {method 'rpartition' of 'str' objects}\n",
      "      109    0.000    0.000    0.000    0.000 {method 'rsplit' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'rstrip' of 'bytes' objects}\n",
      "     1661    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
      "      926    0.001    0.000    0.001    0.000 {method 'search' of 're.Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'searchsorted' of 'numpy.ndarray' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'seek' of '_io.StringIO' objects}\n",
      "       34    0.000    0.000    0.000    0.000 {method 'set' of '_contextvars.ContextVar' objects}\n",
      "      393    0.003    0.000    0.003    0.000 {method 'set_' of 'torch._C._TensorBase' objects}\n",
      "      319    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
      "     1134    0.004    0.000    0.004    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "      706    0.002    0.000    0.002    0.000 {method 'sort' of 'numpy.ndarray' objects}\n",
      "     4823    0.002    0.000    0.002    0.000 {method 'split' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
      "       49    0.002    0.000    0.002    0.000 {method 'squeeze' of 'torch._C._TensorBase' objects}\n",
      "      672    0.000    0.000    0.000    0.000 {method 'start' of 're.Match' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'startswith' of 'bytes' objects}\n",
      "   523401    0.148    0.000    0.148    0.000 {method 'startswith' of 'str' objects}\n",
      "     4221    0.001    0.000    0.001    0.000 {method 'strip' of 'str' objects}\n",
      "      519    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'subn' of 're.Pattern' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "       15    0.000    0.000    0.000    0.000 {method 'swapaxes' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'tell' of '_io.BufferedReader' objects}\n",
      "      122    0.011    0.000    0.011    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'to_array' of 'spacy.tokens.doc.Doc' objects}\n",
      "      533    0.002    0.000    0.002    0.000 {method 'to_dict' of 'spacy.tokens.morphanalysis.MorphAnalysis' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
      "       96    0.003    0.000    0.003    0.000 {method 'transpose' of 'torch._C._TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'truncate' of '_io.StringIO' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'type_as' of 'torch._C._TensorBase' objects}\n",
      "       53    0.001    0.000    0.001    0.000 {method 'unsqueeze' of 'torch._C._TensorBase' objects}\n",
      "      784    0.000    0.000    0.000    0.000 {method 'untyped_storage' of 'torch._C._TensorBase' objects}\n",
      "        8    0.002    0.000    0.006    0.001 {method 'update' of 'collections.OrderedDict' objects}\n",
      "     1397    0.001    0.000    0.001    0.000 {method 'update' of 'dict' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
      "       49    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "     9693    0.002    0.000    0.002    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "41803/41801    0.005    0.000    0.006    0.000 {method 'values' of 'dict' objects}\n",
      "      118    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "       10    0.000    0.000    0.002    0.000 {method 'var' of 'numpy.ndarray' objects}\n",
      "      336    0.008    0.000    0.008    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
      "      276    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'write' of '_ssl._SSLSocket' objects}\n",
      "       50    0.000    0.000    0.000    0.000 {method 'zero_' of 'torch._C._TensorBase' objects}\n",
      "     5815    0.003    0.000    0.003    0.000 {spacy.strings.get_string_id}\n",
      "      179    0.000    0.000    0.003    0.000 {spacy.tokens._retokenize.normalize_token_attrs}\n",
      "     1064    0.000    0.000    0.001    0.000 {spacy.tokens._retokenize.set_token_attrs}\n",
      "        1    0.000    0.000    0.000    0.000 {spacy.vocab.create_vocab}\n",
      "      128    0.024    0.000    0.025    0.000 {srsly.msgpack._unpacker.unpackb}\n",
      "        2    0.001    0.000    0.001    0.000 {srsly.msgpack._unpacker.unpack}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = cProfile.run('run_inference(text, labels_true, model_adapter)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
