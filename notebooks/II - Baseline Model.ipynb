{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032d8c33-2628-483e-ae66-6e9057f36cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c20cb8-378d-4fef-8657-fb70ad1a139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deaa7c0e-ba47-4e5a-b1e9-c862b9afdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5113e191-4e32-4134-9104-73b57a1ddd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "011ac916-258f-4636-973e-56512dc3ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8813aa-b476-4a96-b7bc-2ee7e3f50d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDebertaForTokenClassification, DebertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54261f0-1ce4-42e1-85bd-f6d034573bd4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0734aa-a92e-47da-abb5-a6b1688b04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = './in/train.json'\n",
    "path_test = './in/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc55f461-f2f1-49ad-b2b8-0f4da99e8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = json.load(open(path_train))\n",
    "df_train = pd.json_normalize(train_json)\n",
    "\n",
    "test_json = json.load(open(path_test))\n",
    "df_test = pd.json_normalize(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b91586e-2fe8-4fe3-acad-d3ad10ff8097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f847632a-5270-4318-ae4c-2c606a462aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[True, True, True, False, False, True, False, ...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Assignment:  Visualization Reflection  Submitt...</td>\n",
       "      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
       "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
       "4        56  Assignment:  Visualization Reflection  Submitt...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4  [Assignment, :,   , Visualization,  , Reflecti...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "2  [True, False, False, True, True, False, False,...   \n",
       "3  [True, True, True, False, False, True, False, ...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735c3f1-03b1-4b65-8397-145fba33f0d8",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1865a58d-11ce-4c5c-a61a-f954983bcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: John Doe's, Type: PERSON\n",
      "Entity: john.doe@email.com, Type: ORG\n",
      "Entity: 555, Type: CARDINAL\n",
      "Entity: 123-4567, Type: CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text containing PII\n",
    "text = \"John Doe's email is john.doe@email.com, and his phone number is +1 (555) 123-4567.\"\n",
    "\n",
    "# Process the text with spaCy NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print identified entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e11aca-63d8-46ad-b5aa-9112d69a084d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E047] Can't assign a value to unregistered extension attribute 'label'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLABEL1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLABEL2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# Replace with your actual labels\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(doc, labels):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m \u001b[38;5;241m=\u001b[39m label\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 5: Visualize the document with labels\u001b[39;00m\n\u001b[0;32m     11\u001b[0m displacy\u001b[38;5;241m.\u001b[39mserve(doc, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\tokens\\underscore.py:76\u001b[0m, in \u001b[0;36mUnderscore.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Any):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE047\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     77\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E047] Can't assign a value to unregistered extension attribute 'label'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Step 4: Add custom labels\n",
    "labels = [\"LABEL1\", \"LABEL2\", ...]  # Replace with your actual labels\n",
    "\n",
    "for token, label in zip(doc, labels):\n",
    "    token._.label = label\n",
    "\n",
    "# Step 5: Visualize the document with labels\n",
    "displacy.serve(doc, style=\"ent\")\n",
    "\n",
    "# Optional: Visualize labels beneath the text\n",
    "def visualize_with_labels(doc):\n",
    "    for token in doc:\n",
    "        print(f\"{token.text}\\t{token._.label}\")\n",
    "\n",
    "visualize_with_labels(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d445ae1-e60a-410f-ad58-1d069c57f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'you', 'love', '🤗', 'Transformers', '?', 'We', 'sure', 'do', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Don't you love 🤗 Transformers? We sure do.\"\n",
    "\n",
    "# Process the sentence using SpaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2e1a04-572d-41e7-8890-e752e5e56143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpaCy', 'is', 'a', 'powerful', 'NLP', 'library', 'for', 'Python', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f64ba55-f049-4cbf-91d7-308df1093332",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d65adb1f-7e40-49c7-a92e-6148bd1b5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_tokens_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b92349f6-6731-45c2-bf43-e55e55fe5616",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " '▁2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁Challenge',\n",
       " '▁&',\n",
       " '▁selection',\n",
       " '▁The',\n",
       " '▁tool',\n",
       " '▁I',\n",
       " '▁use',\n",
       " '▁to',\n",
       " '▁help',\n",
       " '▁all',\n",
       " '▁stakeholders',\n",
       " '▁finding',\n",
       " '▁their',\n",
       " '▁way',\n",
       " '▁through',\n",
       " '▁the',\n",
       " '▁complexity',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁project',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '.',\n",
       " '▁What',\n",
       " '▁exactly',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '?',\n",
       " '▁According',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁definition',\n",
       " '▁of',\n",
       " '▁Buz',\n",
       " 'an',\n",
       " '▁T',\n",
       " '.',\n",
       " '▁and',\n",
       " '▁Buz',\n",
       " 'an',\n",
       " '▁B',\n",
       " '.',\n",
       " '▁(',\n",
       " '1999',\n",
       " ',',\n",
       " '▁Des',\n",
       " 's',\n",
       " 'ine',\n",
       " '-',\n",
       " 'moi',\n",
       " '▁l',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '.',\n",
       " '▁Paris',\n",
       " ':',\n",
       " '▁Les',\n",
       " '▁É',\n",
       " 'dition',\n",
       " 's',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '.',\n",
       " ')',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁(',\n",
       " 'or',\n",
       " '▁heuristic',\n",
       " '▁diagram',\n",
       " ')',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁graphic',\n",
       " '▁representation',\n",
       " '▁technique',\n",
       " '▁that',\n",
       " '▁follows',\n",
       " '▁the',\n",
       " '▁natural',\n",
       " '▁functioning',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁and',\n",
       " '▁allows',\n",
       " '▁the',\n",
       " '▁brain',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁potential',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁released',\n",
       " '.',\n",
       " '▁Cf',\n",
       " '▁Annex',\n",
       " '1',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁has',\n",
       " '▁many',\n",
       " '▁advantages',\n",
       " ':',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁accessible',\n",
       " '▁to',\n",
       " '▁all',\n",
       " '▁and',\n",
       " '▁does',\n",
       " '▁not',\n",
       " '▁require',\n",
       " '▁significant',\n",
       " '▁material',\n",
       " '▁investment',\n",
       " '▁and',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁done',\n",
       " '▁quickly',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁scalable',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁allows',\n",
       " '▁categorization',\n",
       " '▁and',\n",
       " '▁linking',\n",
       " '▁of',\n",
       " '▁information',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁applied',\n",
       " '▁to',\n",
       " '▁any',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁situation',\n",
       " ':',\n",
       " '▁note',\n",
       " 'taking',\n",
       " ',',\n",
       " '▁problem',\n",
       " '▁solving',\n",
       " ',',\n",
       " '▁analysis',\n",
       " ',',\n",
       " '▁creation',\n",
       " '▁of',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁suitable',\n",
       " '▁for',\n",
       " '▁all',\n",
       " '▁people',\n",
       " '▁and',\n",
       " '▁is',\n",
       " '▁easy',\n",
       " '▁to',\n",
       " '▁learn',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁fun',\n",
       " '▁and',\n",
       " '▁encourages',\n",
       " '▁exchanges',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁makes',\n",
       " '▁visible',\n",
       " '▁the',\n",
       " '▁dimension',\n",
       " '▁of',\n",
       " '▁projects',\n",
       " ',',\n",
       " '▁opportunities',\n",
       " ',',\n",
       " '▁interconnections',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁synthesize',\n",
       " 's',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁makes',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁understandable',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁allows',\n",
       " '▁you',\n",
       " '▁to',\n",
       " '▁explore',\n",
       " '▁ideas',\n",
       " '▁The',\n",
       " '▁creation',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁starts',\n",
       " '▁with',\n",
       " '▁an',\n",
       " '▁idea',\n",
       " '/',\n",
       " 'problem',\n",
       " '▁located',\n",
       " '▁at',\n",
       " '▁its',\n",
       " '▁center',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁starting',\n",
       " '▁point',\n",
       " '▁generates',\n",
       " '▁ideas',\n",
       " '/',\n",
       " 'work',\n",
       " '▁areas',\n",
       " ',',\n",
       " '▁incremented',\n",
       " '▁around',\n",
       " '▁this',\n",
       " '▁center',\n",
       " '▁in',\n",
       " '▁a',\n",
       " '▁radial',\n",
       " '▁structure',\n",
       " ',',\n",
       " '▁which',\n",
       " '▁in',\n",
       " '▁turn',\n",
       " '▁is',\n",
       " '▁completed',\n",
       " '▁with',\n",
       " '▁as',\n",
       " '▁many',\n",
       " '▁branches',\n",
       " '▁as',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁enables',\n",
       " '▁creativity',\n",
       " '▁and',\n",
       " '▁logic',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁mobilized',\n",
       " ',',\n",
       " '▁it',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁map',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁thoughts',\n",
       " '.',\n",
       " '▁Creativity',\n",
       " '▁is',\n",
       " '▁enhanced',\n",
       " '▁because',\n",
       " '▁participants',\n",
       " '▁feel',\n",
       " '▁comfortable',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁method',\n",
       " '.',\n",
       " '▁Application',\n",
       " '▁&',\n",
       " '▁Insight',\n",
       " '▁I',\n",
       " '▁start',\n",
       " '▁the',\n",
       " '▁process',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁creation',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁standing',\n",
       " '▁around',\n",
       " '▁a',\n",
       " '▁large',\n",
       " '▁board',\n",
       " '▁(',\n",
       " 'white',\n",
       " '▁or',\n",
       " '▁paper',\n",
       " '▁board',\n",
       " ')',\n",
       " '.',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁center',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁board',\n",
       " ',',\n",
       " '▁I',\n",
       " '▁write',\n",
       " '▁and',\n",
       " '▁highlight',\n",
       " '▁the',\n",
       " '▁topic',\n",
       " '▁to',\n",
       " '▁design',\n",
       " '.',\n",
       " '▁Through',\n",
       " '▁a',\n",
       " '▁series',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " ',',\n",
       " '▁I',\n",
       " '▁guide',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁in',\n",
       " '▁modelling',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁adapt',\n",
       " '▁the',\n",
       " '▁series',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " '▁according',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁topic',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁addressed',\n",
       " '.',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " ',',\n",
       " '▁we',\n",
       " '▁can',\n",
       " '▁use',\n",
       " ':',\n",
       " '▁who',\n",
       " ',',\n",
       " '▁what',\n",
       " ',',\n",
       " '▁when',\n",
       " ',',\n",
       " '▁where',\n",
       " ',',\n",
       " '▁why',\n",
       " ',',\n",
       " '▁how',\n",
       " ',',\n",
       " '▁how',\n",
       " '▁much',\n",
       " '.',\n",
       " '▁The',\n",
       " '▁use',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁“',\n",
       " 'why',\n",
       " '”',\n",
       " '▁is',\n",
       " '▁very',\n",
       " '▁interesting',\n",
       " '▁to',\n",
       " '▁understand',\n",
       " '▁the',\n",
       " '▁origin',\n",
       " '.',\n",
       " '▁By',\n",
       " '▁this',\n",
       " '▁way',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁interviewed',\n",
       " '▁person',\n",
       " '▁free',\n",
       " 's',\n",
       " '▁itself',\n",
       " '▁from',\n",
       " '▁paradigms',\n",
       " '▁and',\n",
       " '▁thus',\n",
       " '▁dares',\n",
       " '▁to',\n",
       " '▁propose',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁/',\n",
       " '▁ways',\n",
       " '▁of',\n",
       " '▁functioning',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁plan',\n",
       " '▁two',\n",
       " '▁hours',\n",
       " '▁for',\n",
       " '▁a',\n",
       " '▁workshop',\n",
       " '.',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " '▁2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁After',\n",
       " '▁modelling',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁on',\n",
       " '▁paper',\n",
       " ',',\n",
       " '▁I',\n",
       " '▁propose',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁participants',\n",
       " '▁a',\n",
       " '▁digital',\n",
       " '▁visualization',\n",
       " '▁of',\n",
       " '▁their',\n",
       " '▁work',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁addition',\n",
       " '▁of',\n",
       " '▁color',\n",
       " '▁codes',\n",
       " ',',\n",
       " '▁images',\n",
       " '▁and',\n",
       " '▁interconnections',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁second',\n",
       " '▁workshop',\n",
       " '▁also',\n",
       " '▁last',\n",
       " 's',\n",
       " '▁two',\n",
       " '▁hours',\n",
       " '▁and',\n",
       " '▁allows',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁to',\n",
       " '▁evolve',\n",
       " '.',\n",
       " '▁Once',\n",
       " '▁familiarized',\n",
       " '▁with',\n",
       " '▁it',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁discover',\n",
       " '▁the',\n",
       " '▁power',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁tool',\n",
       " '.',\n",
       " '▁Then',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁second',\n",
       " '▁workshop',\n",
       " '▁brings',\n",
       " '▁out',\n",
       " '▁even',\n",
       " '▁more',\n",
       " '▁ideas',\n",
       " '▁and',\n",
       " '▁constructive',\n",
       " '▁exchanges',\n",
       " '▁between',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '.',\n",
       " '▁Around',\n",
       " '▁this',\n",
       " '▁new',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " ',',\n",
       " '▁they',\n",
       " '▁have',\n",
       " '▁learned',\n",
       " '▁to',\n",
       " '▁work',\n",
       " '▁together',\n",
       " '▁and',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁make',\n",
       " '▁visible',\n",
       " '▁the',\n",
       " '▁untold',\n",
       " '▁ideas',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁now',\n",
       " '▁present',\n",
       " '▁all',\n",
       " '▁the',\n",
       " '▁projects',\n",
       " '▁I',\n",
       " '▁manage',\n",
       " '▁in',\n",
       " '▁this',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁format',\n",
       " '▁in',\n",
       " '▁order',\n",
       " '▁to',\n",
       " '▁ease',\n",
       " '▁rapid',\n",
       " '▁understanding',\n",
       " '▁for',\n",
       " '▁decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '.',\n",
       " '▁These',\n",
       " '▁presentations',\n",
       " '▁are',\n",
       " '▁the',\n",
       " '▁core',\n",
       " '▁of',\n",
       " '▁my',\n",
       " '▁business',\n",
       " '▁models',\n",
       " '.',\n",
       " '▁The',\n",
       " '▁decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '▁are',\n",
       " '▁thus',\n",
       " '▁able',\n",
       " '▁to',\n",
       " '▁identify',\n",
       " '▁the',\n",
       " '▁opportunities',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁projects',\n",
       " '▁and',\n",
       " '▁can',\n",
       " '▁take',\n",
       " '▁quick',\n",
       " '▁decisions',\n",
       " '▁to',\n",
       " '▁validate',\n",
       " '▁them',\n",
       " '.',\n",
       " '▁They',\n",
       " '▁find',\n",
       " '▁answers',\n",
       " '▁to',\n",
       " '▁their',\n",
       " '▁questions',\n",
       " '▁thank',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁schematic',\n",
       " '▁representation',\n",
       " '.',\n",
       " '▁Approach',\n",
       " '▁What',\n",
       " '▁I',\n",
       " '▁find',\n",
       " '▁amazing',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁facilitation',\n",
       " '▁of',\n",
       " '▁this',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁workshop',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁participants',\n",
       " '▁commitment',\n",
       " '▁for',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁helps',\n",
       " '▁to',\n",
       " '▁give',\n",
       " '▁meaning',\n",
       " '.',\n",
       " '▁The',\n",
       " '▁participants',\n",
       " '▁appropriate',\n",
       " '▁the',\n",
       " '▁story',\n",
       " '▁and',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁keep',\n",
       " '▁writing',\n",
       " '▁it',\n",
       " '.',\n",
       " '▁Then',\n",
       " ',',\n",
       " '▁they',\n",
       " '▁easily',\n",
       " '▁become',\n",
       " '▁actors',\n",
       " '▁or',\n",
       " '▁sponsors',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '.',\n",
       " '▁A',\n",
       " '▁trust',\n",
       " '▁relationship',\n",
       " '▁is',\n",
       " '▁built',\n",
       " ',',\n",
       " '▁thus',\n",
       " '▁facilitating',\n",
       " '▁the',\n",
       " '▁implementation',\n",
       " '▁of',\n",
       " '▁related',\n",
       " '▁actions',\n",
       " '.',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " '▁2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁Annex',\n",
       " '▁1',\n",
       " ':',\n",
       " '▁Mind',\n",
       " '▁Map',\n",
       " '▁Shared',\n",
       " '▁facilities',\n",
       " '▁project',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e6eae-bf3a-488d-9b08-132353ffcb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5604d-9691-4d9a-9575-3125d45dac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style = 'span')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d4ce5-7697-437d-8075-f2f693d3164d",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ae0ca-64a5-472c-893f-39c6e0dbd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text containing PII\n",
    "text = df_train.loc[0].full_text\n",
    "\n",
    "# Process the text with spaCy NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print identified entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026e9ea-8b06-48c6-b054-4d2b5b079fde",
   "metadata": {},
   "source": [
    "## Vis Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230d7e7-7153-44de-87fd-7bb261e08d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document = df_train.loc[0].full_text\n",
    "\n",
    "labels = [\n",
    "    {\"start\": 1, \"end\": 2, \"label\": \"ORG\"},\n",
    "    {\"start\": 3, \"end\": 4, \"label\": \"PERSON\"},\n",
    "    # Add more label entries as needed\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(document)\n",
    "entities = [(ent[\"start\"], ent[\"end\"], ent[\"label\"]) for ent in labels]\n",
    "spans = [spacy.tokens.Span(doc, start, end, label=label) for start, end, label in entities]\n",
    "\n",
    "\n",
    "doc.spans['sc'] = spans\n",
    "displacy.render(doc, style=\"span\", jupyter=True)\n",
    "\n",
    "# doc.ents = spans\n",
    "# displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e95fe5f-6ba8-4939-aa81-579631665cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[, ]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spacy.tokens.Span(doc, start, end, label=label) for start, end, label in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47de1448-2095-4438-b803-621c84330ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d373f95d-fe9d-4505-9c63-687bc1d2470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your: 0 - 4\n",
      "long: 5 - 9\n",
      "document: 10 - 18\n",
      "goes: 19 - 23\n",
      "here: 24 - 28\n",
      ".: 28 - 29\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.idx} - {token.idx + len(token.text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e74559-bb82-4b29-8ec2-d3e0863d3010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n",
      "\n",
      "Challenge & selection\n",
      "\n",
      "The tool I use to help all stakeholders finding their way through the complexity of a project is the  mind map.\n",
      "\n",
      "What exactly is a mind map? According to the definition of Buzan T. and Buzan B. (1999, Dessine-moi  l'intelligence. Paris: Les Éditions d'Organisation.), the mind map (or heuristic diagram) is a graphic  representation technique that follows the natural functioning of the mind and allows the brain's  potential to be released. Cf Annex1\n",
      "\n",
      "This tool has many advantages:\n",
      "\n",
      "•  It is accessible to all and does not require significant material investment and can be done  quickly\n",
      "\n",
      "•  It is scalable\n",
      "\n",
      "•  It allows categorization and linking of information\n",
      "\n",
      "•  It can be applied to any type of situation: notetaking, problem solving, analysis, creation of  new ideas\n",
      "\n",
      "•  It is suitable for all people and is easy to learn\n",
      "\n",
      "•  It is fun and encourages exchanges\n",
      "\n",
      "•  It makes visible the dimension of projects, opportunities, interconnections\n",
      "\n",
      "•  It synthesizes\n",
      "\n",
      "•  It makes the project understandable\n",
      "\n",
      "•  It allows you to explore ideas\n",
      "\n",
      "The creation of a mind map starts with an idea/problem located at its center. This starting point  generates ideas/work areas, incremented around this center in a radial structure, which in turn is  completed with as many branches as new ideas.\n",
      "\n",
      "This tool enables creativity and logic to be mobilized, it is a map of the thoughts.\n",
      "\n",
      "Creativity is enhanced because participants feel comfortable with the method.\n",
      "\n",
      "Application & Insight\n",
      "\n",
      "I start the process of the mind map creation with the stakeholders standing around a large board  (white or paper board). In the center of the board, I write and highlight the topic to design.\n",
      "\n",
      "Through a series of questions, I guide the stakeholders in modelling the mind map. I adapt the series  of questions according to the topic to be addressed. In the type of questions, we can use: who, what,  when, where, why, how, how much.\n",
      "\n",
      "The use of the “why” is very interesting to understand the origin. By this way, the interviewed person  frees itself from paradigms and thus dares to propose new ideas / ways of functioning. I plan two  hours for a workshop.\n",
      "\n",
      "Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n",
      "\n",
      "After modelling the mind map on paper, I propose to the participants a digital visualization of their  work with the addition of color codes, images and interconnections. This second workshop also lasts  two hours and allows the mind map to evolve. Once familiarized with it, the stakeholders discover  the power of the tool. Then, the second workshop brings out even more ideas and constructive  exchanges between the stakeholders. Around this new mind map, they have learned to work  together and want to make visible the untold ideas.\n",
      "\n",
      "I now present all the projects I manage in this type of format in order to ease rapid understanding for  decision-makers. These presentations are the core of my business models. The decision-makers are  thus able to identify the opportunities of the projects and can take quick decisions to validate them.  They find answers to their questions thank to a schematic representation.\n",
      "\n",
      "Approach\n",
      "\n",
      "What I find amazing with the facilitation of this type of workshop is the participants commitment for  the project. This tool helps to give meaning. The participants appropriate the story and want to keep  writing it. Then, they easily become actors or sponsors of the project. A trust relationship is built,  thus facilitating the implementation of related actions.\n",
      "\n",
      "Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n",
      "\n",
      "Annex 1: Mind Map Shared facilities project\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( df_train.loc[0].full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2753622-e40b-4a45-8cc4-b3437e0dd2db",
   "metadata": {},
   "source": [
    "# BERT (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ee79a-17f8-44d0-b4d9-9866aa159a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64863289-e7b7-4693-a3ec-a78119a6a80e",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ec3bd-7106-49ea-8017-8d666460340c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print confidence scores for each label\n",
    "confidence_scores = tf.nn.softmax(outputs, axis=-1).numpy()[0]\n",
    "for token, label, confidence_score in zip(tokens, predicted_labels, confidence_scores):\n",
    "    confidence = confidence_score[tokenizer.convert_tokens_to_ids(label)]\n",
    "    print(f\"Token: {token}, Label: {label}, Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e66dce-3661-438b-9abb-debb5c0594c1",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d69838e-9cb9-4917-8502-f5f3c2dbee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Sample text with PII\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"dslim/bert-large-NER\" \n",
    "# model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # You can use other BERT models from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer.encode_plus(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# inputs = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "tokens_full = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "tokens = tokens_full[:500]\n",
    "\n",
    "\n",
    "# print(\"Input Text:\", text)\n",
    "# print(\"Tokenized Input:\", tokens)\n",
    "# print(\"Input IDs:\", inputs)\n",
    "\n",
    "\n",
    "# Perform token classification (NER)\n",
    "outputs = model(inputs[\"input_ids\"])\n",
    "\n",
    "# print(\"Raw Outputs:\", outputs)\n",
    "\n",
    "predictions = tf.argmax(outputs.logits, axis=2)\n",
    "\n",
    "# Decode the predicted labels\n",
    "# predicted_labels = [tokenizer.convert_ids_to_tokens(prediction) for prediction in predictions.numpy()[0]]\n",
    "predicted_labels = tokenizer.convert_ids_to_tokens(predictions.numpy()[0]) \n",
    "\n",
    "# print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "for token, label in zip(inputs[\"input_ids\"].numpy()[0], predicted_labels):\n",
    "    token = tokenizer.decode(token)\n",
    "    if label.startswith('B'):\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = token\n",
    "    elif label.startswith('I'):\n",
    "        current_entity += ' ' + token\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = \"\"\n",
    "\n",
    "# Add the last entity if any\n",
    "if current_entity:\n",
    "    entities.append(current_entity.strip())\n",
    "\n",
    "# Display the identified entities\n",
    "# print(\"Identified Entities:\")\n",
    "# for entity in entities:\n",
    "#     print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3e566-90cf-4462-ab41-7cea0fb57e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67d26be5-faec-48bb-8d5b-7700d63a96d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vis Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72148b83-0105-452f-a3c8-c40b99fde6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(tokens)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae9c49-e2e2-46f0-9977-a718c268aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3e278-7c1a-452e-ac3f-715abffdebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# # Create spaCy Doc object from the original text\n",
    "doc = nlp(text)\n",
    "doc = Doc(doc.vocab, words=tokens)\n",
    "\n",
    "# Initialize spans to store entity positions\n",
    "spans = []\n",
    "\n",
    "# Initialize variables to track current entity\n",
    "current_start = None\n",
    "current_end = None\n",
    "current_label = None\n",
    "\n",
    "# Iterate through tokens and predicted labels\n",
    "for i, (token, label) in enumerate(zip(tokens, predicted_labels)):\n",
    "    if label != '[PAD]':\n",
    "        if label.startswith('B') or label.startswith('I') or label.startswith('[unused'):\n",
    "            # Start or continuation of an entity\n",
    "            if current_start is not None:\n",
    "                spans.append((current_start, current_end, current_label))\n",
    "            current_start = i\n",
    "            current_end = i + 1\n",
    "            current_label = label[2:] if label.startswith('B') or label.startswith('I') else label  # Removing the 'B-' or 'I-' prefix\n",
    "        else:\n",
    "            # Outside of any entity\n",
    "            if current_start is not None:\n",
    "                spans.append((current_start, current_end, current_label))\n",
    "                current_start = None\n",
    "                current_end = None\n",
    "                current_label = None\n",
    "\n",
    "# Add the last entity if any\n",
    "if current_start is not None:\n",
    "    spans.append((current_start, current_end, current_label))\n",
    "\n",
    "# Create a list of entities with start and end positions\n",
    "entities = [{\"start\": start, \"end\": end, \"label\": label} for start, end, label in spans]\n",
    "\n",
    "# Create Span objects and set them in the Doc\n",
    "for ent in entities:\n",
    "    start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "    span = Span(doc, start, end, label=label)\n",
    "    doc.ents = list(doc.ents)\n",
    "\n",
    "# Prepare data for displacy visualization\n",
    "options = {\"ents\": [ent[\"label\"] for ent in entities], \"colors\": {}}\n",
    "for ent in entities:\n",
    "    options[\"colors\"][ent[\"label\"]] = \"yellow\"  # You can change the color\n",
    "\n",
    "# Visualize using displacy\n",
    "displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08d206-825b-4ef8-8fc3-a4939465c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Span objects and set them in the Doc\n",
    "\n",
    "spans_2 = []\n",
    "\n",
    "for ent in entities:\n",
    "    start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "    span = Span(doc, start, end, label=label)\n",
    "    spans_2.append(span)\n",
    "    \n",
    "doc.ents = spans_2\n",
    "\n",
    "# Prepare data for displacy visualization\n",
    "options = {\"ents\": [ent[\"label\"] for ent in entities], \"colors\": {}}\n",
    "for ent in entities:\n",
    "    options[\"colors\"][ent[\"label\"]] = \"yellow\"  # You can change the color\n",
    "\n",
    "# Visualize using displacy\n",
    "displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323fdbf5-4b3d-4865-a8aa-91414be97abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750aeb2-4cb1-4801-9c97-b0b8b43671bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(full_text,tokens,pred):\n",
    "    \n",
    "    nlp = spacy.blank(\"en\")\n",
    "    # # Create spaCy Doc object from the original text\n",
    "    doc = nlp(full_text)\n",
    "    doc = Doc(doc.vocab, words=tokens)\n",
    "\n",
    "    # Initialize spans to store entity positions\n",
    "    spans = []\n",
    "\n",
    "    # Initialize variables to track current entity\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "    current_label = None\n",
    "\n",
    "    # Iterate through tokens and predicted labels\n",
    "    for i, (token, label) in enumerate(zip(tokens, predicted_labels)):\n",
    "        if label != '[PAD]':\n",
    "            if label.startswith('B') or label.startswith('I') or label.startswith('[unused'):\n",
    "                # Start or continuation of an entity\n",
    "                if current_start is not None:\n",
    "                    spans.append((current_start, current_end, current_label))\n",
    "                current_start = i\n",
    "                current_end = i + 1\n",
    "                current_label = label[2:] if label.startswith('B') or label.startswith('I') else label  # Removing the 'B-' or 'I-' prefix\n",
    "            else:\n",
    "                # Outside of any entity\n",
    "                if current_start is not None:\n",
    "                    spans.append((current_start, current_end, current_label))\n",
    "                    current_start = None\n",
    "                    current_end = None\n",
    "                    current_label = None\n",
    "\n",
    "    # Add the last entity if any\n",
    "    if current_start is not None:\n",
    "        spans.append((current_start, current_end, current_label))\n",
    "        \n",
    "    spans = [Span(doc, start, end, label=label) for start, end, label in spans]\n",
    "    \n",
    "    doc.spans[\"sc\"] = spans\n",
    "\n",
    "\n",
    "    # Create a list of entities with start and end positions\n",
    "#     entities = [{\"start\": start, \"end\": end, \"label\": label} for start, end, label in spans]\n",
    "\n",
    "#     # Create Span objects and set them in the Doc\n",
    "#     for ent in entities:\n",
    "#         start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "#         span = Span(doc, start, end, label=label)\n",
    "#         doc.ents = list(doc.ents)\n",
    "\n",
    "    # Prepare data for displacy visualization\n",
    "    options = {\"ents\": [ent[\"label\"] for ent in entities], \"colors\": {}}\n",
    "    for ent in entities:\n",
    "        options[\"colors\"][ent[\"label\"]] = \"yellow\"  # You can change the color\n",
    "\n",
    "    # Visualize using displacy\n",
    "    displacy.render(doc, style=\"span\", options=options, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fee5e-894d-4ba6-827c-57db230c4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(tokens)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa183b-d51f-4c48-97f3-9f6b6905bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(text,tokens,predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5436e10-dc6d-412e-abe6-e268e6b15f56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f0d6ad4-3555-4211-8b20-37617db63fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 1.18MB/s]\n",
      "D:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Thorn\\.cache\\huggingface\\hub\\models--geckos--deberta-base-fine-tuned-ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 798k/798k [00:00<00:00, 3.30MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 10.1MB/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████| 778/778 [00:00<?, ?B/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.36M/1.36M [00:01<00:00, 1.27MB/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████| 1.13k/1.13k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "geckos/deberta-base-fine-tuned-ner does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeckos/deberta-base-fine-tuned-ner\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# You can use other BERT models from Hugging Face\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DebertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFDebertaForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize the input text\u001b[39;00m\n\u001b[0;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2833\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for sharded checkpoints using safetensors is coming soon!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m     )\n\u001b[0;32m   2832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 2833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2834\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2835\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for PyTorch weights. Use `from_pt=True` to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2836\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2837\u001b[0m     )\n\u001b[0;32m   2838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2842\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: geckos/deberta-base-fine-tuned-ner does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "# Sample text with PII\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "# model_name = \"knowledgator/UTC-DeBERTa-large\"\n",
    "model_name = \"geckos/deberta-base-fine-tuned-ner\"  # You can use other BERT models from Hugging Face\n",
    "tokenizer = DebertaTokenizer.from_pretrained(model_name)\n",
    "model = TFDebertaForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer.encode_plus(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# inputs = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "tokens_full = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "tokens = tokens_full[:500]\n",
    "\n",
    "\n",
    "# print(\"Input Text:\", text)\n",
    "# print(\"Tokenized Input:\", tokens)\n",
    "# print(\"Input IDs:\", inputs)\n",
    "\n",
    "\n",
    "# Perform token classification (NER)\n",
    "outputs = model(inputs[\"input_ids\"])\n",
    "\n",
    "# print(\"Raw Outputs:\", outputs)\n",
    "\n",
    "predictions = tf.argmax(outputs.logits, axis=2)\n",
    "\n",
    "# Decode the predicted labels\n",
    "# predicted_labels = [tokenizer.convert_ids_to_tokens(prediction) for prediction in predictions.numpy()[0]]\n",
    "predicted_labels = tokenizer.convert_ids_to_tokens(predictions.numpy()[0]) \n",
    "\n",
    "# print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "for token, label in zip(inputs[\"input_ids\"].numpy()[0], predicted_labels):\n",
    "    token = tokenizer.decode(token)\n",
    "    if label.startswith('B'):\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = token\n",
    "    elif label.startswith('I'):\n",
    "        current_entity += ' ' + token\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = \"\"\n",
    "\n",
    "# Add the last entity if any\n",
    "if current_entity:\n",
    "    entities.append(current_entity.strip())\n",
    "\n",
    "# Display the identified entities\n",
    "# print(\"Identified Entities:\")\n",
    "# for entity in entities:\n",
    "#     print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e66e85c-3a78-46b8-8ee7-99fef67815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701da4b-51b6-446d-90b3-061be462a497",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "874008ae-c258-4f51-947b-f138504917b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DebertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "text = \"John Doe's email is john.doe@email.com, and his phone number is +1 (555) 123-4567.\"\n",
    "\n",
    "# model_name = \"geckos/deberta-base-fine-tuned-ner\" \n",
    "# model_name = \"knowledgator/UTC-DeBERTa-large\"\n",
    "# model_name = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "model_name = \"Yanis/microsoft-deberta-v3-large_ner_conll2003-anonimization_TRY_1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec807ead-6a28-43e6-89fd-35e73d32ab49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  2,  2,  2,  2,  0,  0, 14, 14, 14, 14, 14, 14, 14, 14,  0,  0,  0,\n",
       "          0,  0,  0,  4,  4,  4,  4,  4,  4,  4,  4,  4,  0,  2]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8aeb5de-641e-482b-ac4f-34711dd2db0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Civil state',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'O',\n",
       " 'Name']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80e01186-6b3b-46da-9161-6fef4ef8cb58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁John',\n",
       " '▁Doe',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁email',\n",
       " '▁is',\n",
       " '▁john',\n",
       " '.',\n",
       " 'do',\n",
       " 'e',\n",
       " '@',\n",
       " 'email',\n",
       " '.',\n",
       " 'com',\n",
       " ',',\n",
       " '▁and',\n",
       " '▁his',\n",
       " '▁phone',\n",
       " '▁number',\n",
       " '▁is',\n",
       " '▁+',\n",
       " '1',\n",
       " '▁(',\n",
       " '555',\n",
       " ')',\n",
       " '▁123',\n",
       " '-',\n",
       " '45',\n",
       " '67',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c033c326-0af7-4b8e-b24c-71c8371666c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  610, 28484,    18,  1047,    16, 41906,     4,   417,  3540,  1039,\n",
       "         10555,     4,   175,     6,     8,    39,  1028,   346,    16,  2055,\n",
       "           134,    36, 33772,    43, 17072,    12,  1898,  4111,     4]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89ab8ba-96d1-43e9-809e-d9449fbf065e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ace1c-190f-4354-a51a-72a67fa27392",
   "metadata": {},
   "source": [
    "## Run  (Yanis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6def2d66-f3d1-4ea3-8577-48958a9e0f89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Religious beliefs',\n",
       " 'Religious beliefs',\n",
       " 'Physical addresses',\n",
       " 'Religious beliefs',\n",
       " 'Religious beliefs',\n",
       " 'Physical addresses',\n",
       " 'Phone Numbers',\n",
       " 'Religious beliefs',\n",
       " 'O',\n",
       " 'Physical addresses',\n",
       " 'O',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Email address',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Email address',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "import torch\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "# model_name = \"geckos/deberta-base-fine-tuned-ner\" \n",
    "# model_name = \"knowledgator/UTC-DeBERTa-large\"\n",
    "# model_name = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "model_name = \"Yanis/microsoft-deberta-v3-large_ner_conll2003-anonimization_TRY_1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e43ee-251b-41a6-801c-11423b43fa65",
   "metadata": {},
   "source": [
    "## Run (lakshyakh93/deberta_finetuned_pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfed32fb-fcf0-4199-bb33-bbf71b4ff40e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'O',\n",
       " 'I-FIRSTNAME',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'I-COMPANY_NAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'O',\n",
       " 'I-FIRSTNAME',\n",
       " 'I-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "import torch\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "model_name = \"lakshyakh93/deberta_finetuned_pii\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DebertaForTokenClassification.from_pretrained(model_name)\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a418da7e-b1bb-4aa0-babd-65027c35cbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_tokens_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08ffcce4-bc40-461b-8a00-6429e1c35750",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " '▁2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁Challenge',\n",
       " '▁&',\n",
       " '▁selection',\n",
       " '▁The',\n",
       " '▁tool',\n",
       " '▁I',\n",
       " '▁use',\n",
       " '▁to',\n",
       " '▁help',\n",
       " '▁all',\n",
       " '▁stakeholders',\n",
       " '▁finding',\n",
       " '▁their',\n",
       " '▁way',\n",
       " '▁through',\n",
       " '▁the',\n",
       " '▁complexity',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁project',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '.',\n",
       " '▁What',\n",
       " '▁exactly',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '?',\n",
       " '▁According',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁definition',\n",
       " '▁of',\n",
       " '▁Buz',\n",
       " 'an',\n",
       " '▁T',\n",
       " '.',\n",
       " '▁and',\n",
       " '▁Buz',\n",
       " 'an',\n",
       " '▁B',\n",
       " '.',\n",
       " '▁(',\n",
       " '1999',\n",
       " ',',\n",
       " '▁Des',\n",
       " 's',\n",
       " 'ine',\n",
       " '-',\n",
       " 'moi',\n",
       " '▁l',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '.',\n",
       " '▁Paris',\n",
       " ':',\n",
       " '▁Les',\n",
       " '▁É',\n",
       " 'dition',\n",
       " 's',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '.',\n",
       " ')',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁(',\n",
       " 'or',\n",
       " '▁heuristic',\n",
       " '▁diagram',\n",
       " ')',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁graphic',\n",
       " '▁representation',\n",
       " '▁technique',\n",
       " '▁that',\n",
       " '▁follows',\n",
       " '▁the',\n",
       " '▁natural',\n",
       " '▁functioning',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁and',\n",
       " '▁allows',\n",
       " '▁the',\n",
       " '▁brain',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁potential',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁released',\n",
       " '.',\n",
       " '▁Cf',\n",
       " '▁Annex',\n",
       " '1',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁has',\n",
       " '▁many',\n",
       " '▁advantages',\n",
       " ':',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁accessible',\n",
       " '▁to',\n",
       " '▁all',\n",
       " '▁and',\n",
       " '▁does',\n",
       " '▁not',\n",
       " '▁require',\n",
       " '▁significant',\n",
       " '▁material',\n",
       " '▁investment',\n",
       " '▁and',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁done',\n",
       " '▁quickly',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁scalable',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁allows',\n",
       " '▁categorization',\n",
       " '▁and',\n",
       " '▁linking',\n",
       " '▁of',\n",
       " '▁information',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁applied',\n",
       " '▁to',\n",
       " '▁any',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁situation',\n",
       " ':',\n",
       " '▁note',\n",
       " 'taking',\n",
       " ',',\n",
       " '▁problem',\n",
       " '▁solving',\n",
       " ',',\n",
       " '▁analysis',\n",
       " ',',\n",
       " '▁creation',\n",
       " '▁of',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁suitable',\n",
       " '▁for',\n",
       " '▁all',\n",
       " '▁people',\n",
       " '▁and',\n",
       " '▁is',\n",
       " '▁easy',\n",
       " '▁to',\n",
       " '▁learn',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁is',\n",
       " '▁fun',\n",
       " '▁and',\n",
       " '▁encourages',\n",
       " '▁exchanges',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁makes',\n",
       " '▁visible',\n",
       " '▁the',\n",
       " '▁dimension',\n",
       " '▁of',\n",
       " '▁projects',\n",
       " ',',\n",
       " '▁opportunities',\n",
       " ',',\n",
       " '▁interconnections',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁synthesize',\n",
       " 's',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁makes',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁understandable',\n",
       " '▁•',\n",
       " '▁It',\n",
       " '▁allows',\n",
       " '▁you',\n",
       " '▁to',\n",
       " '▁explore',\n",
       " '▁ideas',\n",
       " '▁The',\n",
       " '▁creation',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁starts',\n",
       " '▁with',\n",
       " '▁an',\n",
       " '▁idea',\n",
       " '/',\n",
       " 'problem',\n",
       " '▁located',\n",
       " '▁at',\n",
       " '▁its',\n",
       " '▁center',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁starting',\n",
       " '▁point',\n",
       " '▁generates',\n",
       " '▁ideas',\n",
       " '/',\n",
       " 'work',\n",
       " '▁areas',\n",
       " ',',\n",
       " '▁incremented',\n",
       " '▁around',\n",
       " '▁this',\n",
       " '▁center',\n",
       " '▁in',\n",
       " '▁a',\n",
       " '▁radial',\n",
       " '▁structure',\n",
       " ',',\n",
       " '▁which',\n",
       " '▁in',\n",
       " '▁turn',\n",
       " '▁is',\n",
       " '▁completed',\n",
       " '▁with',\n",
       " '▁as',\n",
       " '▁many',\n",
       " '▁branches',\n",
       " '▁as',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁enables',\n",
       " '▁creativity',\n",
       " '▁and',\n",
       " '▁logic',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁mobilized',\n",
       " ',',\n",
       " '▁it',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁map',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁thoughts',\n",
       " '.',\n",
       " '▁Creativity',\n",
       " '▁is',\n",
       " '▁enhanced',\n",
       " '▁because',\n",
       " '▁participants',\n",
       " '▁feel',\n",
       " '▁comfortable',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁method',\n",
       " '.',\n",
       " '▁Application',\n",
       " '▁&',\n",
       " '▁Insight',\n",
       " '▁I',\n",
       " '▁start',\n",
       " '▁the',\n",
       " '▁process',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁creation',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁standing',\n",
       " '▁around',\n",
       " '▁a',\n",
       " '▁large',\n",
       " '▁board',\n",
       " '▁(',\n",
       " 'white',\n",
       " '▁or',\n",
       " '▁paper',\n",
       " '▁board',\n",
       " ')',\n",
       " '.',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁center',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁board',\n",
       " ',',\n",
       " '▁I',\n",
       " '▁write',\n",
       " '▁and',\n",
       " '▁highlight',\n",
       " '▁the',\n",
       " '▁topic',\n",
       " '▁to',\n",
       " '▁design',\n",
       " '.',\n",
       " '▁Through',\n",
       " '▁a',\n",
       " '▁series',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " ',',\n",
       " '▁I',\n",
       " '▁guide',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁in',\n",
       " '▁modelling',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁adapt',\n",
       " '▁the',\n",
       " '▁series',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " '▁according',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁topic',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁addressed',\n",
       " '.',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁questions',\n",
       " ',',\n",
       " '▁we',\n",
       " '▁can',\n",
       " '▁use',\n",
       " ':',\n",
       " '▁who',\n",
       " ',',\n",
       " '▁what',\n",
       " ',',\n",
       " '▁when',\n",
       " ',',\n",
       " '▁where',\n",
       " ',',\n",
       " '▁why',\n",
       " ',',\n",
       " '▁how',\n",
       " ',',\n",
       " '▁how',\n",
       " '▁much',\n",
       " '.',\n",
       " '▁The',\n",
       " '▁use',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁“',\n",
       " 'why',\n",
       " '”',\n",
       " '▁is',\n",
       " '▁very',\n",
       " '▁interesting',\n",
       " '▁to',\n",
       " '▁understand',\n",
       " '▁the',\n",
       " '▁origin',\n",
       " '.',\n",
       " '▁By',\n",
       " '▁this',\n",
       " '▁way',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁interviewed',\n",
       " '▁person',\n",
       " '▁free',\n",
       " 's',\n",
       " '▁itself',\n",
       " '▁from',\n",
       " '▁paradigms',\n",
       " '▁and',\n",
       " '▁thus',\n",
       " '▁dares',\n",
       " '▁to',\n",
       " '▁propose',\n",
       " '▁new',\n",
       " '▁ideas',\n",
       " '▁/',\n",
       " '▁ways',\n",
       " '▁of',\n",
       " '▁functioning',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁plan',\n",
       " '▁two',\n",
       " '▁hours',\n",
       " '▁for',\n",
       " '▁a',\n",
       " '▁workshop',\n",
       " '.',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " '▁2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁After',\n",
       " '▁modelling',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁on',\n",
       " '▁paper',\n",
       " ',',\n",
       " '▁I',\n",
       " '▁propose',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁participants',\n",
       " '▁a',\n",
       " '▁digital',\n",
       " '▁visualization',\n",
       " '▁of',\n",
       " '▁their',\n",
       " '▁work',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁addition',\n",
       " '▁of',\n",
       " '▁color',\n",
       " '▁codes',\n",
       " ',',\n",
       " '▁images',\n",
       " '▁and',\n",
       " '▁interconnections',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁second',\n",
       " '▁workshop',\n",
       " '▁also',\n",
       " '▁last',\n",
       " 's',\n",
       " '▁two',\n",
       " '▁hours',\n",
       " '▁and',\n",
       " '▁allows',\n",
       " '▁the',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " '▁to',\n",
       " '▁evolve',\n",
       " '.',\n",
       " '▁Once',\n",
       " '▁familiarized',\n",
       " '▁with',\n",
       " '▁it',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '▁discover',\n",
       " '▁the',\n",
       " '▁power',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁tool',\n",
       " '.',\n",
       " '▁Then',\n",
       " ',',\n",
       " '▁the',\n",
       " '▁second',\n",
       " '▁workshop',\n",
       " '▁brings',\n",
       " '▁out',\n",
       " '▁even',\n",
       " '▁more',\n",
       " '▁ideas',\n",
       " '▁and',\n",
       " '▁constructive',\n",
       " '▁exchanges',\n",
       " '▁between',\n",
       " '▁the',\n",
       " '▁stakeholders',\n",
       " '.',\n",
       " '▁Around',\n",
       " '▁this',\n",
       " '▁new',\n",
       " '▁mind',\n",
       " '▁map',\n",
       " ',',\n",
       " '▁they',\n",
       " '▁have',\n",
       " '▁learned',\n",
       " '▁to',\n",
       " '▁work',\n",
       " '▁together',\n",
       " '▁and',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁make',\n",
       " '▁visible',\n",
       " '▁the',\n",
       " '▁untold',\n",
       " '▁ideas',\n",
       " '.',\n",
       " '▁I',\n",
       " '▁now',\n",
       " '▁present',\n",
       " '▁all',\n",
       " '▁the',\n",
       " '▁projects',\n",
       " '▁I',\n",
       " '▁manage',\n",
       " '▁in',\n",
       " '▁this',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁format',\n",
       " '▁in',\n",
       " '▁order',\n",
       " '▁to',\n",
       " '▁ease',\n",
       " '▁rapid',\n",
       " '▁understanding',\n",
       " '▁for',\n",
       " '▁decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '.',\n",
       " '▁These',\n",
       " '▁presentations',\n",
       " '▁are',\n",
       " '▁the',\n",
       " '▁core',\n",
       " '▁of',\n",
       " '▁my',\n",
       " '▁business',\n",
       " '▁models',\n",
       " '.',\n",
       " '▁The',\n",
       " '▁decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '▁are',\n",
       " '▁thus',\n",
       " '▁able',\n",
       " '▁to',\n",
       " '▁identify',\n",
       " '▁the',\n",
       " '▁opportunities',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁projects',\n",
       " '▁and',\n",
       " '▁can',\n",
       " '▁take',\n",
       " '▁quick',\n",
       " '▁decisions',\n",
       " '▁to',\n",
       " '▁validate',\n",
       " '▁them',\n",
       " '.',\n",
       " '▁They',\n",
       " '▁find',\n",
       " '▁answers',\n",
       " '▁to',\n",
       " '▁their',\n",
       " '▁questions',\n",
       " '▁thank',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁schematic',\n",
       " '▁representation',\n",
       " '.',\n",
       " '▁Approach',\n",
       " '▁What',\n",
       " '▁I',\n",
       " '▁find',\n",
       " '▁amazing',\n",
       " '▁with',\n",
       " '▁the',\n",
       " '▁facilitation',\n",
       " '▁of',\n",
       " '▁this',\n",
       " '▁type',\n",
       " '▁of',\n",
       " '▁workshop',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁participants',\n",
       " '▁commitment',\n",
       " '▁for',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '.',\n",
       " '▁This',\n",
       " '▁tool',\n",
       " '▁helps',\n",
       " '▁to',\n",
       " '▁give',\n",
       " '▁meaning',\n",
       " '.',\n",
       " '▁The',\n",
       " '▁participants',\n",
       " '▁appropriate',\n",
       " '▁the',\n",
       " '▁story',\n",
       " '▁and',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁keep',\n",
       " '▁writing',\n",
       " '▁it',\n",
       " '.',\n",
       " '▁Then',\n",
       " ',',\n",
       " '▁they',\n",
       " '▁easily',\n",
       " '▁become',\n",
       " '▁actors',\n",
       " '▁or',\n",
       " '▁sponsors',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '.',\n",
       " '▁A',\n",
       " '▁trust',\n",
       " '▁relationship',\n",
       " '▁is',\n",
       " '▁built',\n",
       " ',',\n",
       " '▁thus',\n",
       " '▁facilitating',\n",
       " '▁the',\n",
       " '▁implementation',\n",
       " '▁of',\n",
       " '▁related',\n",
       " '▁actions',\n",
       " '.',\n",
       " '▁Design',\n",
       " '▁Thinking',\n",
       " '▁for',\n",
       " '▁innovation',\n",
       " '▁reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " '▁2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " '▁S',\n",
       " 'ylla',\n",
       " '▁Annex',\n",
       " '▁1',\n",
       " ':',\n",
       " '▁Mind',\n",
       " '▁Map',\n",
       " '▁Shared',\n",
       " '▁facilities',\n",
       " '▁project',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900dcd-4b7a-4493-bf50-45025e011898",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d12f5-3be8-4115-b3e2-321376bfed0d",
   "metadata": {},
   "source": [
    "## Token Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77dafabd-e415-4126-8897-b56b35b96744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n",
    "    \n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa1abdce-6f19-4534-9c59-da6983a3761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MAX_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1b67f43-8583-489d-b2e0-ac2ce154354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokens = tokenize(df_train.loc[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9ec72ae-70db-4733-a2e3-4e056d2e591e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'token_map'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e42f06-ed65-49ef-9088-25a9c988bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f21aa4b-7586-4531-864b-4698b6372808",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'ĠDesign',\n",
       " 'ĠThinking',\n",
       " 'Ġfor',\n",
       " 'Ġinnovation',\n",
       " 'Ġreflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'Ġ2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'ath',\n",
       " 'al',\n",
       " 'ie',\n",
       " 'ĠSy',\n",
       " 'lla',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'Chall',\n",
       " 'enge',\n",
       " 'Ġ&',\n",
       " 'Ġselection',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'The',\n",
       " 'Ġtool',\n",
       " 'ĠI',\n",
       " 'Ġuse',\n",
       " 'Ġto',\n",
       " 'Ġhelp',\n",
       " 'Ġall',\n",
       " 'Ġstakeholders',\n",
       " 'Ġfinding',\n",
       " 'Ġtheir',\n",
       " 'Ġway',\n",
       " 'Ġthrough',\n",
       " 'Ġthe',\n",
       " 'Ġcomplexity',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġproject',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġ',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'What',\n",
       " 'Ġexactly',\n",
       " 'Ġis',\n",
       " 'Ġa',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " '?',\n",
       " 'ĠAccording',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġdefinition',\n",
       " 'Ġof',\n",
       " 'ĠBu',\n",
       " 'zan',\n",
       " 'ĠT',\n",
       " '.',\n",
       " 'Ġand',\n",
       " 'ĠBu',\n",
       " 'zan',\n",
       " 'ĠB',\n",
       " '.',\n",
       " 'Ġ(',\n",
       " '1999',\n",
       " ',',\n",
       " 'ĠD',\n",
       " 'ess',\n",
       " 'ine',\n",
       " '-',\n",
       " 'mo',\n",
       " 'i',\n",
       " 'Ġ',\n",
       " 'Ġl',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'ĠParis',\n",
       " ':',\n",
       " 'ĠLes',\n",
       " 'ĠÃī',\n",
       " 'd',\n",
       " 'itions',\n",
       " 'Ġd',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '.),',\n",
       " 'Ġthe',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " 'Ġ(',\n",
       " 'or',\n",
       " 'Ġhe',\n",
       " 'uristic',\n",
       " 'Ġdiagram',\n",
       " ')',\n",
       " 'Ġis',\n",
       " 'Ġa',\n",
       " 'Ġgraphic',\n",
       " 'Ġ',\n",
       " 'Ġrepresentation',\n",
       " 'Ġtechnique',\n",
       " 'Ġthat',\n",
       " 'Ġfollows',\n",
       " 'Ġthe',\n",
       " 'Ġnatural',\n",
       " 'Ġfunctioning',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġmind',\n",
       " 'Ġand',\n",
       " 'Ġallows',\n",
       " 'Ġthe',\n",
       " 'Ġbrain',\n",
       " \"'s\",\n",
       " 'Ġ',\n",
       " 'Ġpotential',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġreleased',\n",
       " '.',\n",
       " 'ĠCf',\n",
       " 'ĠAnnex',\n",
       " '1',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'This',\n",
       " 'Ġtool',\n",
       " 'Ġhas',\n",
       " 'Ġmany',\n",
       " 'Ġadvantages',\n",
       " ':',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġis',\n",
       " 'Ġaccessible',\n",
       " 'Ġto',\n",
       " 'Ġall',\n",
       " 'Ġand',\n",
       " 'Ġdoes',\n",
       " 'Ġnot',\n",
       " 'Ġrequire',\n",
       " 'Ġsignificant',\n",
       " 'Ġmaterial',\n",
       " 'Ġinvestment',\n",
       " 'Ġand',\n",
       " 'Ġcan',\n",
       " 'Ġbe',\n",
       " 'Ġdone',\n",
       " 'Ġ',\n",
       " 'Ġquickly',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġis',\n",
       " 'Ġscalable',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġallows',\n",
       " 'Ġcategor',\n",
       " 'ization',\n",
       " 'Ġand',\n",
       " 'Ġlinking',\n",
       " 'Ġof',\n",
       " 'Ġinformation',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġcan',\n",
       " 'Ġbe',\n",
       " 'Ġapplied',\n",
       " 'Ġto',\n",
       " 'Ġany',\n",
       " 'Ġtype',\n",
       " 'Ġof',\n",
       " 'Ġsituation',\n",
       " ':',\n",
       " 'Ġnot',\n",
       " 'et',\n",
       " 'aking',\n",
       " ',',\n",
       " 'Ġproblem',\n",
       " 'Ġsolving',\n",
       " ',',\n",
       " 'Ġanalysis',\n",
       " ',',\n",
       " 'Ġcreation',\n",
       " 'Ġof',\n",
       " 'Ġ',\n",
       " 'Ġnew',\n",
       " 'Ġideas',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġis',\n",
       " 'Ġsuitable',\n",
       " 'Ġfor',\n",
       " 'Ġall',\n",
       " 'Ġpeople',\n",
       " 'Ġand',\n",
       " 'Ġis',\n",
       " 'Ġeasy',\n",
       " 'Ġto',\n",
       " 'Ġlearn',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġis',\n",
       " 'Ġfun',\n",
       " 'Ġand',\n",
       " 'Ġencourages',\n",
       " 'Ġexchanges',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġmakes',\n",
       " 'Ġvisible',\n",
       " 'Ġthe',\n",
       " 'Ġdimension',\n",
       " 'Ġof',\n",
       " 'Ġprojects',\n",
       " ',',\n",
       " 'Ġopportunities',\n",
       " ',',\n",
       " 'Ġinter',\n",
       " 'connect',\n",
       " 'ions',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġsynthes',\n",
       " 'izes',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġmakes',\n",
       " 'Ġthe',\n",
       " 'Ġproject',\n",
       " 'Ġunderstandable',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'âĢ¢',\n",
       " 'Ġ',\n",
       " 'ĠIt',\n",
       " 'Ġallows',\n",
       " 'Ġyou',\n",
       " 'Ġto',\n",
       " 'Ġexplore',\n",
       " 'Ġideas',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'The',\n",
       " 'Ġcreation',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " 'Ġstarts',\n",
       " 'Ġwith',\n",
       " 'Ġan',\n",
       " 'Ġidea',\n",
       " '/',\n",
       " 'problem',\n",
       " 'Ġlocated',\n",
       " 'Ġat',\n",
       " 'Ġits',\n",
       " 'Ġcenter',\n",
       " '.',\n",
       " 'ĠThis',\n",
       " 'Ġstarting',\n",
       " 'Ġpoint',\n",
       " 'Ġ',\n",
       " 'Ġgenerates',\n",
       " 'Ġideas',\n",
       " '/',\n",
       " 'work',\n",
       " 'Ġareas',\n",
       " ',',\n",
       " 'Ġincre',\n",
       " 'mented',\n",
       " 'Ġaround',\n",
       " 'Ġthis',\n",
       " 'Ġcenter',\n",
       " 'Ġin',\n",
       " 'Ġa',\n",
       " 'Ġradial',\n",
       " 'Ġstructure',\n",
       " ',',\n",
       " 'Ġwhich',\n",
       " 'Ġin',\n",
       " 'Ġturn',\n",
       " 'Ġis',\n",
       " 'Ġ',\n",
       " 'Ġcompleted',\n",
       " 'Ġwith',\n",
       " 'Ġas',\n",
       " 'Ġmany',\n",
       " 'Ġbranches',\n",
       " 'Ġas',\n",
       " 'Ġnew',\n",
       " 'Ġideas',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'This',\n",
       " 'Ġtool',\n",
       " 'Ġenables',\n",
       " 'Ġcreativity',\n",
       " 'Ġand',\n",
       " 'Ġlogic',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġmobilized',\n",
       " ',',\n",
       " 'Ġit',\n",
       " 'Ġis',\n",
       " 'Ġa',\n",
       " 'Ġmap',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġthoughts',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'Creat',\n",
       " 'ivity',\n",
       " 'Ġis',\n",
       " 'Ġenhanced',\n",
       " 'Ġbecause',\n",
       " 'Ġparticipants',\n",
       " 'Ġfeel',\n",
       " 'Ġcomfortable',\n",
       " 'Ġwith',\n",
       " 'Ġthe',\n",
       " 'Ġmethod',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'Application',\n",
       " 'Ġ&',\n",
       " 'ĠInsight',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'I',\n",
       " 'Ġstart',\n",
       " 'Ġthe',\n",
       " 'Ġprocess',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " 'Ġcreation',\n",
       " 'Ġwith',\n",
       " 'Ġthe',\n",
       " 'Ġstakeholders',\n",
       " 'Ġstanding',\n",
       " 'Ġaround',\n",
       " 'Ġa',\n",
       " 'Ġlarge',\n",
       " 'Ġboard',\n",
       " 'Ġ',\n",
       " 'Ġ(',\n",
       " 'white',\n",
       " 'Ġor',\n",
       " 'Ġpaper',\n",
       " 'Ġboard',\n",
       " ').',\n",
       " 'ĠIn',\n",
       " 'Ġthe',\n",
       " 'Ġcenter',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġboard',\n",
       " ',',\n",
       " 'ĠI',\n",
       " 'Ġwrite',\n",
       " 'Ġand',\n",
       " 'Ġhighlight',\n",
       " 'Ġthe',\n",
       " 'Ġtopic',\n",
       " 'Ġto',\n",
       " 'Ġdesign',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'Through',\n",
       " 'Ġa',\n",
       " 'Ġseries',\n",
       " 'Ġof',\n",
       " 'Ġquestions',\n",
       " ',',\n",
       " 'ĠI',\n",
       " 'Ġguide',\n",
       " 'Ġthe',\n",
       " 'Ġstakeholders',\n",
       " 'Ġin',\n",
       " 'Ġmodelling',\n",
       " 'Ġthe',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " '.',\n",
       " 'ĠI',\n",
       " 'Ġadapt',\n",
       " 'Ġthe',\n",
       " 'Ġseries',\n",
       " 'Ġ',\n",
       " 'Ġof',\n",
       " 'Ġquestions',\n",
       " 'Ġaccording',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġtopic',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġaddressed',\n",
       " '.',\n",
       " 'ĠIn',\n",
       " 'Ġthe',\n",
       " 'Ġtype',\n",
       " 'Ġof',\n",
       " 'Ġquestions',\n",
       " ',',\n",
       " 'Ġwe',\n",
       " 'Ġcan',\n",
       " 'Ġuse',\n",
       " ':',\n",
       " 'Ġwho',\n",
       " ',',\n",
       " 'Ġwhat',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'Ġwhen',\n",
       " ',',\n",
       " 'Ġwhere',\n",
       " ',',\n",
       " 'Ġwhy',\n",
       " ',',\n",
       " 'Ġhow',\n",
       " ',',\n",
       " 'Ġhow',\n",
       " 'Ġmuch',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'The',\n",
       " 'Ġuse',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'ĠâĢ',\n",
       " 'ľ',\n",
       " 'why',\n",
       " 'âĢ',\n",
       " 'Ŀ',\n",
       " 'Ġis',\n",
       " 'Ġvery',\n",
       " 'Ġinteresting',\n",
       " 'Ġto',\n",
       " 'Ġunderstand',\n",
       " 'Ġthe',\n",
       " 'Ġorigin',\n",
       " '.',\n",
       " 'ĠBy',\n",
       " 'Ġthis',\n",
       " 'Ġway',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġinterviewed',\n",
       " 'Ġperson',\n",
       " 'Ġ',\n",
       " 'Ġfre',\n",
       " 'es',\n",
       " 'Ġitself',\n",
       " 'Ġfrom',\n",
       " 'Ġparad',\n",
       " 'ig',\n",
       " 'ms',\n",
       " 'Ġand',\n",
       " 'Ġthus',\n",
       " 'Ġd',\n",
       " 'ares',\n",
       " 'Ġto',\n",
       " 'Ġpropose',\n",
       " 'Ġnew',\n",
       " 'Ġideas',\n",
       " 'Ġ/',\n",
       " 'Ġways',\n",
       " 'Ġof',\n",
       " 'Ġfunctioning',\n",
       " '.',\n",
       " 'ĠI',\n",
       " 'Ġplan',\n",
       " 'Ġtwo',\n",
       " 'Ġ',\n",
       " 'Ġhours',\n",
       " 'Ġfor',\n",
       " 'Ġa',\n",
       " 'Ġworkshop',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'Design',\n",
       " 'ĠThinking',\n",
       " 'Ġfor',\n",
       " 'Ġinnovation',\n",
       " 'Ġreflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'Ġ2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'ath',\n",
       " 'al',\n",
       " 'ie',\n",
       " 'ĠSy',\n",
       " 'lla',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'After',\n",
       " 'Ġmodelling',\n",
       " 'Ġthe',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " 'Ġon',\n",
       " 'Ġpaper',\n",
       " ',',\n",
       " 'ĠI',\n",
       " 'Ġpropose',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġparticipants',\n",
       " 'Ġa',\n",
       " 'Ġdigital',\n",
       " 'Ġvisualization',\n",
       " 'Ġof',\n",
       " 'Ġtheir',\n",
       " 'Ġ',\n",
       " 'Ġwork',\n",
       " 'Ġwith',\n",
       " 'Ġthe',\n",
       " 'Ġaddition',\n",
       " 'Ġof',\n",
       " 'Ġcolor',\n",
       " 'Ġcodes',\n",
       " ',',\n",
       " 'Ġimages',\n",
       " 'Ġand',\n",
       " 'Ġinter',\n",
       " 'connect',\n",
       " 'ions',\n",
       " '.',\n",
       " 'ĠThis',\n",
       " 'Ġsecond',\n",
       " 'Ġworkshop',\n",
       " 'Ġalso',\n",
       " 'Ġlasts',\n",
       " 'Ġ',\n",
       " 'Ġtwo',\n",
       " 'Ġhours',\n",
       " 'Ġand',\n",
       " 'Ġallows',\n",
       " 'Ġthe',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " 'Ġto',\n",
       " 'Ġevolve',\n",
       " '.',\n",
       " 'ĠOnce',\n",
       " 'Ġfamiliar',\n",
       " 'ized',\n",
       " 'Ġwith',\n",
       " 'Ġit',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġstakeholders',\n",
       " 'Ġdiscover',\n",
       " 'Ġ',\n",
       " 'Ġthe',\n",
       " 'Ġpower',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġtool',\n",
       " '.',\n",
       " 'ĠThen',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġsecond',\n",
       " 'Ġworkshop',\n",
       " 'Ġbrings',\n",
       " 'Ġout',\n",
       " 'Ġeven',\n",
       " 'Ġmore',\n",
       " 'Ġideas',\n",
       " 'Ġand',\n",
       " 'Ġconstructive',\n",
       " 'Ġ',\n",
       " 'Ġexchanges',\n",
       " 'Ġbetween',\n",
       " 'Ġthe',\n",
       " 'Ġstakeholders',\n",
       " '.',\n",
       " 'ĠAround',\n",
       " 'Ġthis',\n",
       " 'Ġnew',\n",
       " 'Ġmind',\n",
       " 'Ġmap',\n",
       " ',',\n",
       " 'Ġthey',\n",
       " 'Ġhave',\n",
       " 'Ġlearned',\n",
       " 'Ġto',\n",
       " 'Ġwork',\n",
       " 'Ġ',\n",
       " 'Ġtogether',\n",
       " 'Ġand',\n",
       " 'Ġwant',\n",
       " 'Ġto',\n",
       " 'Ġmake',\n",
       " 'Ġvisible',\n",
       " 'Ġthe',\n",
       " 'Ġuntold',\n",
       " 'Ġideas',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'I',\n",
       " 'Ġnow',\n",
       " 'Ġpresent',\n",
       " 'Ġall',\n",
       " 'Ġthe',\n",
       " 'Ġprojects',\n",
       " 'ĠI',\n",
       " 'Ġmanage',\n",
       " 'Ġin',\n",
       " 'Ġthis',\n",
       " 'Ġtype',\n",
       " 'Ġof',\n",
       " 'Ġformat',\n",
       " 'Ġin',\n",
       " 'Ġorder',\n",
       " 'Ġto',\n",
       " 'Ġease',\n",
       " 'Ġrapid',\n",
       " 'Ġunderstanding',\n",
       " 'Ġfor',\n",
       " 'Ġ',\n",
       " 'Ġdecision',\n",
       " '-',\n",
       " 'makers',\n",
       " '.',\n",
       " 'ĠThese',\n",
       " 'Ġpresentations',\n",
       " 'Ġare',\n",
       " 'Ġthe',\n",
       " 'Ġcore',\n",
       " 'Ġof',\n",
       " 'Ġmy',\n",
       " 'Ġbusiness',\n",
       " 'Ġmodels',\n",
       " '.',\n",
       " 'ĠThe',\n",
       " 'Ġdecision',\n",
       " '-',\n",
       " 'makers',\n",
       " 'Ġare',\n",
       " 'Ġ',\n",
       " 'Ġthus',\n",
       " 'Ġable',\n",
       " 'Ġto',\n",
       " 'Ġidentify',\n",
       " 'Ġthe',\n",
       " 'Ġopportunities',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġprojects',\n",
       " 'Ġand',\n",
       " 'Ġcan',\n",
       " 'Ġtake',\n",
       " 'Ġquick',\n",
       " 'Ġdecisions',\n",
       " 'Ġto',\n",
       " 'Ġvalidate',\n",
       " 'Ġthem',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'ĠThey',\n",
       " 'Ġfind',\n",
       " 'Ġanswers',\n",
       " 'Ġto',\n",
       " 'Ġtheir',\n",
       " 'Ġquestions',\n",
       " 'Ġthank',\n",
       " 'Ġto',\n",
       " 'Ġa',\n",
       " 'Ġschematic',\n",
       " 'Ġrepresentation',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'App',\n",
       " 'roach',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'What',\n",
       " 'ĠI',\n",
       " 'Ġfind',\n",
       " 'Ġamazing',\n",
       " 'Ġwith',\n",
       " 'Ġthe',\n",
       " 'Ġfac',\n",
       " 'ilitation',\n",
       " 'Ġof',\n",
       " 'Ġthis',\n",
       " 'Ġtype',\n",
       " 'Ġof',\n",
       " 'Ġworkshop',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġparticipants',\n",
       " 'Ġcommitment',\n",
       " 'Ġfor',\n",
       " 'Ġ',\n",
       " 'Ġthe',\n",
       " 'Ġproject',\n",
       " '.',\n",
       " 'ĠThis',\n",
       " 'Ġtool',\n",
       " 'Ġhelps',\n",
       " 'Ġto',\n",
       " 'Ġgive',\n",
       " 'Ġmeaning',\n",
       " '.',\n",
       " 'ĠThe',\n",
       " 'Ġparticipants',\n",
       " 'Ġappropriate',\n",
       " 'Ġthe',\n",
       " 'Ġstory',\n",
       " 'Ġand',\n",
       " 'Ġwant',\n",
       " 'Ġto',\n",
       " 'Ġkeep',\n",
       " 'Ġ',\n",
       " 'Ġwriting',\n",
       " 'Ġit',\n",
       " '.',\n",
       " 'ĠThen',\n",
       " ',',\n",
       " 'Ġthey',\n",
       " 'Ġeasily',\n",
       " 'Ġbecome',\n",
       " 'Ġactors',\n",
       " 'Ġor',\n",
       " 'Ġsponsors',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġproject',\n",
       " '.',\n",
       " 'ĠA',\n",
       " 'Ġtrust',\n",
       " 'Ġrelationship',\n",
       " 'Ġis',\n",
       " 'Ġbuilt',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'Ġthus',\n",
       " 'Ġfacilitating',\n",
       " 'Ġthe',\n",
       " 'Ġimplementation',\n",
       " 'Ġof',\n",
       " 'Ġrelated',\n",
       " 'Ġactions',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'Design',\n",
       " 'ĠThinking',\n",
       " 'Ġfor',\n",
       " 'Ġinnovation',\n",
       " 'Ġreflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'Ġ2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'ath',\n",
       " 'al',\n",
       " 'ie',\n",
       " 'ĠSy',\n",
       " 'lla',\n",
       " 'Ċ',\n",
       " 'Ċ',\n",
       " 'An',\n",
       " 'nex',\n",
       " 'Ġ1',\n",
       " ':',\n",
       " 'ĠMind',\n",
       " 'ĠMap',\n",
       " 'ĠShared',\n",
       " 'Ġfacilities',\n",
       " 'Ġproject',\n",
       " 'ĊĊ',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = temp_tokens['input_ids']\n",
    "tokens_temp = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "tokens_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddcdb950-5abd-4878-960b-aecc2df625e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4514e19-8a0d-4a36-af54-c56f45c0d38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaaa31-0241-479c-8fe9-6ea367e80d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "def chunk_and_process(input_text, tokenizer, model, max_chunk_length=512):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(input_text, max_length=max_chunk_length, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Get the chunks\n",
    "    overflow_to_sample_mapping = tokens.pop(\"overflow_to_sample_mapping\")\n",
    "    offsets = tokens.pop(\"offset_mapping\")\n",
    "\n",
    "    # Process each chunk independently\n",
    "    all_predictions = []\n",
    "    for i, offset in enumerate(offsets):\n",
    "        # Select the relevant chunk based on offsets\n",
    "        start, end = offset[0], offset[-1]\n",
    "        chunk_input = input_text[start:end]\n",
    "\n",
    "        # Tokenize the chunk\n",
    "        chunk_tokens = tokenizer(chunk_input, max_length=max_chunk_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Process the chunk through the model\n",
    "        with torch.no_grad():\n",
    "            chunk_predictions = model(**chunk_tokens).logits  # Assuming model returns logits for token classification\n",
    "\n",
    "        all_predictions.append(chunk_predictions)\n",
    "\n",
    "    # Combine or aggregate final predictions based on your task\n",
    "    # (This might involve handling overlaps between chunks for token classification)\n",
    "    final_predictions = process_final_predictions(all_predictions)\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "# Example usage within a validation loop\n",
    "def validate(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Assuming your batch contains 'input_text' and 'labels'\n",
    "            input_text = batch['input_text']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            # Chunk and process each input text\n",
    "            for text, label in zip(input_text, labels):\n",
    "                predictions = chunk_and_process(text, tokenizer, model)\n",
    "                all_predictions.append(predictions)\n",
    "                all_labels.append(label)\n",
    "\n",
    "    # Evaluate your predictions and labels as needed\n",
    "    # ...\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your_model_name\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"your_model_name\")\n",
    "\n",
    "# Assuming you have a validation dataloader\n",
    "validate(model, validation_dataloader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba847d4-1856-4833-954e-543e59cd6e6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "your_model_name is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/your_model_name/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    386\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    387\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    388\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65c0c30c-78dcedc7472510e10d3feaf7;9207f0b1-1d32-4bd0-896d-b356cf97d984)\n\nRepository Not Found for url: https://huggingface.co/your_model_name/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[0;32m     26\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[0;32m     27\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: label_tensor}\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour_model_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Example data and labels\u001b[39;00m\n\u001b[0;32m     33\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the first document.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnother example document.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:758\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    760\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:590\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    589\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 590\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    606\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\hub.py:406\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: your_model_name is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the input text\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Convert label to tensor (adjust this based on your label format)\n",
    "        label_tensor = torch.tensor(label)\n",
    "\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': label_tensor}\n",
    "\n",
    "# Example usage:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your_model_name\")\n",
    "\n",
    "# Example data and labels\n",
    "data = [\"This is the first document.\", \"Another example document.\"]\n",
    "labels = [[1, 0, 0, 2, 0, 0], [1, 0, 0, 0, 0, 2]]\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = TokenClassificationDataset(data, labels, tokenizer)\n",
    "\n",
    "# Specify batch size and create the DataLoader\n",
    "batch_size = 2\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    # Your training or validation logic here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
