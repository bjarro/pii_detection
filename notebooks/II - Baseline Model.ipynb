{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032d8c33-2628-483e-ae66-6e9057f36cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c20cb8-378d-4fef-8657-fb70ad1a139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deaa7c0e-ba47-4e5a-b1e9-c862b9afdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5113e191-4e32-4134-9104-73b57a1ddd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "011ac916-258f-4636-973e-56512dc3ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8813aa-b476-4a96-b7bc-2ee7e3f50d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDebertaForTokenClassification, DebertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54261f0-1ce4-42e1-85bd-f6d034573bd4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0734aa-a92e-47da-abb5-a6b1688b04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = './in/train.json'\n",
    "path_test = './in/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc55f461-f2f1-49ad-b2b8-0f4da99e8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = json.load(open(path_train))\n",
    "df_train = pd.json_normalize(train_json)\n",
    "\n",
    "test_json = json.load(open(path_test))\n",
    "df_test = pd.json_normalize(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b91586e-2fe8-4fe3-acad-d3ad10ff8097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f847632a-5270-4318-ae4c-2c606a462aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[True, True, True, False, False, True, False, ...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Assignment:Â  VisualizationÂ ReflectionÂ  Submitt...</td>\n",
       "      <td>[Assignment, :, Â  , Visualization, Â , Reflecti...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
       "3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
       "4        56  Assignment:Â  VisualizationÂ ReflectionÂ  Submitt...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4  [Assignment, :, Â  , Visualization, Â , Reflecti...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "2  [True, False, False, True, True, False, False,...   \n",
       "3  [True, True, True, False, False, True, False, ...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735c3f1-03b1-4b65-8397-145fba33f0d8",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1865a58d-11ce-4c5c-a61a-f954983bcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: John Doe's, Type: PERSON\n",
      "Entity: john.doe@email.com, Type: ORG\n",
      "Entity: 555, Type: CARDINAL\n",
      "Entity: 123-4567, Type: CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text containing PII\n",
    "text = \"John Doe's email is john.doe@email.com, and his phone number is +1 (555) 123-4567.\"\n",
    "\n",
    "# Process the text with spaCy NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print identified entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e11aca-63d8-46ad-b5aa-9112d69a084d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E047] Can't assign a value to unregistered extension attribute 'label'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLABEL1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLABEL2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# Replace with your actual labels\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(doc, labels):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m \u001b[38;5;241m=\u001b[39m label\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 5: Visualize the document with labels\u001b[39;00m\n\u001b[0;32m     11\u001b[0m displacy\u001b[38;5;241m.\u001b[39mserve(doc, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\tokens\\underscore.py:76\u001b[0m, in \u001b[0;36mUnderscore.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Any):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE047\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     77\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E047] Can't assign a value to unregistered extension attribute 'label'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Step 4: Add custom labels\n",
    "labels = [\"LABEL1\", \"LABEL2\", ...]  # Replace with your actual labels\n",
    "\n",
    "for token, label in zip(doc, labels):\n",
    "    token._.label = label\n",
    "\n",
    "# Step 5: Visualize the document with labels\n",
    "displacy.serve(doc, style=\"ent\")\n",
    "\n",
    "# Optional: Visualize labels beneath the text\n",
    "def visualize_with_labels(doc):\n",
    "    for token in doc:\n",
    "        print(f\"{token.text}\\t{token._.label}\")\n",
    "\n",
    "visualize_with_labels(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d445ae1-e60a-410f-ad58-1d069c57f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'you', 'love', 'ðŸ¤—', 'Transformers', '?', 'We', 'sure', 'do', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Don't you love ðŸ¤— Transformers? We sure do.\"\n",
    "\n",
    "# Process the sentence using SpaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2e1a04-572d-41e7-8890-e752e5e56143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpaCy', 'is', 'a', 'powerful', 'NLP', 'library', 'for', 'Python', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f64ba55-f049-4cbf-91d7-308df1093332",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d65adb1f-7e40-49c7-a92e-6148bd1b5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_tokens_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b92349f6-6731-45c2-bf43-e55e55fe5616",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'â–Design',\n",
       " 'â–Thinking',\n",
       " 'â–for',\n",
       " 'â–innovation',\n",
       " 'â–reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'â–2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " 'â–S',\n",
       " 'ylla',\n",
       " 'â–Challenge',\n",
       " 'â–&',\n",
       " 'â–selection',\n",
       " 'â–The',\n",
       " 'â–tool',\n",
       " 'â–I',\n",
       " 'â–use',\n",
       " 'â–to',\n",
       " 'â–help',\n",
       " 'â–all',\n",
       " 'â–stakeholders',\n",
       " 'â–finding',\n",
       " 'â–their',\n",
       " 'â–way',\n",
       " 'â–through',\n",
       " 'â–the',\n",
       " 'â–complexity',\n",
       " 'â–of',\n",
       " 'â–a',\n",
       " 'â–project',\n",
       " 'â–is',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " '.',\n",
       " 'â–What',\n",
       " 'â–exactly',\n",
       " 'â–is',\n",
       " 'â–a',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " '?',\n",
       " 'â–According',\n",
       " 'â–to',\n",
       " 'â–the',\n",
       " 'â–definition',\n",
       " 'â–of',\n",
       " 'â–Buz',\n",
       " 'an',\n",
       " 'â–T',\n",
       " '.',\n",
       " 'â–and',\n",
       " 'â–Buz',\n",
       " 'an',\n",
       " 'â–B',\n",
       " '.',\n",
       " 'â–(',\n",
       " '1999',\n",
       " ',',\n",
       " 'â–Des',\n",
       " 's',\n",
       " 'ine',\n",
       " '-',\n",
       " 'moi',\n",
       " 'â–l',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'â–Paris',\n",
       " ':',\n",
       " 'â–Les',\n",
       " 'â–Ã‰',\n",
       " 'dition',\n",
       " 's',\n",
       " 'â–d',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '.',\n",
       " ')',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–(',\n",
       " 'or',\n",
       " 'â–heuristic',\n",
       " 'â–diagram',\n",
       " ')',\n",
       " 'â–is',\n",
       " 'â–a',\n",
       " 'â–graphic',\n",
       " 'â–representation',\n",
       " 'â–technique',\n",
       " 'â–that',\n",
       " 'â–follows',\n",
       " 'â–the',\n",
       " 'â–natural',\n",
       " 'â–functioning',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–and',\n",
       " 'â–allows',\n",
       " 'â–the',\n",
       " 'â–brain',\n",
       " \"'\",\n",
       " 's',\n",
       " 'â–potential',\n",
       " 'â–to',\n",
       " 'â–be',\n",
       " 'â–released',\n",
       " '.',\n",
       " 'â–Cf',\n",
       " 'â–Annex',\n",
       " '1',\n",
       " 'â–This',\n",
       " 'â–tool',\n",
       " 'â–has',\n",
       " 'â–many',\n",
       " 'â–advantages',\n",
       " ':',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–accessible',\n",
       " 'â–to',\n",
       " 'â–all',\n",
       " 'â–and',\n",
       " 'â–does',\n",
       " 'â–not',\n",
       " 'â–require',\n",
       " 'â–significant',\n",
       " 'â–material',\n",
       " 'â–investment',\n",
       " 'â–and',\n",
       " 'â–can',\n",
       " 'â–be',\n",
       " 'â–done',\n",
       " 'â–quickly',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–scalable',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–allows',\n",
       " 'â–categorization',\n",
       " 'â–and',\n",
       " 'â–linking',\n",
       " 'â–of',\n",
       " 'â–information',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–can',\n",
       " 'â–be',\n",
       " 'â–applied',\n",
       " 'â–to',\n",
       " 'â–any',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–situation',\n",
       " ':',\n",
       " 'â–note',\n",
       " 'taking',\n",
       " ',',\n",
       " 'â–problem',\n",
       " 'â–solving',\n",
       " ',',\n",
       " 'â–analysis',\n",
       " ',',\n",
       " 'â–creation',\n",
       " 'â–of',\n",
       " 'â–new',\n",
       " 'â–ideas',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–suitable',\n",
       " 'â–for',\n",
       " 'â–all',\n",
       " 'â–people',\n",
       " 'â–and',\n",
       " 'â–is',\n",
       " 'â–easy',\n",
       " 'â–to',\n",
       " 'â–learn',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–fun',\n",
       " 'â–and',\n",
       " 'â–encourages',\n",
       " 'â–exchanges',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–makes',\n",
       " 'â–visible',\n",
       " 'â–the',\n",
       " 'â–dimension',\n",
       " 'â–of',\n",
       " 'â–projects',\n",
       " ',',\n",
       " 'â–opportunities',\n",
       " ',',\n",
       " 'â–interconnections',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–synthesize',\n",
       " 's',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–makes',\n",
       " 'â–the',\n",
       " 'â–project',\n",
       " 'â–understandable',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–allows',\n",
       " 'â–you',\n",
       " 'â–to',\n",
       " 'â–explore',\n",
       " 'â–ideas',\n",
       " 'â–The',\n",
       " 'â–creation',\n",
       " 'â–of',\n",
       " 'â–a',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–starts',\n",
       " 'â–with',\n",
       " 'â–an',\n",
       " 'â–idea',\n",
       " '/',\n",
       " 'problem',\n",
       " 'â–located',\n",
       " 'â–at',\n",
       " 'â–its',\n",
       " 'â–center',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–starting',\n",
       " 'â–point',\n",
       " 'â–generates',\n",
       " 'â–ideas',\n",
       " '/',\n",
       " 'work',\n",
       " 'â–areas',\n",
       " ',',\n",
       " 'â–incremented',\n",
       " 'â–around',\n",
       " 'â–this',\n",
       " 'â–center',\n",
       " 'â–in',\n",
       " 'â–a',\n",
       " 'â–radial',\n",
       " 'â–structure',\n",
       " ',',\n",
       " 'â–which',\n",
       " 'â–in',\n",
       " 'â–turn',\n",
       " 'â–is',\n",
       " 'â–completed',\n",
       " 'â–with',\n",
       " 'â–as',\n",
       " 'â–many',\n",
       " 'â–branches',\n",
       " 'â–as',\n",
       " 'â–new',\n",
       " 'â–ideas',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–tool',\n",
       " 'â–enables',\n",
       " 'â–creativity',\n",
       " 'â–and',\n",
       " 'â–logic',\n",
       " 'â–to',\n",
       " 'â–be',\n",
       " 'â–mobilized',\n",
       " ',',\n",
       " 'â–it',\n",
       " 'â–is',\n",
       " 'â–a',\n",
       " 'â–map',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–thoughts',\n",
       " '.',\n",
       " 'â–Creativity',\n",
       " 'â–is',\n",
       " 'â–enhanced',\n",
       " 'â–because',\n",
       " 'â–participants',\n",
       " 'â–feel',\n",
       " 'â–comfortable',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–method',\n",
       " '.',\n",
       " 'â–Application',\n",
       " 'â–&',\n",
       " 'â–Insight',\n",
       " 'â–I',\n",
       " 'â–start',\n",
       " 'â–the',\n",
       " 'â–process',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–creation',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " 'â–standing',\n",
       " 'â–around',\n",
       " 'â–a',\n",
       " 'â–large',\n",
       " 'â–board',\n",
       " 'â–(',\n",
       " 'white',\n",
       " 'â–or',\n",
       " 'â–paper',\n",
       " 'â–board',\n",
       " ')',\n",
       " '.',\n",
       " 'â–In',\n",
       " 'â–the',\n",
       " 'â–center',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–board',\n",
       " ',',\n",
       " 'â–I',\n",
       " 'â–write',\n",
       " 'â–and',\n",
       " 'â–highlight',\n",
       " 'â–the',\n",
       " 'â–topic',\n",
       " 'â–to',\n",
       " 'â–design',\n",
       " '.',\n",
       " 'â–Through',\n",
       " 'â–a',\n",
       " 'â–series',\n",
       " 'â–of',\n",
       " 'â–questions',\n",
       " ',',\n",
       " 'â–I',\n",
       " 'â–guide',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " 'â–in',\n",
       " 'â–modelling',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " '.',\n",
       " 'â–I',\n",
       " 'â–adapt',\n",
       " 'â–the',\n",
       " 'â–series',\n",
       " 'â–of',\n",
       " 'â–questions',\n",
       " 'â–according',\n",
       " 'â–to',\n",
       " 'â–the',\n",
       " 'â–topic',\n",
       " 'â–to',\n",
       " 'â–be',\n",
       " 'â–addressed',\n",
       " '.',\n",
       " 'â–In',\n",
       " 'â–the',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–questions',\n",
       " ',',\n",
       " 'â–we',\n",
       " 'â–can',\n",
       " 'â–use',\n",
       " ':',\n",
       " 'â–who',\n",
       " ',',\n",
       " 'â–what',\n",
       " ',',\n",
       " 'â–when',\n",
       " ',',\n",
       " 'â–where',\n",
       " ',',\n",
       " 'â–why',\n",
       " ',',\n",
       " 'â–how',\n",
       " ',',\n",
       " 'â–how',\n",
       " 'â–much',\n",
       " '.',\n",
       " 'â–The',\n",
       " 'â–use',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–â€œ',\n",
       " 'why',\n",
       " 'â€',\n",
       " 'â–is',\n",
       " 'â–very',\n",
       " 'â–interesting',\n",
       " 'â–to',\n",
       " 'â–understand',\n",
       " 'â–the',\n",
       " 'â–origin',\n",
       " '.',\n",
       " 'â–By',\n",
       " 'â–this',\n",
       " 'â–way',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–interviewed',\n",
       " 'â–person',\n",
       " 'â–free',\n",
       " 's',\n",
       " 'â–itself',\n",
       " 'â–from',\n",
       " 'â–paradigms',\n",
       " 'â–and',\n",
       " 'â–thus',\n",
       " 'â–dares',\n",
       " 'â–to',\n",
       " 'â–propose',\n",
       " 'â–new',\n",
       " 'â–ideas',\n",
       " 'â–/',\n",
       " 'â–ways',\n",
       " 'â–of',\n",
       " 'â–functioning',\n",
       " '.',\n",
       " 'â–I',\n",
       " 'â–plan',\n",
       " 'â–two',\n",
       " 'â–hours',\n",
       " 'â–for',\n",
       " 'â–a',\n",
       " 'â–workshop',\n",
       " '.',\n",
       " 'â–Design',\n",
       " 'â–Thinking',\n",
       " 'â–for',\n",
       " 'â–innovation',\n",
       " 'â–reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'â–2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " 'â–S',\n",
       " 'ylla',\n",
       " 'â–After',\n",
       " 'â–modelling',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–on',\n",
       " 'â–paper',\n",
       " ',',\n",
       " 'â–I',\n",
       " 'â–propose',\n",
       " 'â–to',\n",
       " 'â–the',\n",
       " 'â–participants',\n",
       " 'â–a',\n",
       " 'â–digital',\n",
       " 'â–visualization',\n",
       " 'â–of',\n",
       " 'â–their',\n",
       " 'â–work',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–addition',\n",
       " 'â–of',\n",
       " 'â–color',\n",
       " 'â–codes',\n",
       " ',',\n",
       " 'â–images',\n",
       " 'â–and',\n",
       " 'â–interconnections',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–second',\n",
       " 'â–workshop',\n",
       " 'â–also',\n",
       " 'â–last',\n",
       " 's',\n",
       " 'â–two',\n",
       " 'â–hours',\n",
       " 'â–and',\n",
       " 'â–allows',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–to',\n",
       " 'â–evolve',\n",
       " '.',\n",
       " 'â–Once',\n",
       " 'â–familiarized',\n",
       " 'â–with',\n",
       " 'â–it',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " 'â–discover',\n",
       " 'â–the',\n",
       " 'â–power',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–tool',\n",
       " '.',\n",
       " 'â–Then',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–second',\n",
       " 'â–workshop',\n",
       " 'â–brings',\n",
       " 'â–out',\n",
       " 'â–even',\n",
       " 'â–more',\n",
       " 'â–ideas',\n",
       " 'â–and',\n",
       " 'â–constructive',\n",
       " 'â–exchanges',\n",
       " 'â–between',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " '.',\n",
       " 'â–Around',\n",
       " 'â–this',\n",
       " 'â–new',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " ',',\n",
       " 'â–they',\n",
       " 'â–have',\n",
       " 'â–learned',\n",
       " 'â–to',\n",
       " 'â–work',\n",
       " 'â–together',\n",
       " 'â–and',\n",
       " 'â–want',\n",
       " 'â–to',\n",
       " 'â–make',\n",
       " 'â–visible',\n",
       " 'â–the',\n",
       " 'â–untold',\n",
       " 'â–ideas',\n",
       " '.',\n",
       " 'â–I',\n",
       " 'â–now',\n",
       " 'â–present',\n",
       " 'â–all',\n",
       " 'â–the',\n",
       " 'â–projects',\n",
       " 'â–I',\n",
       " 'â–manage',\n",
       " 'â–in',\n",
       " 'â–this',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–format',\n",
       " 'â–in',\n",
       " 'â–order',\n",
       " 'â–to',\n",
       " 'â–ease',\n",
       " 'â–rapid',\n",
       " 'â–understanding',\n",
       " 'â–for',\n",
       " 'â–decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '.',\n",
       " 'â–These',\n",
       " 'â–presentations',\n",
       " 'â–are',\n",
       " 'â–the',\n",
       " 'â–core',\n",
       " 'â–of',\n",
       " 'â–my',\n",
       " 'â–business',\n",
       " 'â–models',\n",
       " '.',\n",
       " 'â–The',\n",
       " 'â–decision',\n",
       " '-',\n",
       " 'makers',\n",
       " 'â–are',\n",
       " 'â–thus',\n",
       " 'â–able',\n",
       " 'â–to',\n",
       " 'â–identify',\n",
       " 'â–the',\n",
       " 'â–opportunities',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–projects',\n",
       " 'â–and',\n",
       " 'â–can',\n",
       " 'â–take',\n",
       " 'â–quick',\n",
       " 'â–decisions',\n",
       " 'â–to',\n",
       " 'â–validate',\n",
       " 'â–them',\n",
       " '.',\n",
       " 'â–They',\n",
       " 'â–find',\n",
       " 'â–answers',\n",
       " 'â–to',\n",
       " 'â–their',\n",
       " 'â–questions',\n",
       " 'â–thank',\n",
       " 'â–to',\n",
       " 'â–a',\n",
       " 'â–schematic',\n",
       " 'â–representation',\n",
       " '.',\n",
       " 'â–Approach',\n",
       " 'â–What',\n",
       " 'â–I',\n",
       " 'â–find',\n",
       " 'â–amazing',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–facilitation',\n",
       " 'â–of',\n",
       " 'â–this',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–workshop',\n",
       " 'â–is',\n",
       " 'â–the',\n",
       " 'â–participants',\n",
       " 'â–commitment',\n",
       " 'â–for',\n",
       " 'â–the',\n",
       " 'â–project',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–tool',\n",
       " 'â–helps',\n",
       " 'â–to',\n",
       " 'â–give',\n",
       " 'â–meaning',\n",
       " '.',\n",
       " 'â–The',\n",
       " 'â–participants',\n",
       " 'â–appropriate',\n",
       " 'â–the',\n",
       " 'â–story',\n",
       " 'â–and',\n",
       " 'â–want',\n",
       " 'â–to',\n",
       " 'â–keep',\n",
       " 'â–writing',\n",
       " 'â–it',\n",
       " '.',\n",
       " 'â–Then',\n",
       " ',',\n",
       " 'â–they',\n",
       " 'â–easily',\n",
       " 'â–become',\n",
       " 'â–actors',\n",
       " 'â–or',\n",
       " 'â–sponsors',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–project',\n",
       " '.',\n",
       " 'â–A',\n",
       " 'â–trust',\n",
       " 'â–relationship',\n",
       " 'â–is',\n",
       " 'â–built',\n",
       " ',',\n",
       " 'â–thus',\n",
       " 'â–facilitating',\n",
       " 'â–the',\n",
       " 'â–implementation',\n",
       " 'â–of',\n",
       " 'â–related',\n",
       " 'â–actions',\n",
       " '.',\n",
       " 'â–Design',\n",
       " 'â–Thinking',\n",
       " 'â–for',\n",
       " 'â–innovation',\n",
       " 'â–reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'â–2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " 'â–S',\n",
       " 'ylla',\n",
       " 'â–Annex',\n",
       " 'â–1',\n",
       " ':',\n",
       " 'â–Mind',\n",
       " 'â–Map',\n",
       " 'â–Shared',\n",
       " 'â–facilities',\n",
       " 'â–project',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e6eae-bf3a-488d-9b08-132353ffcb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5604d-9691-4d9a-9575-3125d45dac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style = 'span')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d4ce5-7697-437d-8075-f2f693d3164d",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ae0ca-64a5-472c-893f-39c6e0dbd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text containing PII\n",
    "text = df_train.loc[0].full_text\n",
    "\n",
    "# Process the text with spaCy NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print identified entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026e9ea-8b06-48c6-b054-4d2b5b079fde",
   "metadata": {},
   "source": [
    "## Vis Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230d7e7-7153-44de-87fd-7bb261e08d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document = df_train.loc[0].full_text\n",
    "\n",
    "labels = [\n",
    "    {\"start\": 1, \"end\": 2, \"label\": \"ORG\"},\n",
    "    {\"start\": 3, \"end\": 4, \"label\": \"PERSON\"},\n",
    "    # Add more label entries as needed\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(document)\n",
    "entities = [(ent[\"start\"], ent[\"end\"], ent[\"label\"]) for ent in labels]\n",
    "spans = [spacy.tokens.Span(doc, start, end, label=label) for start, end, label in entities]\n",
    "\n",
    "\n",
    "doc.spans['sc'] = spans\n",
    "displacy.render(doc, style=\"span\", jupyter=True)\n",
    "\n",
    "# doc.ents = spans\n",
    "# displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e95fe5f-6ba8-4939-aa81-579631665cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[, ]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spacy.tokens.Span(doc, start, end, label=label) for start, end, label in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47de1448-2095-4438-b803-621c84330ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d373f95d-fe9d-4505-9c63-687bc1d2470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your: 0 - 4\n",
      "long: 5 - 9\n",
      "document: 10 - 18\n",
      "goes: 19 - 23\n",
      "here: 24 - 28\n",
      ".: 28 - 29\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.idx} - {token.idx + len(token.text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e74559-bb82-4b29-8ec2-d3e0863d3010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n",
      "\n",
      "Challenge & selection\n",
      "\n",
      "The tool I use to help all stakeholders finding their way through the complexity of a project is the  mind map.\n",
      "\n",
      "What exactly is a mind map? According to the definition of Buzan T. and Buzan B. (1999, Dessine-moi  l'intelligence. Paris: Les Ã‰ditions d'Organisation.), the mind map (or heuristic diagram) is a graphic  representation technique that follows the natural functioning of the mind and allows the brain's  potential to be released. Cf Annex1\n",
      "\n",
      "This tool has many advantages:\n",
      "\n",
      "â€¢  It is accessible to all and does not require significant material investment and can be done  quickly\n",
      "\n",
      "â€¢  It is scalable\n",
      "\n",
      "â€¢  It allows categorization and linking of information\n",
      "\n",
      "â€¢  It can be applied to any type of situation: notetaking, problem solving, analysis, creation of  new ideas\n",
      "\n",
      "â€¢  It is suitable for all people and is easy to learn\n",
      "\n",
      "â€¢  It is fun and encourages exchanges\n",
      "\n",
      "â€¢  It makes visible the dimension of projects, opportunities, interconnections\n",
      "\n",
      "â€¢  It synthesizes\n",
      "\n",
      "â€¢  It makes the project understandable\n",
      "\n",
      "â€¢  It allows you to explore ideas\n",
      "\n",
      "The creation of a mind map starts with an idea/problem located at its center. This starting point  generates ideas/work areas, incremented around this center in a radial structure, which in turn is  completed with as many branches as new ideas.\n",
      "\n",
      "This tool enables creativity and logic to be mobilized, it is a map of the thoughts.\n",
      "\n",
      "Creativity is enhanced because participants feel comfortable with the method.\n",
      "\n",
      "Application & Insight\n",
      "\n",
      "I start the process of the mind map creation with the stakeholders standing around a large board  (white or paper board). In the center of the board, I write and highlight the topic to design.\n",
      "\n",
      "Through a series of questions, I guide the stakeholders in modelling the mind map. I adapt the series  of questions according to the topic to be addressed. In the type of questions, we can use: who, what,  when, where, why, how, how much.\n",
      "\n",
      "The use of the â€œwhyâ€ is very interesting to understand the origin. By this way, the interviewed person  frees itself from paradigms and thus dares to propose new ideas / ways of functioning. I plan two  hours for a workshop.\n",
      "\n",
      "Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n",
      "\n",
      "After modelling the mind map on paper, I propose to the participants a digital visualization of their  work with the addition of color codes, images and interconnections. This second workshop also lasts  two hours and allows the mind map to evolve. Once familiarized with it, the stakeholders discover  the power of the tool. Then, the second workshop brings out even more ideas and constructive  exchanges between the stakeholders. Around this new mind map, they have learned to work  together and want to make visible the untold ideas.\n",
      "\n",
      "I now present all the projects I manage in this type of format in order to ease rapid understanding for  decision-makers. These presentations are the core of my business models. The decision-makers are  thus able to identify the opportunities of the projects and can take quick decisions to validate them.  They find answers to their questions thank to a schematic representation.\n",
      "\n",
      "Approach\n",
      "\n",
      "What I find amazing with the facilitation of this type of workshop is the participants commitment for  the project. This tool helps to give meaning. The participants appropriate the story and want to keep  writing it. Then, they easily become actors or sponsors of the project. A trust relationship is built,  thus facilitating the implementation of related actions.\n",
      "\n",
      "Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\n",
      "\n",
      "Annex 1: Mind Map Shared facilities project\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( df_train.loc[0].full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2753622-e40b-4a45-8cc4-b3437e0dd2db",
   "metadata": {},
   "source": [
    "# BERT (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ee79a-17f8-44d0-b4d9-9866aa159a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64863289-e7b7-4693-a3ec-a78119a6a80e",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ec3bd-7106-49ea-8017-8d666460340c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print confidence scores for each label\n",
    "confidence_scores = tf.nn.softmax(outputs, axis=-1).numpy()[0]\n",
    "for token, label, confidence_score in zip(tokens, predicted_labels, confidence_scores):\n",
    "    confidence = confidence_score[tokenizer.convert_tokens_to_ids(label)]\n",
    "    print(f\"Token: {token}, Label: {label}, Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e66dce-3661-438b-9abb-debb5c0594c1",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d69838e-9cb9-4917-8502-f5f3c2dbee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Sample text with PII\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"dslim/bert-large-NER\" \n",
    "# model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # You can use other BERT models from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer.encode_plus(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# inputs = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "tokens_full = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "tokens = tokens_full[:500]\n",
    "\n",
    "\n",
    "# print(\"Input Text:\", text)\n",
    "# print(\"Tokenized Input:\", tokens)\n",
    "# print(\"Input IDs:\", inputs)\n",
    "\n",
    "\n",
    "# Perform token classification (NER)\n",
    "outputs = model(inputs[\"input_ids\"])\n",
    "\n",
    "# print(\"Raw Outputs:\", outputs)\n",
    "\n",
    "predictions = tf.argmax(outputs.logits, axis=2)\n",
    "\n",
    "# Decode the predicted labels\n",
    "# predicted_labels = [tokenizer.convert_ids_to_tokens(prediction) for prediction in predictions.numpy()[0]]\n",
    "predicted_labels = tokenizer.convert_ids_to_tokens(predictions.numpy()[0]) \n",
    "\n",
    "# print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "for token, label in zip(inputs[\"input_ids\"].numpy()[0], predicted_labels):\n",
    "    token = tokenizer.decode(token)\n",
    "    if label.startswith('B'):\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = token\n",
    "    elif label.startswith('I'):\n",
    "        current_entity += ' ' + token\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = \"\"\n",
    "\n",
    "# Add the last entity if any\n",
    "if current_entity:\n",
    "    entities.append(current_entity.strip())\n",
    "\n",
    "# Display the identified entities\n",
    "# print(\"Identified Entities:\")\n",
    "# for entity in entities:\n",
    "#     print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3e566-90cf-4462-ab41-7cea0fb57e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67d26be5-faec-48bb-8d5b-7700d63a96d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vis Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72148b83-0105-452f-a3c8-c40b99fde6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(tokens)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae9c49-e2e2-46f0-9977-a718c268aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3e278-7c1a-452e-ac3f-715abffdebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# # Create spaCy Doc object from the original text\n",
    "doc = nlp(text)\n",
    "doc = Doc(doc.vocab, words=tokens)\n",
    "\n",
    "# Initialize spans to store entity positions\n",
    "spans = []\n",
    "\n",
    "# Initialize variables to track current entity\n",
    "current_start = None\n",
    "current_end = None\n",
    "current_label = None\n",
    "\n",
    "# Iterate through tokens and predicted labels\n",
    "for i, (token, label) in enumerate(zip(tokens, predicted_labels)):\n",
    "    if label != '[PAD]':\n",
    "        if label.startswith('B') or label.startswith('I') or label.startswith('[unused'):\n",
    "            # Start or continuation of an entity\n",
    "            if current_start is not None:\n",
    "                spans.append((current_start, current_end, current_label))\n",
    "            current_start = i\n",
    "            current_end = i + 1\n",
    "            current_label = label[2:] if label.startswith('B') or label.startswith('I') else label  # Removing the 'B-' or 'I-' prefix\n",
    "        else:\n",
    "            # Outside of any entity\n",
    "            if current_start is not None:\n",
    "                spans.append((current_start, current_end, current_label))\n",
    "                current_start = None\n",
    "                current_end = None\n",
    "                current_label = None\n",
    "\n",
    "# Add the last entity if any\n",
    "if current_start is not None:\n",
    "    spans.append((current_start, current_end, current_label))\n",
    "\n",
    "# Create a list of entities with start and end positions\n",
    "entities = [{\"start\": start, \"end\": end, \"label\": label} for start, end, label in spans]\n",
    "\n",
    "# Create Span objects and set them in the Doc\n",
    "for ent in entities:\n",
    "    start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "    span = Span(doc, start, end, label=label)\n",
    "    doc.ents = list(doc.ents)\n",
    "\n",
    "# Prepare data for displacy visualization\n",
    "options = {\"ents\": [ent[\"label\"] for ent in entities], \"colors\": {}}\n",
    "for ent in entities:\n",
    "    options[\"colors\"][ent[\"label\"]] = \"yellow\"  # You can change the color\n",
    "\n",
    "# Visualize using displacy\n",
    "displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08d206-825b-4ef8-8fc3-a4939465c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Span objects and set them in the Doc\n",
    "\n",
    "spans_2 = []\n",
    "\n",
    "for ent in entities:\n",
    "    start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "    span = Span(doc, start, end, label=label)\n",
    "    spans_2.append(span)\n",
    "    \n",
    "doc.ents = spans_2\n",
    "\n",
    "# Prepare data for displacy visualization\n",
    "options = {\"ents\": [ent[\"label\"] for ent in entities], \"colors\": {}}\n",
    "for ent in entities:\n",
    "    options[\"colors\"][ent[\"label\"]] = \"yellow\"  # You can change the color\n",
    "\n",
    "# Visualize using displacy\n",
    "displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323fdbf5-4b3d-4865-a8aa-91414be97abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750aeb2-4cb1-4801-9c97-b0b8b43671bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(full_text,tokens,pred):\n",
    "    \n",
    "    nlp = spacy.blank(\"en\")\n",
    "    # # Create spaCy Doc object from the original text\n",
    "    doc = nlp(full_text)\n",
    "    doc = Doc(doc.vocab, words=tokens)\n",
    "\n",
    "    # Initialize spans to store entity positions\n",
    "    spans = []\n",
    "\n",
    "    # Initialize variables to track current entity\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "    current_label = None\n",
    "\n",
    "    # Iterate through tokens and predicted labels\n",
    "    for i, (token, label) in enumerate(zip(tokens, predicted_labels)):\n",
    "        if label != '[PAD]':\n",
    "            if label.startswith('B') or label.startswith('I') or label.startswith('[unused'):\n",
    "                # Start or continuation of an entity\n",
    "                if current_start is not None:\n",
    "                    spans.append((current_start, current_end, current_label))\n",
    "                current_start = i\n",
    "                current_end = i + 1\n",
    "                current_label = label[2:] if label.startswith('B') or label.startswith('I') else label  # Removing the 'B-' or 'I-' prefix\n",
    "            else:\n",
    "                # Outside of any entity\n",
    "                if current_start is not None:\n",
    "                    spans.append((current_start, current_end, current_label))\n",
    "                    current_start = None\n",
    "                    current_end = None\n",
    "                    current_label = None\n",
    "\n",
    "    # Add the last entity if any\n",
    "    if current_start is not None:\n",
    "        spans.append((current_start, current_end, current_label))\n",
    "        \n",
    "    spans = [Span(doc, start, end, label=label) for start, end, label in spans]\n",
    "    \n",
    "    doc.spans[\"sc\"] = spans\n",
    "\n",
    "\n",
    "    # Create a list of entities with start and end positions\n",
    "#     entities = [{\"start\": start, \"end\": end, \"label\": label} for start, end, label in spans]\n",
    "\n",
    "#     # Create Span objects and set them in the Doc\n",
    "#     for ent in entities:\n",
    "#         start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "#         span = Span(doc, start, end, label=label)\n",
    "#         doc.ents = list(doc.ents)\n",
    "\n",
    "    # Prepare data for displacy visualization\n",
    "    options = {\"ents\": [ent[\"label\"] for ent in entities], \"colors\": {}}\n",
    "    for ent in entities:\n",
    "        options[\"colors\"][ent[\"label\"]] = \"yellow\"  # You can change the color\n",
    "\n",
    "    # Visualize using displacy\n",
    "    displacy.render(doc, style=\"span\", options=options, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fee5e-894d-4ba6-827c-57db230c4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(tokens)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa183b-d51f-4c48-97f3-9f6b6905bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(text,tokens,predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5436e10-dc6d-412e-abe6-e268e6b15f56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f0d6ad4-3555-4211-8b20-37617db63fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.18k/1.18k [00:00<00:00, 1.18MB/s]\n",
      "D:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Thorn\\.cache\\huggingface\\hub\\models--geckos--deberta-base-fine-tuned-ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 3.30MB/s]\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 10.1MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778/778 [00:00<?, ?B/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:01<00:00, 1.27MB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.13k/1.13k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "geckos/deberta-base-fine-tuned-ner does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeckos/deberta-base-fine-tuned-ner\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# You can use other BERT models from Hugging Face\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DebertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFDebertaForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize the input text\u001b[39;00m\n\u001b[0;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py:2833\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for sharded checkpoints using safetensors is coming soon!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m     )\n\u001b[0;32m   2832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m-> 2833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2834\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2835\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for PyTorch weights. Use `from_pt=True` to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2836\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2837\u001b[0m     )\n\u001b[0;32m   2838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2842\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: geckos/deberta-base-fine-tuned-ner does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "# Sample text with PII\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "# model_name = \"knowledgator/UTC-DeBERTa-large\"\n",
    "model_name = \"geckos/deberta-base-fine-tuned-ner\"  # You can use other BERT models from Hugging Face\n",
    "tokenizer = DebertaTokenizer.from_pretrained(model_name)\n",
    "model = TFDebertaForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer.encode_plus(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# inputs = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "tokens_full = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "tokens = tokens_full[:500]\n",
    "\n",
    "\n",
    "# print(\"Input Text:\", text)\n",
    "# print(\"Tokenized Input:\", tokens)\n",
    "# print(\"Input IDs:\", inputs)\n",
    "\n",
    "\n",
    "# Perform token classification (NER)\n",
    "outputs = model(inputs[\"input_ids\"])\n",
    "\n",
    "# print(\"Raw Outputs:\", outputs)\n",
    "\n",
    "predictions = tf.argmax(outputs.logits, axis=2)\n",
    "\n",
    "# Decode the predicted labels\n",
    "# predicted_labels = [tokenizer.convert_ids_to_tokens(prediction) for prediction in predictions.numpy()[0]]\n",
    "predicted_labels = tokenizer.convert_ids_to_tokens(predictions.numpy()[0]) \n",
    "\n",
    "# print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "for token, label in zip(inputs[\"input_ids\"].numpy()[0], predicted_labels):\n",
    "    token = tokenizer.decode(token)\n",
    "    if label.startswith('B'):\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = token\n",
    "    elif label.startswith('I'):\n",
    "        current_entity += ' ' + token\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append(current_entity.strip())\n",
    "        current_entity = \"\"\n",
    "\n",
    "# Add the last entity if any\n",
    "if current_entity:\n",
    "    entities.append(current_entity.strip())\n",
    "\n",
    "# Display the identified entities\n",
    "# print(\"Identified Entities:\")\n",
    "# for entity in entities:\n",
    "#     print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e66e85c-3a78-46b8-8ee7-99fef67815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701da4b-51b6-446d-90b3-061be462a497",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "874008ae-c258-4f51-947b-f138504917b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DebertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "text = \"John Doe's email is john.doe@email.com, and his phone number is +1 (555) 123-4567.\"\n",
    "\n",
    "# model_name = \"geckos/deberta-base-fine-tuned-ner\" \n",
    "# model_name = \"knowledgator/UTC-DeBERTa-large\"\n",
    "# model_name = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "model_name = \"Yanis/microsoft-deberta-v3-large_ner_conll2003-anonimization_TRY_1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec807ead-6a28-43e6-89fd-35e73d32ab49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  2,  2,  2,  2,  0,  0, 14, 14, 14, 14, 14, 14, 14, 14,  0,  0,  0,\n",
       "          0,  0,  0,  4,  4,  4,  4,  4,  4,  4,  4,  4,  0,  2]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8aeb5de-641e-482b-ac4f-34711dd2db0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Civil state',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'Email address',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'Phone Numbers',\n",
       " 'O',\n",
       " 'Name']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80e01186-6b3b-46da-9161-6fef4ef8cb58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'â–John',\n",
       " 'â–Doe',\n",
       " \"'\",\n",
       " 's',\n",
       " 'â–email',\n",
       " 'â–is',\n",
       " 'â–john',\n",
       " '.',\n",
       " 'do',\n",
       " 'e',\n",
       " '@',\n",
       " 'email',\n",
       " '.',\n",
       " 'com',\n",
       " ',',\n",
       " 'â–and',\n",
       " 'â–his',\n",
       " 'â–phone',\n",
       " 'â–number',\n",
       " 'â–is',\n",
       " 'â–+',\n",
       " '1',\n",
       " 'â–(',\n",
       " '555',\n",
       " ')',\n",
       " 'â–123',\n",
       " '-',\n",
       " '45',\n",
       " '67',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c033c326-0af7-4b8e-b24c-71c8371666c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  610, 28484,    18,  1047,    16, 41906,     4,   417,  3540,  1039,\n",
       "         10555,     4,   175,     6,     8,    39,  1028,   346,    16,  2055,\n",
       "           134,    36, 33772,    43, 17072,    12,  1898,  4111,     4]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89ab8ba-96d1-43e9-809e-d9449fbf065e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ace1c-190f-4354-a51a-72a67fa27392",
   "metadata": {},
   "source": [
    "## Run  (Yanis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6def2d66-f3d1-4ea3-8577-48958a9e0f89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Religious beliefs',\n",
       " 'Religious beliefs',\n",
       " 'Physical addresses',\n",
       " 'Religious beliefs',\n",
       " 'Religious beliefs',\n",
       " 'Physical addresses',\n",
       " 'Phone Numbers',\n",
       " 'Religious beliefs',\n",
       " 'O',\n",
       " 'Physical addresses',\n",
       " 'O',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'Health insurance information',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Email address',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Email address',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'Name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Name']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "import torch\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "# model_name = \"geckos/deberta-base-fine-tuned-ner\" \n",
    "# model_name = \"knowledgator/UTC-DeBERTa-large\"\n",
    "# model_name = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "model_name = \"Yanis/microsoft-deberta-v3-large_ner_conll2003-anonimization_TRY_1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e43ee-251b-41a6-801c-11423b43fa65",
   "metadata": {},
   "source": [
    "## Run (lakshyakh93/deberta_finetuned_pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfed32fb-fcf0-4199-bb33-bbf71b4ff40e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'O',\n",
       " 'I-FIRSTNAME',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'I-COMPANY_NAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'O',\n",
       " 'I-FIRSTNAME',\n",
       " 'I-FIRSTNAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'O',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-FIRSTNAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'B-MIDDLENAME',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaForTokenClassification, DebertaV2ForTokenClassification\n",
    "import torch\n",
    "\n",
    "full_text = df_train.loc[0].full_text\n",
    "text = full_text\n",
    "model_name = \"lakshyakh93/deberta_finetuned_pii\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DebertaForTokenClassification.from_pretrained(model_name)\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a418da7e-b1bb-4aa0-babd-65027c35cbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_tokens_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08ffcce4-bc40-461b-8a00-6429e1c35750",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'â–Design',\n",
       " 'â–Thinking',\n",
       " 'â–for',\n",
       " 'â–innovation',\n",
       " 'â–reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'â–2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " 'â–S',\n",
       " 'ylla',\n",
       " 'â–Challenge',\n",
       " 'â–&',\n",
       " 'â–selection',\n",
       " 'â–The',\n",
       " 'â–tool',\n",
       " 'â–I',\n",
       " 'â–use',\n",
       " 'â–to',\n",
       " 'â–help',\n",
       " 'â–all',\n",
       " 'â–stakeholders',\n",
       " 'â–finding',\n",
       " 'â–their',\n",
       " 'â–way',\n",
       " 'â–through',\n",
       " 'â–the',\n",
       " 'â–complexity',\n",
       " 'â–of',\n",
       " 'â–a',\n",
       " 'â–project',\n",
       " 'â–is',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " '.',\n",
       " 'â–What',\n",
       " 'â–exactly',\n",
       " 'â–is',\n",
       " 'â–a',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " '?',\n",
       " 'â–According',\n",
       " 'â–to',\n",
       " 'â–the',\n",
       " 'â–definition',\n",
       " 'â–of',\n",
       " 'â–Buz',\n",
       " 'an',\n",
       " 'â–T',\n",
       " '.',\n",
       " 'â–and',\n",
       " 'â–Buz',\n",
       " 'an',\n",
       " 'â–B',\n",
       " '.',\n",
       " 'â–(',\n",
       " '1999',\n",
       " ',',\n",
       " 'â–Des',\n",
       " 's',\n",
       " 'ine',\n",
       " '-',\n",
       " 'moi',\n",
       " 'â–l',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'â–Paris',\n",
       " ':',\n",
       " 'â–Les',\n",
       " 'â–Ã‰',\n",
       " 'dition',\n",
       " 's',\n",
       " 'â–d',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '.',\n",
       " ')',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–(',\n",
       " 'or',\n",
       " 'â–heuristic',\n",
       " 'â–diagram',\n",
       " ')',\n",
       " 'â–is',\n",
       " 'â–a',\n",
       " 'â–graphic',\n",
       " 'â–representation',\n",
       " 'â–technique',\n",
       " 'â–that',\n",
       " 'â–follows',\n",
       " 'â–the',\n",
       " 'â–natural',\n",
       " 'â–functioning',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–and',\n",
       " 'â–allows',\n",
       " 'â–the',\n",
       " 'â–brain',\n",
       " \"'\",\n",
       " 's',\n",
       " 'â–potential',\n",
       " 'â–to',\n",
       " 'â–be',\n",
       " 'â–released',\n",
       " '.',\n",
       " 'â–Cf',\n",
       " 'â–Annex',\n",
       " '1',\n",
       " 'â–This',\n",
       " 'â–tool',\n",
       " 'â–has',\n",
       " 'â–many',\n",
       " 'â–advantages',\n",
       " ':',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–accessible',\n",
       " 'â–to',\n",
       " 'â–all',\n",
       " 'â–and',\n",
       " 'â–does',\n",
       " 'â–not',\n",
       " 'â–require',\n",
       " 'â–significant',\n",
       " 'â–material',\n",
       " 'â–investment',\n",
       " 'â–and',\n",
       " 'â–can',\n",
       " 'â–be',\n",
       " 'â–done',\n",
       " 'â–quickly',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–scalable',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–allows',\n",
       " 'â–categorization',\n",
       " 'â–and',\n",
       " 'â–linking',\n",
       " 'â–of',\n",
       " 'â–information',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–can',\n",
       " 'â–be',\n",
       " 'â–applied',\n",
       " 'â–to',\n",
       " 'â–any',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–situation',\n",
       " ':',\n",
       " 'â–note',\n",
       " 'taking',\n",
       " ',',\n",
       " 'â–problem',\n",
       " 'â–solving',\n",
       " ',',\n",
       " 'â–analysis',\n",
       " ',',\n",
       " 'â–creation',\n",
       " 'â–of',\n",
       " 'â–new',\n",
       " 'â–ideas',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–suitable',\n",
       " 'â–for',\n",
       " 'â–all',\n",
       " 'â–people',\n",
       " 'â–and',\n",
       " 'â–is',\n",
       " 'â–easy',\n",
       " 'â–to',\n",
       " 'â–learn',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–is',\n",
       " 'â–fun',\n",
       " 'â–and',\n",
       " 'â–encourages',\n",
       " 'â–exchanges',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–makes',\n",
       " 'â–visible',\n",
       " 'â–the',\n",
       " 'â–dimension',\n",
       " 'â–of',\n",
       " 'â–projects',\n",
       " ',',\n",
       " 'â–opportunities',\n",
       " ',',\n",
       " 'â–interconnections',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–synthesize',\n",
       " 's',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–makes',\n",
       " 'â–the',\n",
       " 'â–project',\n",
       " 'â–understandable',\n",
       " 'â–â€¢',\n",
       " 'â–It',\n",
       " 'â–allows',\n",
       " 'â–you',\n",
       " 'â–to',\n",
       " 'â–explore',\n",
       " 'â–ideas',\n",
       " 'â–The',\n",
       " 'â–creation',\n",
       " 'â–of',\n",
       " 'â–a',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–starts',\n",
       " 'â–with',\n",
       " 'â–an',\n",
       " 'â–idea',\n",
       " '/',\n",
       " 'problem',\n",
       " 'â–located',\n",
       " 'â–at',\n",
       " 'â–its',\n",
       " 'â–center',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–starting',\n",
       " 'â–point',\n",
       " 'â–generates',\n",
       " 'â–ideas',\n",
       " '/',\n",
       " 'work',\n",
       " 'â–areas',\n",
       " ',',\n",
       " 'â–incremented',\n",
       " 'â–around',\n",
       " 'â–this',\n",
       " 'â–center',\n",
       " 'â–in',\n",
       " 'â–a',\n",
       " 'â–radial',\n",
       " 'â–structure',\n",
       " ',',\n",
       " 'â–which',\n",
       " 'â–in',\n",
       " 'â–turn',\n",
       " 'â–is',\n",
       " 'â–completed',\n",
       " 'â–with',\n",
       " 'â–as',\n",
       " 'â–many',\n",
       " 'â–branches',\n",
       " 'â–as',\n",
       " 'â–new',\n",
       " 'â–ideas',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–tool',\n",
       " 'â–enables',\n",
       " 'â–creativity',\n",
       " 'â–and',\n",
       " 'â–logic',\n",
       " 'â–to',\n",
       " 'â–be',\n",
       " 'â–mobilized',\n",
       " ',',\n",
       " 'â–it',\n",
       " 'â–is',\n",
       " 'â–a',\n",
       " 'â–map',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–thoughts',\n",
       " '.',\n",
       " 'â–Creativity',\n",
       " 'â–is',\n",
       " 'â–enhanced',\n",
       " 'â–because',\n",
       " 'â–participants',\n",
       " 'â–feel',\n",
       " 'â–comfortable',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–method',\n",
       " '.',\n",
       " 'â–Application',\n",
       " 'â–&',\n",
       " 'â–Insight',\n",
       " 'â–I',\n",
       " 'â–start',\n",
       " 'â–the',\n",
       " 'â–process',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–creation',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " 'â–standing',\n",
       " 'â–around',\n",
       " 'â–a',\n",
       " 'â–large',\n",
       " 'â–board',\n",
       " 'â–(',\n",
       " 'white',\n",
       " 'â–or',\n",
       " 'â–paper',\n",
       " 'â–board',\n",
       " ')',\n",
       " '.',\n",
       " 'â–In',\n",
       " 'â–the',\n",
       " 'â–center',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–board',\n",
       " ',',\n",
       " 'â–I',\n",
       " 'â–write',\n",
       " 'â–and',\n",
       " 'â–highlight',\n",
       " 'â–the',\n",
       " 'â–topic',\n",
       " 'â–to',\n",
       " 'â–design',\n",
       " '.',\n",
       " 'â–Through',\n",
       " 'â–a',\n",
       " 'â–series',\n",
       " 'â–of',\n",
       " 'â–questions',\n",
       " ',',\n",
       " 'â–I',\n",
       " 'â–guide',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " 'â–in',\n",
       " 'â–modelling',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " '.',\n",
       " 'â–I',\n",
       " 'â–adapt',\n",
       " 'â–the',\n",
       " 'â–series',\n",
       " 'â–of',\n",
       " 'â–questions',\n",
       " 'â–according',\n",
       " 'â–to',\n",
       " 'â–the',\n",
       " 'â–topic',\n",
       " 'â–to',\n",
       " 'â–be',\n",
       " 'â–addressed',\n",
       " '.',\n",
       " 'â–In',\n",
       " 'â–the',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–questions',\n",
       " ',',\n",
       " 'â–we',\n",
       " 'â–can',\n",
       " 'â–use',\n",
       " ':',\n",
       " 'â–who',\n",
       " ',',\n",
       " 'â–what',\n",
       " ',',\n",
       " 'â–when',\n",
       " ',',\n",
       " 'â–where',\n",
       " ',',\n",
       " 'â–why',\n",
       " ',',\n",
       " 'â–how',\n",
       " ',',\n",
       " 'â–how',\n",
       " 'â–much',\n",
       " '.',\n",
       " 'â–The',\n",
       " 'â–use',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–â€œ',\n",
       " 'why',\n",
       " 'â€',\n",
       " 'â–is',\n",
       " 'â–very',\n",
       " 'â–interesting',\n",
       " 'â–to',\n",
       " 'â–understand',\n",
       " 'â–the',\n",
       " 'â–origin',\n",
       " '.',\n",
       " 'â–By',\n",
       " 'â–this',\n",
       " 'â–way',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–interviewed',\n",
       " 'â–person',\n",
       " 'â–free',\n",
       " 's',\n",
       " 'â–itself',\n",
       " 'â–from',\n",
       " 'â–paradigms',\n",
       " 'â–and',\n",
       " 'â–thus',\n",
       " 'â–dares',\n",
       " 'â–to',\n",
       " 'â–propose',\n",
       " 'â–new',\n",
       " 'â–ideas',\n",
       " 'â–/',\n",
       " 'â–ways',\n",
       " 'â–of',\n",
       " 'â–functioning',\n",
       " '.',\n",
       " 'â–I',\n",
       " 'â–plan',\n",
       " 'â–two',\n",
       " 'â–hours',\n",
       " 'â–for',\n",
       " 'â–a',\n",
       " 'â–workshop',\n",
       " '.',\n",
       " 'â–Design',\n",
       " 'â–Thinking',\n",
       " 'â–for',\n",
       " 'â–innovation',\n",
       " 'â–reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'â–2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " 'â–S',\n",
       " 'ylla',\n",
       " 'â–After',\n",
       " 'â–modelling',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–on',\n",
       " 'â–paper',\n",
       " ',',\n",
       " 'â–I',\n",
       " 'â–propose',\n",
       " 'â–to',\n",
       " 'â–the',\n",
       " 'â–participants',\n",
       " 'â–a',\n",
       " 'â–digital',\n",
       " 'â–visualization',\n",
       " 'â–of',\n",
       " 'â–their',\n",
       " 'â–work',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–addition',\n",
       " 'â–of',\n",
       " 'â–color',\n",
       " 'â–codes',\n",
       " ',',\n",
       " 'â–images',\n",
       " 'â–and',\n",
       " 'â–interconnections',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–second',\n",
       " 'â–workshop',\n",
       " 'â–also',\n",
       " 'â–last',\n",
       " 's',\n",
       " 'â–two',\n",
       " 'â–hours',\n",
       " 'â–and',\n",
       " 'â–allows',\n",
       " 'â–the',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " 'â–to',\n",
       " 'â–evolve',\n",
       " '.',\n",
       " 'â–Once',\n",
       " 'â–familiarized',\n",
       " 'â–with',\n",
       " 'â–it',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " 'â–discover',\n",
       " 'â–the',\n",
       " 'â–power',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–tool',\n",
       " '.',\n",
       " 'â–Then',\n",
       " ',',\n",
       " 'â–the',\n",
       " 'â–second',\n",
       " 'â–workshop',\n",
       " 'â–brings',\n",
       " 'â–out',\n",
       " 'â–even',\n",
       " 'â–more',\n",
       " 'â–ideas',\n",
       " 'â–and',\n",
       " 'â–constructive',\n",
       " 'â–exchanges',\n",
       " 'â–between',\n",
       " 'â–the',\n",
       " 'â–stakeholders',\n",
       " '.',\n",
       " 'â–Around',\n",
       " 'â–this',\n",
       " 'â–new',\n",
       " 'â–mind',\n",
       " 'â–map',\n",
       " ',',\n",
       " 'â–they',\n",
       " 'â–have',\n",
       " 'â–learned',\n",
       " 'â–to',\n",
       " 'â–work',\n",
       " 'â–together',\n",
       " 'â–and',\n",
       " 'â–want',\n",
       " 'â–to',\n",
       " 'â–make',\n",
       " 'â–visible',\n",
       " 'â–the',\n",
       " 'â–untold',\n",
       " 'â–ideas',\n",
       " '.',\n",
       " 'â–I',\n",
       " 'â–now',\n",
       " 'â–present',\n",
       " 'â–all',\n",
       " 'â–the',\n",
       " 'â–projects',\n",
       " 'â–I',\n",
       " 'â–manage',\n",
       " 'â–in',\n",
       " 'â–this',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–format',\n",
       " 'â–in',\n",
       " 'â–order',\n",
       " 'â–to',\n",
       " 'â–ease',\n",
       " 'â–rapid',\n",
       " 'â–understanding',\n",
       " 'â–for',\n",
       " 'â–decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '.',\n",
       " 'â–These',\n",
       " 'â–presentations',\n",
       " 'â–are',\n",
       " 'â–the',\n",
       " 'â–core',\n",
       " 'â–of',\n",
       " 'â–my',\n",
       " 'â–business',\n",
       " 'â–models',\n",
       " '.',\n",
       " 'â–The',\n",
       " 'â–decision',\n",
       " '-',\n",
       " 'makers',\n",
       " 'â–are',\n",
       " 'â–thus',\n",
       " 'â–able',\n",
       " 'â–to',\n",
       " 'â–identify',\n",
       " 'â–the',\n",
       " 'â–opportunities',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–projects',\n",
       " 'â–and',\n",
       " 'â–can',\n",
       " 'â–take',\n",
       " 'â–quick',\n",
       " 'â–decisions',\n",
       " 'â–to',\n",
       " 'â–validate',\n",
       " 'â–them',\n",
       " '.',\n",
       " 'â–They',\n",
       " 'â–find',\n",
       " 'â–answers',\n",
       " 'â–to',\n",
       " 'â–their',\n",
       " 'â–questions',\n",
       " 'â–thank',\n",
       " 'â–to',\n",
       " 'â–a',\n",
       " 'â–schematic',\n",
       " 'â–representation',\n",
       " '.',\n",
       " 'â–Approach',\n",
       " 'â–What',\n",
       " 'â–I',\n",
       " 'â–find',\n",
       " 'â–amazing',\n",
       " 'â–with',\n",
       " 'â–the',\n",
       " 'â–facilitation',\n",
       " 'â–of',\n",
       " 'â–this',\n",
       " 'â–type',\n",
       " 'â–of',\n",
       " 'â–workshop',\n",
       " 'â–is',\n",
       " 'â–the',\n",
       " 'â–participants',\n",
       " 'â–commitment',\n",
       " 'â–for',\n",
       " 'â–the',\n",
       " 'â–project',\n",
       " '.',\n",
       " 'â–This',\n",
       " 'â–tool',\n",
       " 'â–helps',\n",
       " 'â–to',\n",
       " 'â–give',\n",
       " 'â–meaning',\n",
       " '.',\n",
       " 'â–The',\n",
       " 'â–participants',\n",
       " 'â–appropriate',\n",
       " 'â–the',\n",
       " 'â–story',\n",
       " 'â–and',\n",
       " 'â–want',\n",
       " 'â–to',\n",
       " 'â–keep',\n",
       " 'â–writing',\n",
       " 'â–it',\n",
       " '.',\n",
       " 'â–Then',\n",
       " ',',\n",
       " 'â–they',\n",
       " 'â–easily',\n",
       " 'â–become',\n",
       " 'â–actors',\n",
       " 'â–or',\n",
       " 'â–sponsors',\n",
       " 'â–of',\n",
       " 'â–the',\n",
       " 'â–project',\n",
       " '.',\n",
       " 'â–A',\n",
       " 'â–trust',\n",
       " 'â–relationship',\n",
       " 'â–is',\n",
       " 'â–built',\n",
       " ',',\n",
       " 'â–thus',\n",
       " 'â–facilitating',\n",
       " 'â–the',\n",
       " 'â–implementation',\n",
       " 'â–of',\n",
       " 'â–related',\n",
       " 'â–actions',\n",
       " '.',\n",
       " 'â–Design',\n",
       " 'â–Thinking',\n",
       " 'â–for',\n",
       " 'â–innovation',\n",
       " 'â–reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'â–2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'atha',\n",
       " 'lie',\n",
       " 'â–S',\n",
       " 'ylla',\n",
       " 'â–Annex',\n",
       " 'â–1',\n",
       " ':',\n",
       " 'â–Mind',\n",
       " 'â–Map',\n",
       " 'â–Shared',\n",
       " 'â–facilities',\n",
       " 'â–project',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13900dcd-4b7a-4493-bf50-45025e011898",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d12f5-3be8-4115-b3e2-321376bfed0d",
   "metadata": {},
   "source": [
    "## Token Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77dafabd-e415-4126-8897-b56b35b96744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n",
    "    \n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa1abdce-6f19-4534-9c59-da6983a3761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_MAX_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1b67f43-8583-489d-b2e0-ac2ce154354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokens = tokenize(df_train.loc[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9ec72ae-70db-4733-a2e3-4e056d2e591e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'token_map'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e42f06-ed65-49ef-9088-25a9c988bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f21aa4b-7586-4531-864b-4698b6372808",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Ä Design',\n",
       " 'Ä Thinking',\n",
       " 'Ä for',\n",
       " 'Ä innovation',\n",
       " 'Ä reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'Ä 2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'ath',\n",
       " 'al',\n",
       " 'ie',\n",
       " 'Ä Sy',\n",
       " 'lla',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Chall',\n",
       " 'enge',\n",
       " 'Ä &',\n",
       " 'Ä selection',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'The',\n",
       " 'Ä tool',\n",
       " 'Ä I',\n",
       " 'Ä use',\n",
       " 'Ä to',\n",
       " 'Ä help',\n",
       " 'Ä all',\n",
       " 'Ä stakeholders',\n",
       " 'Ä finding',\n",
       " 'Ä their',\n",
       " 'Ä way',\n",
       " 'Ä through',\n",
       " 'Ä the',\n",
       " 'Ä complexity',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä project',\n",
       " 'Ä is',\n",
       " 'Ä the',\n",
       " 'Ä ',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'What',\n",
       " 'Ä exactly',\n",
       " 'Ä is',\n",
       " 'Ä a',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " '?',\n",
       " 'Ä According',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä definition',\n",
       " 'Ä of',\n",
       " 'Ä Bu',\n",
       " 'zan',\n",
       " 'Ä T',\n",
       " '.',\n",
       " 'Ä and',\n",
       " 'Ä Bu',\n",
       " 'zan',\n",
       " 'Ä B',\n",
       " '.',\n",
       " 'Ä (',\n",
       " '1999',\n",
       " ',',\n",
       " 'Ä D',\n",
       " 'ess',\n",
       " 'ine',\n",
       " '-',\n",
       " 'mo',\n",
       " 'i',\n",
       " 'Ä ',\n",
       " 'Ä l',\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'Ä Paris',\n",
       " ':',\n",
       " 'Ä Les',\n",
       " 'Ä ÃƒÄ«',\n",
       " 'd',\n",
       " 'itions',\n",
       " 'Ä d',\n",
       " \"'\",\n",
       " 'Organ',\n",
       " 'isation',\n",
       " '.),',\n",
       " 'Ä the',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " 'Ä (',\n",
       " 'or',\n",
       " 'Ä he',\n",
       " 'uristic',\n",
       " 'Ä diagram',\n",
       " ')',\n",
       " 'Ä is',\n",
       " 'Ä a',\n",
       " 'Ä graphic',\n",
       " 'Ä ',\n",
       " 'Ä representation',\n",
       " 'Ä technique',\n",
       " 'Ä that',\n",
       " 'Ä follows',\n",
       " 'Ä the',\n",
       " 'Ä natural',\n",
       " 'Ä functioning',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä mind',\n",
       " 'Ä and',\n",
       " 'Ä allows',\n",
       " 'Ä the',\n",
       " 'Ä brain',\n",
       " \"'s\",\n",
       " 'Ä ',\n",
       " 'Ä potential',\n",
       " 'Ä to',\n",
       " 'Ä be',\n",
       " 'Ä released',\n",
       " '.',\n",
       " 'Ä Cf',\n",
       " 'Ä Annex',\n",
       " '1',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'This',\n",
       " 'Ä tool',\n",
       " 'Ä has',\n",
       " 'Ä many',\n",
       " 'Ä advantages',\n",
       " ':',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä is',\n",
       " 'Ä accessible',\n",
       " 'Ä to',\n",
       " 'Ä all',\n",
       " 'Ä and',\n",
       " 'Ä does',\n",
       " 'Ä not',\n",
       " 'Ä require',\n",
       " 'Ä significant',\n",
       " 'Ä material',\n",
       " 'Ä investment',\n",
       " 'Ä and',\n",
       " 'Ä can',\n",
       " 'Ä be',\n",
       " 'Ä done',\n",
       " 'Ä ',\n",
       " 'Ä quickly',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä is',\n",
       " 'Ä scalable',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä allows',\n",
       " 'Ä categor',\n",
       " 'ization',\n",
       " 'Ä and',\n",
       " 'Ä linking',\n",
       " 'Ä of',\n",
       " 'Ä information',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä can',\n",
       " 'Ä be',\n",
       " 'Ä applied',\n",
       " 'Ä to',\n",
       " 'Ä any',\n",
       " 'Ä type',\n",
       " 'Ä of',\n",
       " 'Ä situation',\n",
       " ':',\n",
       " 'Ä not',\n",
       " 'et',\n",
       " 'aking',\n",
       " ',',\n",
       " 'Ä problem',\n",
       " 'Ä solving',\n",
       " ',',\n",
       " 'Ä analysis',\n",
       " ',',\n",
       " 'Ä creation',\n",
       " 'Ä of',\n",
       " 'Ä ',\n",
       " 'Ä new',\n",
       " 'Ä ideas',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä is',\n",
       " 'Ä suitable',\n",
       " 'Ä for',\n",
       " 'Ä all',\n",
       " 'Ä people',\n",
       " 'Ä and',\n",
       " 'Ä is',\n",
       " 'Ä easy',\n",
       " 'Ä to',\n",
       " 'Ä learn',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä is',\n",
       " 'Ä fun',\n",
       " 'Ä and',\n",
       " 'Ä encourages',\n",
       " 'Ä exchanges',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä makes',\n",
       " 'Ä visible',\n",
       " 'Ä the',\n",
       " 'Ä dimension',\n",
       " 'Ä of',\n",
       " 'Ä projects',\n",
       " ',',\n",
       " 'Ä opportunities',\n",
       " ',',\n",
       " 'Ä inter',\n",
       " 'connect',\n",
       " 'ions',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä synthes',\n",
       " 'izes',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä makes',\n",
       " 'Ä the',\n",
       " 'Ä project',\n",
       " 'Ä understandable',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Ã¢Ä¢Â¢',\n",
       " 'Ä ',\n",
       " 'Ä It',\n",
       " 'Ä allows',\n",
       " 'Ä you',\n",
       " 'Ä to',\n",
       " 'Ä explore',\n",
       " 'Ä ideas',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'The',\n",
       " 'Ä creation',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " 'Ä starts',\n",
       " 'Ä with',\n",
       " 'Ä an',\n",
       " 'Ä idea',\n",
       " '/',\n",
       " 'problem',\n",
       " 'Ä located',\n",
       " 'Ä at',\n",
       " 'Ä its',\n",
       " 'Ä center',\n",
       " '.',\n",
       " 'Ä This',\n",
       " 'Ä starting',\n",
       " 'Ä point',\n",
       " 'Ä ',\n",
       " 'Ä generates',\n",
       " 'Ä ideas',\n",
       " '/',\n",
       " 'work',\n",
       " 'Ä areas',\n",
       " ',',\n",
       " 'Ä incre',\n",
       " 'mented',\n",
       " 'Ä around',\n",
       " 'Ä this',\n",
       " 'Ä center',\n",
       " 'Ä in',\n",
       " 'Ä a',\n",
       " 'Ä radial',\n",
       " 'Ä structure',\n",
       " ',',\n",
       " 'Ä which',\n",
       " 'Ä in',\n",
       " 'Ä turn',\n",
       " 'Ä is',\n",
       " 'Ä ',\n",
       " 'Ä completed',\n",
       " 'Ä with',\n",
       " 'Ä as',\n",
       " 'Ä many',\n",
       " 'Ä branches',\n",
       " 'Ä as',\n",
       " 'Ä new',\n",
       " 'Ä ideas',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'This',\n",
       " 'Ä tool',\n",
       " 'Ä enables',\n",
       " 'Ä creativity',\n",
       " 'Ä and',\n",
       " 'Ä logic',\n",
       " 'Ä to',\n",
       " 'Ä be',\n",
       " 'Ä mobilized',\n",
       " ',',\n",
       " 'Ä it',\n",
       " 'Ä is',\n",
       " 'Ä a',\n",
       " 'Ä map',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä thoughts',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Creat',\n",
       " 'ivity',\n",
       " 'Ä is',\n",
       " 'Ä enhanced',\n",
       " 'Ä because',\n",
       " 'Ä participants',\n",
       " 'Ä feel',\n",
       " 'Ä comfortable',\n",
       " 'Ä with',\n",
       " 'Ä the',\n",
       " 'Ä method',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Application',\n",
       " 'Ä &',\n",
       " 'Ä Insight',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'I',\n",
       " 'Ä start',\n",
       " 'Ä the',\n",
       " 'Ä process',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " 'Ä creation',\n",
       " 'Ä with',\n",
       " 'Ä the',\n",
       " 'Ä stakeholders',\n",
       " 'Ä standing',\n",
       " 'Ä around',\n",
       " 'Ä a',\n",
       " 'Ä large',\n",
       " 'Ä board',\n",
       " 'Ä ',\n",
       " 'Ä (',\n",
       " 'white',\n",
       " 'Ä or',\n",
       " 'Ä paper',\n",
       " 'Ä board',\n",
       " ').',\n",
       " 'Ä In',\n",
       " 'Ä the',\n",
       " 'Ä center',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä board',\n",
       " ',',\n",
       " 'Ä I',\n",
       " 'Ä write',\n",
       " 'Ä and',\n",
       " 'Ä highlight',\n",
       " 'Ä the',\n",
       " 'Ä topic',\n",
       " 'Ä to',\n",
       " 'Ä design',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Through',\n",
       " 'Ä a',\n",
       " 'Ä series',\n",
       " 'Ä of',\n",
       " 'Ä questions',\n",
       " ',',\n",
       " 'Ä I',\n",
       " 'Ä guide',\n",
       " 'Ä the',\n",
       " 'Ä stakeholders',\n",
       " 'Ä in',\n",
       " 'Ä modelling',\n",
       " 'Ä the',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " '.',\n",
       " 'Ä I',\n",
       " 'Ä adapt',\n",
       " 'Ä the',\n",
       " 'Ä series',\n",
       " 'Ä ',\n",
       " 'Ä of',\n",
       " 'Ä questions',\n",
       " 'Ä according',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä topic',\n",
       " 'Ä to',\n",
       " 'Ä be',\n",
       " 'Ä addressed',\n",
       " '.',\n",
       " 'Ä In',\n",
       " 'Ä the',\n",
       " 'Ä type',\n",
       " 'Ä of',\n",
       " 'Ä questions',\n",
       " ',',\n",
       " 'Ä we',\n",
       " 'Ä can',\n",
       " 'Ä use',\n",
       " ':',\n",
       " 'Ä who',\n",
       " ',',\n",
       " 'Ä what',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'Ä when',\n",
       " ',',\n",
       " 'Ä where',\n",
       " ',',\n",
       " 'Ä why',\n",
       " ',',\n",
       " 'Ä how',\n",
       " ',',\n",
       " 'Ä how',\n",
       " 'Ä much',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'The',\n",
       " 'Ä use',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä Ã¢Ä¢',\n",
       " 'Ä¾',\n",
       " 'why',\n",
       " 'Ã¢Ä¢',\n",
       " 'Ä¿',\n",
       " 'Ä is',\n",
       " 'Ä very',\n",
       " 'Ä interesting',\n",
       " 'Ä to',\n",
       " 'Ä understand',\n",
       " 'Ä the',\n",
       " 'Ä origin',\n",
       " '.',\n",
       " 'Ä By',\n",
       " 'Ä this',\n",
       " 'Ä way',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä interviewed',\n",
       " 'Ä person',\n",
       " 'Ä ',\n",
       " 'Ä fre',\n",
       " 'es',\n",
       " 'Ä itself',\n",
       " 'Ä from',\n",
       " 'Ä parad',\n",
       " 'ig',\n",
       " 'ms',\n",
       " 'Ä and',\n",
       " 'Ä thus',\n",
       " 'Ä d',\n",
       " 'ares',\n",
       " 'Ä to',\n",
       " 'Ä propose',\n",
       " 'Ä new',\n",
       " 'Ä ideas',\n",
       " 'Ä /',\n",
       " 'Ä ways',\n",
       " 'Ä of',\n",
       " 'Ä functioning',\n",
       " '.',\n",
       " 'Ä I',\n",
       " 'Ä plan',\n",
       " 'Ä two',\n",
       " 'Ä ',\n",
       " 'Ä hours',\n",
       " 'Ä for',\n",
       " 'Ä a',\n",
       " 'Ä workshop',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Design',\n",
       " 'Ä Thinking',\n",
       " 'Ä for',\n",
       " 'Ä innovation',\n",
       " 'Ä reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'Ä 2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'ath',\n",
       " 'al',\n",
       " 'ie',\n",
       " 'Ä Sy',\n",
       " 'lla',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'After',\n",
       " 'Ä modelling',\n",
       " 'Ä the',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " 'Ä on',\n",
       " 'Ä paper',\n",
       " ',',\n",
       " 'Ä I',\n",
       " 'Ä propose',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä participants',\n",
       " 'Ä a',\n",
       " 'Ä digital',\n",
       " 'Ä visualization',\n",
       " 'Ä of',\n",
       " 'Ä their',\n",
       " 'Ä ',\n",
       " 'Ä work',\n",
       " 'Ä with',\n",
       " 'Ä the',\n",
       " 'Ä addition',\n",
       " 'Ä of',\n",
       " 'Ä color',\n",
       " 'Ä codes',\n",
       " ',',\n",
       " 'Ä images',\n",
       " 'Ä and',\n",
       " 'Ä inter',\n",
       " 'connect',\n",
       " 'ions',\n",
       " '.',\n",
       " 'Ä This',\n",
       " 'Ä second',\n",
       " 'Ä workshop',\n",
       " 'Ä also',\n",
       " 'Ä lasts',\n",
       " 'Ä ',\n",
       " 'Ä two',\n",
       " 'Ä hours',\n",
       " 'Ä and',\n",
       " 'Ä allows',\n",
       " 'Ä the',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " 'Ä to',\n",
       " 'Ä evolve',\n",
       " '.',\n",
       " 'Ä Once',\n",
       " 'Ä familiar',\n",
       " 'ized',\n",
       " 'Ä with',\n",
       " 'Ä it',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä stakeholders',\n",
       " 'Ä discover',\n",
       " 'Ä ',\n",
       " 'Ä the',\n",
       " 'Ä power',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä tool',\n",
       " '.',\n",
       " 'Ä Then',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä second',\n",
       " 'Ä workshop',\n",
       " 'Ä brings',\n",
       " 'Ä out',\n",
       " 'Ä even',\n",
       " 'Ä more',\n",
       " 'Ä ideas',\n",
       " 'Ä and',\n",
       " 'Ä constructive',\n",
       " 'Ä ',\n",
       " 'Ä exchanges',\n",
       " 'Ä between',\n",
       " 'Ä the',\n",
       " 'Ä stakeholders',\n",
       " '.',\n",
       " 'Ä Around',\n",
       " 'Ä this',\n",
       " 'Ä new',\n",
       " 'Ä mind',\n",
       " 'Ä map',\n",
       " ',',\n",
       " 'Ä they',\n",
       " 'Ä have',\n",
       " 'Ä learned',\n",
       " 'Ä to',\n",
       " 'Ä work',\n",
       " 'Ä ',\n",
       " 'Ä together',\n",
       " 'Ä and',\n",
       " 'Ä want',\n",
       " 'Ä to',\n",
       " 'Ä make',\n",
       " 'Ä visible',\n",
       " 'Ä the',\n",
       " 'Ä untold',\n",
       " 'Ä ideas',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'I',\n",
       " 'Ä now',\n",
       " 'Ä present',\n",
       " 'Ä all',\n",
       " 'Ä the',\n",
       " 'Ä projects',\n",
       " 'Ä I',\n",
       " 'Ä manage',\n",
       " 'Ä in',\n",
       " 'Ä this',\n",
       " 'Ä type',\n",
       " 'Ä of',\n",
       " 'Ä format',\n",
       " 'Ä in',\n",
       " 'Ä order',\n",
       " 'Ä to',\n",
       " 'Ä ease',\n",
       " 'Ä rapid',\n",
       " 'Ä understanding',\n",
       " 'Ä for',\n",
       " 'Ä ',\n",
       " 'Ä decision',\n",
       " '-',\n",
       " 'makers',\n",
       " '.',\n",
       " 'Ä These',\n",
       " 'Ä presentations',\n",
       " 'Ä are',\n",
       " 'Ä the',\n",
       " 'Ä core',\n",
       " 'Ä of',\n",
       " 'Ä my',\n",
       " 'Ä business',\n",
       " 'Ä models',\n",
       " '.',\n",
       " 'Ä The',\n",
       " 'Ä decision',\n",
       " '-',\n",
       " 'makers',\n",
       " 'Ä are',\n",
       " 'Ä ',\n",
       " 'Ä thus',\n",
       " 'Ä able',\n",
       " 'Ä to',\n",
       " 'Ä identify',\n",
       " 'Ä the',\n",
       " 'Ä opportunities',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä projects',\n",
       " 'Ä and',\n",
       " 'Ä can',\n",
       " 'Ä take',\n",
       " 'Ä quick',\n",
       " 'Ä decisions',\n",
       " 'Ä to',\n",
       " 'Ä validate',\n",
       " 'Ä them',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Ä They',\n",
       " 'Ä find',\n",
       " 'Ä answers',\n",
       " 'Ä to',\n",
       " 'Ä their',\n",
       " 'Ä questions',\n",
       " 'Ä thank',\n",
       " 'Ä to',\n",
       " 'Ä a',\n",
       " 'Ä schematic',\n",
       " 'Ä representation',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'App',\n",
       " 'roach',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'What',\n",
       " 'Ä I',\n",
       " 'Ä find',\n",
       " 'Ä amazing',\n",
       " 'Ä with',\n",
       " 'Ä the',\n",
       " 'Ä fac',\n",
       " 'ilitation',\n",
       " 'Ä of',\n",
       " 'Ä this',\n",
       " 'Ä type',\n",
       " 'Ä of',\n",
       " 'Ä workshop',\n",
       " 'Ä is',\n",
       " 'Ä the',\n",
       " 'Ä participants',\n",
       " 'Ä commitment',\n",
       " 'Ä for',\n",
       " 'Ä ',\n",
       " 'Ä the',\n",
       " 'Ä project',\n",
       " '.',\n",
       " 'Ä This',\n",
       " 'Ä tool',\n",
       " 'Ä helps',\n",
       " 'Ä to',\n",
       " 'Ä give',\n",
       " 'Ä meaning',\n",
       " '.',\n",
       " 'Ä The',\n",
       " 'Ä participants',\n",
       " 'Ä appropriate',\n",
       " 'Ä the',\n",
       " 'Ä story',\n",
       " 'Ä and',\n",
       " 'Ä want',\n",
       " 'Ä to',\n",
       " 'Ä keep',\n",
       " 'Ä ',\n",
       " 'Ä writing',\n",
       " 'Ä it',\n",
       " '.',\n",
       " 'Ä Then',\n",
       " ',',\n",
       " 'Ä they',\n",
       " 'Ä easily',\n",
       " 'Ä become',\n",
       " 'Ä actors',\n",
       " 'Ä or',\n",
       " 'Ä sponsors',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä project',\n",
       " '.',\n",
       " 'Ä A',\n",
       " 'Ä trust',\n",
       " 'Ä relationship',\n",
       " 'Ä is',\n",
       " 'Ä built',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'Ä thus',\n",
       " 'Ä facilitating',\n",
       " 'Ä the',\n",
       " 'Ä implementation',\n",
       " 'Ä of',\n",
       " 'Ä related',\n",
       " 'Ä actions',\n",
       " '.',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'Design',\n",
       " 'Ä Thinking',\n",
       " 'Ä for',\n",
       " 'Ä innovation',\n",
       " 'Ä reflex',\n",
       " 'ion',\n",
       " '-',\n",
       " 'Av',\n",
       " 'ril',\n",
       " 'Ä 2021',\n",
       " '-',\n",
       " 'N',\n",
       " 'ath',\n",
       " 'al',\n",
       " 'ie',\n",
       " 'Ä Sy',\n",
       " 'lla',\n",
       " 'ÄŠ',\n",
       " 'ÄŠ',\n",
       " 'An',\n",
       " 'nex',\n",
       " 'Ä 1',\n",
       " ':',\n",
       " 'Ä Mind',\n",
       " 'Ä Map',\n",
       " 'Ä Shared',\n",
       " 'Ä facilities',\n",
       " 'Ä project',\n",
       " 'ÄŠÄŠ',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = temp_tokens['input_ids']\n",
    "tokens_temp = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "tokens_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddcdb950-5abd-4878-960b-aecc2df625e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4514e19-8a0d-4a36-af54-c56f45c0d38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaaa31-0241-479c-8fe9-6ea367e80d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "def chunk_and_process(input_text, tokenizer, model, max_chunk_length=512):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(input_text, max_length=max_chunk_length, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Get the chunks\n",
    "    overflow_to_sample_mapping = tokens.pop(\"overflow_to_sample_mapping\")\n",
    "    offsets = tokens.pop(\"offset_mapping\")\n",
    "\n",
    "    # Process each chunk independently\n",
    "    all_predictions = []\n",
    "    for i, offset in enumerate(offsets):\n",
    "        # Select the relevant chunk based on offsets\n",
    "        start, end = offset[0], offset[-1]\n",
    "        chunk_input = input_text[start:end]\n",
    "\n",
    "        # Tokenize the chunk\n",
    "        chunk_tokens = tokenizer(chunk_input, max_length=max_chunk_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Process the chunk through the model\n",
    "        with torch.no_grad():\n",
    "            chunk_predictions = model(**chunk_tokens).logits  # Assuming model returns logits for token classification\n",
    "\n",
    "        all_predictions.append(chunk_predictions)\n",
    "\n",
    "    # Combine or aggregate final predictions based on your task\n",
    "    # (This might involve handling overlaps between chunks for token classification)\n",
    "    final_predictions = process_final_predictions(all_predictions)\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "# Example usage within a validation loop\n",
    "def validate(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Assuming your batch contains 'input_text' and 'labels'\n",
    "            input_text = batch['input_text']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            # Chunk and process each input text\n",
    "            for text, label in zip(input_text, labels):\n",
    "                predictions = chunk_and_process(text, tokenizer, model)\n",
    "                all_predictions.append(predictions)\n",
    "                all_labels.append(label)\n",
    "\n",
    "    # Evaluate your predictions and labels as needed\n",
    "    # ...\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your_model_name\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"your_model_name\")\n",
    "\n",
    "# Assuming you have a validation dataloader\n",
    "validate(model, validation_dataloader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba847d4-1856-4833-954e-543e59cd6e6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "your_model_name is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/your_model_name/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    386\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    387\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    388\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65c0c30c-78dcedc7472510e10d3feaf7;9207f0b1-1d32-4bd0-896d-b356cf97d984)\n\nRepository Not Found for url: https://huggingface.co/your_model_name/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[0;32m     26\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[0;32m     27\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: label_tensor}\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour_model_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Example data and labels\u001b[39;00m\n\u001b[0;32m     33\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the first document.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnother example document.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:758\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    760\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:590\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    589\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 590\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    606\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\utils\\hub.py:406\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: your_model_name is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the input text\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Convert label to tensor (adjust this based on your label format)\n",
    "        label_tensor = torch.tensor(label)\n",
    "\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': label_tensor}\n",
    "\n",
    "# Example usage:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your_model_name\")\n",
    "\n",
    "# Example data and labels\n",
    "data = [\"This is the first document.\", \"Another example document.\"]\n",
    "labels = [[1, 0, 0, 2, 0, 0], [1, 0, 0, 0, 0, 2]]\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = TokenClassificationDataset(data, labels, tokenizer)\n",
    "\n",
    "# Specify batch size and create the DataLoader\n",
    "batch_size = 2\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    # Your training or validation logic here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
